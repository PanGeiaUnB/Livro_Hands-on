{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Reinforcement Learning - RL</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O aprendizado por reforço (RL - Reinforcement Learning) é um campo de Inteligência Artificial que tenta, através do recebimento de recompensas, ensinar um agente inteligente a decidir ações adequadas para uma determinada situação vivenciada. De maneira intuitiva, o processo é semelhante ao adestramento de um cachorro que aprende a realizar algumas tarefas a partir do recebimento de biscoitos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.1. Equação de Bellman</h4>\n",
    "\n",
    "Para compreender o aprendizado do crítico, voltaremos à definição da variável $Q(s,a)$. Repare que utilizando as propriedades de um somatório, podemos escrever o retorno como:\n",
    "\n",
    "$$ R_t = r_t + \\sum_{k=t+1}^{T} r_k \\\\\n",
    "   R_t = r_t + R_{t+1}\n",
    " $$\n",
    "\n",
    "Nos próximos passos, não entraremos na matemática da dedução por acreditar que o seu entendimento demanda um pouco mais de tempo, mas deixamos uma [referência](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html) que faz essa dedução completa.\n",
    "\n",
    "Para nós, o importante é compreendermos a noção de que o retorno presente pode ser decomposto na recompensa atual adicionada ao retorno do próximo evento e que isso pode ser estendido para o valor do estado-ação, por ser uma variável em função do retorno. \n",
    "\n",
    "$$ Q(s,a) = r_t + Q(s',a') $$\n",
    "\n",
    "em que $s'$ é o estado futuro (no evento $t+1$) e $a'$ é a ação tomada no evento $t+1$. Entretanto, em um cenário em que tanto a transição de estados, quanto a tomada de decisão não são conhecidas, é necessário tratarmos com as **esperanças** dessas variáveis. Assim:\n",
    "\n",
    "$$ Q(s,a) = \\mathbb{E}_{s'\\mathtt{\\sim}P}\\{r_t + \\mathbb{E}_{a'\\mathtt{\\sim}\\mu}[Q(s',a')]\\} $$\n",
    "\n",
    "em que $P$ é a probabilidade de transição dos estados e $\\mu$ é a política para tomada de decisão adotada pelo ator. Essa última equação é conhecida como **Equação de Bellman**, e é a base para o aprendizado de quase todas as arquiteturas de DRL. \n",
    "\n",
    "Na maioria das referências, veremos um termo $\\gamma$ multiplicando a esperança de $Q(s',a')$, chamado de fator de desconto. Essa variável serve para o algoritmo dar mais importância às experiências recentes, em detrimento das que estão em um futuro mais distante. Nessa explicação, ele foi suprimido por simplificação.\n",
    "\n",
    "Vejamos, agora, como podemos utilizar essa equação para ensinar a rede neural do crítico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.2. Processo de aprendizagem</h4>\n",
    "\n",
    "Pensemos no início do treinamento do algoritmo: a rede neural do crítico faz uma estimativa para o valor de $Q$, mas por ainda não estar treinada, essa estimativa não é confiável. Nesse sentido, o que precisaríamos fazer para a rede aprender a melhorar essa estimativa? Qual seria o valor esperado que utilizaríamos para calcular o custo que seria usado no otimizador da rede?\n",
    "\n",
    "Voltemos à Equação de Bellman. Repare que, se a nossa forma de calcular o $Q$ é a partir de uma rede neural, precisaríamos usá-la para calcular ambos os lados da equação:\n",
    "\n",
    "*    No primeiro, passaríamos à rede o estado e ação atuais ($s$ e $a$) como entrada e receberíamos a estimativa de $Q(s,a)$ como saída;\n",
    "*    No segundo, passaríamos à rede o estado e ação futuras ($s'$ e $a'$) como entrada e receberíamos a estimativa de $Q(s',a')$ como saída.\n",
    "\n",
    "Para completar a equação, precisaríamos, ao executar a ação $a$ no estado $s$, coletar $r_t$, que é calculada de maneira determinística pelo próprio ambiente e enviada para o algoritmo.\n",
    "\n",
    "Apesar de a equação de Bellman ser matematicamente bem definida, de forma que os dois lados da equação precisem ser iguais, ao utilizar uma rede neural para o cálculo do $Q$, estamos realizando apenas uma estimativa para essa variável, de forma que, na prática, haverá divergência entre os dois lados da equação.\n",
    "\n",
    "Mas como podemos usar tudo isso para ensinar uma rede totalmente leiga a aprender a estimar o valor de $Q$? \n",
    "\n",
    "Pense que os dois lados da equação de Bellman, calculados a partir da utilização da estimativa da rede neural, possuem grandes incertezas, principalmente no início do processo de aprendizagem. Entretanto, como foi dito, $r_t$ é uma variável coletada diretamente do ambiente, **não havendo incerteza alguma em seu valor**.\n",
    "\n",
    "Assim, graças a esse fato, podemos dizer que, utilizando redes neurais para estimar o valor de $Q$, o cálculo do lado direito da equação de Bellman possui **menos incerteza** do que o lado esquerdo da equação, que depende apenas da saída da rede.\n",
    "\n",
    "Dessa forma, é possível treinar a rede neural do crítico utilizando uma função de perda que calcule o custo considerando $Q(s,a)$ como a estimativa feita pela rede e o lado direito da equação como o valor esperado.\n",
    "\n",
    "Perceba que o valor esperado para treinar a rede depende da saída da própria rede, algo que é inesperado e contraintuitivo, mas pouco a pouco os valores de recompensa recebidos vão direcionando a rede para a convergência, de forma que as estimativas vão melhorando a cada época de treinamento.\n",
    "\n",
    "O fato apontado acima é o principal responsável pelas redes de DRL serem mais instáveis e possuírem convergência mais lenta do que as redes de ML/DL tradicionais, o que faz com que essas arquiteturas precisem ter um treinamento mais longo, além de usarem diversas técnicas para ajudar na convergência do aprendizado.\n",
    "\n",
    "A imagem abaixo ilustra o treinamento de um DDPG em um único evento. Na figura, as setas verdes significam a passagem de entradas e saídas até a estimativa dos dois lados da equação de Bellman e as setas vermelhas mostram o processo de atualização dos parâmetros das redes a partir dos custos calculados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Deep Q-Network - DQN</h3>\n",
    "\n",
    "O DRL é uma consequência de toda teoria de RL desenvolvida ao longo de anos. Em 2013, Volodymyr Mnih propõe utilizar uma rede neural profunda como agente de um problema de RL, [leia mais sobre](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), que até então tentava modelar os agentes utilizando, principalmente, modelos estatísticos. A aplicação do artigo, inclusive, era produzir um agente capaz de jogar *games* clássicos do console Atari.\n",
    "\n",
    "Dessa forma, o DRL surge da ideia de se incorporar o *Deep Learning* em problemas de *Reinforcement Learning*, visando resolver problemas de complexidade elevada. A partir dessa união, precisamos entender como a rede neural aprenderá a tomar decisões a partir de um estado do ambiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.1. Aprendizagem da rede e técnicas de conversão</h4>\n",
    "\n",
    "Existem alguns conceitos chave para o RL que servirão como base para a maioria dos algoritmos da área, de forma que é fundamental conhecê-los para conseguir compreender as aplicações desse campo. São eles:\n",
    "\n",
    "*   Ambiente;\n",
    "*   Agente $\\mu$;\n",
    "*   Estado $s$; \n",
    "*   Recompensa $r$.\n",
    "\n",
    "Demonstraremos abaixo o funcionamento deste algoritmo utilizando o problema conhecido como 'cart pole' que consiste em equilibrar uma haste acima de uma base que se move horizontalmente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.2. Ambiente</h4>\n",
    "\n",
    "O ambiente é a representação do problema que será utilizada para permitir que um algoritmo de *Deep Reinforcement Learning* aprenda a tomar decisões baseadas no estado em que o ambiente se encontra. Esse elemento é responsável por modelar a transição de estados, ou seja, como a ação realizada modifica o ambiente como um todo. \n",
    "\n",
    "Para o exemplo que adotamos, o ambiente é o jogo como um todo.\n",
    " \n",
    "O nosso ambiente será importado da biblioteca <a href=\"https://gym.openai.com/envs/CartPole-v1/\">GYM</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.3. Agente</h4>\n",
    "\n",
    "O agente é um algoritmo inteligente capaz de compreender o estado em que o ambiente se encontra e, a partir disso, decidir quais ações executadas gerarão recompensas altas. \n",
    "\n",
    "Assim, o papel do agente é aprimorar a sua política de decisão de ações baseada no estado, visando sempre encontrar a política que lhe traga o máximo retorno possível, sendo este a soma das recompensas recebidas. A política do agente é denotada como $\\mu$.\n",
    "\n",
    "No exemplo adotado, o agente faria o papel que o ser humano faz ao jogar o *game*, decidindo quais ações devem ser executadas pelo personagem (para qual lado ele deveria equilibrar a base para a haste não cair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.4. Estado</h4>\n",
    "\n",
    "O estado, denotado como $s$, é definido como a situação do ambiente em determinado instante, ou seja, como o ambiente se encontra no momento da observação feita. A representação do estado faz parte da modelagem do ambiente, já que um único problema apresenta diferentes maneiras de modelagem do vetor que representa o estado, de forma que é necessário decidir qual dessas maneiras é a mais adequada para o entendimento do agente.\n",
    "\n",
    "Como citado, não existe apenas uma modelagem do estado, mas podemos defini-lo no problema como a imagem que aparece na tela do monitor, já que é isso que o nosso cérebro utiliza para interpretar o ambiente e fazer as decisões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.5. Recompensa</h4>\n",
    "\n",
    "A recompensa é a variável responsável por avaliar o quão positiva ou negativa foi uma determinada ação executada. A partir dessa grandeza é possível ensinar a rede a decidir as ações que serão tomadas, visando sempre maximizar a recompensa recebida.\n",
    "\n",
    "Essa variável, denotada por $r$, é, geralmente, um número real, cujo valor é definido pelas chamadas funções de recompensa, que são funções responsáveis por calcular a recompensa da ação executada a partir de padrões definidos na modelagem do ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Resolução de problema</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.1. Porque não usar os modelos convencionais já conhecidos</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.2. Implementação memória do <i>replay buffer<i></h4>\n",
    "\n",
    "Foi uma ideia proposta em [1993](https://www.semanticscholar.org/paper/Reinforcement-learning-for-robots-using-neural-Lin/54c4cf3a8168c1b70f91cf78a3dc98b671935492?p2df) pois ao utilizar-se uma DNN em problemas como esse, facilmente ocorria overfitting. Para resolver esse problema nós armazenamos as experiencias incluindo transições de estado, recompensas e ações que são necessárias para o bom treinamento da rede,com essas informações serão criados mini-batches para atualizar a rede neural. Através dessa técnica nós obtemos os seguintes resultados : \n",
    "\n",
    "*   Reduz a tendencia nas atualizações da DNN\n",
    "*   Aumenta a velocidade de aprendizado com mini-batches  \n",
    "*   Reutiliza transições passadas para evitar um problema conhecido como ['catastrophic forgetting'](https://towardsdatascience.com/tagged/catastrophic-forgetting?p=4672e8843a7f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.3. Implementção <i>target</i> e <i>policy</i></h4>\n",
    "\n",
    "No cálculo do erro da Diferença Temporal a função target é mudada frequentemente, quando a função target se mostra instável, o treinamento se torna difícil. Em vista disso a técnica de 'Target Network' corrige os parâmetros da função target e atualiza a cada cem episódios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.4. <i>Exploration</i>, <i>exploitation</i> e <i>trade off</i></h4>\n",
    "\n",
    "Nessa técnica um agente tem que decidir a cada iteração se ele irá tomar a melhor decisão de acordo com a estimativa atual, ou se ele irá tomar uma decisão de \"exploração\" que não é considerada ideal segundo a estimativa. Explorar pode trazer novas informações sobre o ambiente e talvez chegar a estados que nunca foram atingidos antes e provavelmente não seriam se o algoritmo seguisse a tendência, com isso podendo chegar a resultados melhores posteriormente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Problema do cart pole</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4.1. Ambiente GYM</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas usadas \n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4.2. Implementação de uma DQN usando o GYM e o problema do cart pole</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, 64)\n",
    "        self.l2 = nn.Linear(64, 128)\n",
    "        self.l3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # Tamanho do batch\n",
    "GAMMA = 0.9  # Taxa de aprendizado\n",
    "TARGET_UPDATE = 5  # Episódios de atualização das redes target\n",
    "\n",
    "### Parâmetros de exploração ###\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 100\n",
    "\n",
    "### Preparação das redes neurais ###\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Definição do otimizador\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "memory = ReplayMemory(10000)  # Instanciamento da memória\n",
    "\n",
    "\n",
    "def select_action(state, train=True):\n",
    "    '''\n",
    "    Função que seleciona a ação que será realizada\n",
    "    '''\n",
    "    global i_episode\n",
    "\n",
    "    sample = random.random()  # Amostra de um número racional entre 0 e 1\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * i_episode /\n",
    "                 EPS_DECAY)  # Definição da probabilidade de explorar no i_episode\n",
    "    tensor_state = torch.autograd.Variable(state).type(\n",
    "        torch.FloatTensor).to(device)  # ajuste do formato do estado\n",
    "\n",
    "    if train:\n",
    "        if sample > eps_threshold:  # Explotação\n",
    "            with torch.no_grad():\n",
    "                return policy_net(tensor_state).data.max(1)[1].view(1, 1).to(device)\n",
    "        else:  # Exploração\n",
    "            return torch.LongTensor([[random.randrange(2)]]).to(device)\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(tensor_state).data.max(1)[1].view(1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    '''\n",
    "    Função utilizada para aprendizado das redes neurais\n",
    "    '''\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:  # Verifica se a memória ja tem BATCH_SIZE's amostras\n",
    "        return\n",
    "\n",
    "    # Define que as redes estão em treinamento\n",
    "    policy_net.train()\n",
    "    target_net.train()\n",
    "\n",
    "    # Amostra a memória em BATCH_SIZE's transições\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Define os tensores que serão utilizados no treinamento,\n",
    "    # agrupando os estados, ações e recompensas de todas as transições\n",
    "    batch_next_state = torch.autograd.Variable(\n",
    "        torch.cat(batch.next_state)).to(device)\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "    # Calcula os Q(s,a) a partir dos estados do batch e seleciona aqueles\n",
    "    # cujas ações foram realizadas e armazenadas no batch\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Calcula os Q(s',a') a partir dos estados futuros do batch\n",
    "    # e selecionam os de maior valor\n",
    "    with torch.no_grad():\n",
    "        next_state_values = target_net(batch_next_state).detach().max(1)[0]\n",
    "\n",
    "    # Calcula o lado direito da equação de Bellman\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Calcula a diferença entre os dois lados da equação de Bellman\n",
    "    loss = criterion(state_action_values,\n",
    "                     expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Otimiza os parâmetros da rede\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "list_retorno = []\n",
    "for i_episode in range(num_episodes):\n",
    "    # Inicializa o ambiente e coleta o estado inicial\n",
    "    state = env.reset()\n",
    "    # Inicializa as variáveis de retorno e eventos\n",
    "    retorno, steps = 0, 0\n",
    "    # Inicializa a contagem de eventos no episódio\n",
    "    for t in count():\n",
    "        env.render()  # Renderiza o ambiente\n",
    "\n",
    "        action = select_action(torch.FloatTensor([state])).to(\n",
    "            device)  # Seleciona a ação a partir do estado\n",
    "        # Executa a ação e coleta o prox estado,\n",
    "        next_state, reward, done, _ = env.step(action[0].item())\n",
    "        # a recompensa e se o episódio foi finalizado\n",
    "        # Soma a recompensa ao retorno\n",
    "        retorno += reward\n",
    "\n",
    "        # Apesar da coleta da recompensa acima, recalculamos a recompensa de outra maneira,\n",
    "        # já que o cálculo de recompensa original não é muito eficiente\n",
    "        if done:\n",
    "            if steps < 30:\n",
    "                reward -= 10\n",
    "            else:\n",
    "                reward = -1\n",
    "        if steps > 100:\n",
    "            reward += 1\n",
    "        if steps > 200:\n",
    "            reward += 1\n",
    "        if steps > 300:\n",
    "            reward += 1\n",
    "\n",
    "        # Guarda a experiência na memória\n",
    "        memory.push(torch.FloatTensor([state]),\n",
    "                    action,  # action is already a tensor\n",
    "                    torch.FloatTensor([next_state]),\n",
    "                    torch.FloatTensor([reward]).to(device))\n",
    "\n",
    "        # Move para o proximo estado e atualiza o ponteiro de eventos\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        # Otimiza a rede neural do crítico\n",
    "        optimize_model()\n",
    "        # Finaliza o episódio se tiver perdido ou após 5 mil eventos\n",
    "        if done or (steps == 2500):\n",
    "            break\n",
    "\n",
    "    # Atualiza a rede target\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # Printa o retorno do episódio e guarda em uma lista\n",
    "    print(f'Episodio {i_episode}: retorno={round(retorno,2)}')\n",
    "    list_retorno.append(retorno)\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"files/cart_pole.png\"></img>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Feedback do treinamento</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(range(1, num_episodes+1), list_retorno)\n",
    "plt.ylabel('Retorno')\n",
    "plt.xlabel('Episódios')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<style>\n",
    ".teste{\n",
    "    color:cyan;\n",
    "}\n",
    "</style>\n",
    "<h2 class=\"teste\">1. Aprendizado por reforço</h2>\n",
    "\n",
    "O aprendizado por reforço (RL - *Reinforcement Learning*) é um campo de Inteligência Artificial que tenta, através do recebimento de recompensas, ensinar um agente inteligente a decidir ações adequadas para uma determinada situação vivenciada. De maneira intuitiva, o processo é semelhante ao adestramento de um cachorro que aprende a realizar algumas tarefas a partir do recebimento de biscoitos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Conceitos Básicos\n",
    "\n",
    "Existem alguns conceitos chave para o RL que servirão como base para a maioria dos algoritmos da área, de forma que é fundamental conhecê-los para conseguir compreender as aplicações desse campo. São eles:\n",
    "\n",
    "*   Ambiente;\n",
    "*   Agente $\\mu$;\n",
    "*   Estado $s$; \n",
    "*   Recompensa $r$.\n",
    "\n",
    "Demonstraremos abaixo o funcionamento deste algoritmo utilizando o problema conhecido como 'cart pole' que consiste em equilibrar uma haste acima de uma base que se move horizontalmente. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"files/cart_pole.png\"></img>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Ambiente\n",
    "\n",
    "O ambiente é a representação do problema que será utilizada para permitir que um algoritmo de *Deep Reinforcement Learning* aprenda a tomar decisões baseadas no estado em que o ambiente se encontra. Esse elemento é responsável por modelar a transição de estados, ou seja, como a ação realizada modifica o ambiente como um todo. \n",
    "\n",
    "Para o exemplo que adotamos, o ambiente é o jogo como um todo.\n",
    " \n",
    "O nosso ambiente será importado da biblioteca <a href=\"https://gym.openai.com/envs/CartPole-v1/\">GYM</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Agente\n",
    "\n",
    "O agente é um algoritmo inteligente capaz de compreender o estado em que o ambiente se encontra e, a partir disso, decidir quais ações executadas gerarão recompensas altas. \n",
    "\n",
    "Assim, o papel do agente é aprimorar a sua política de decisão de ações baseada no estado, visando sempre encontrar a política que lhe traga o máximo retorno possível, sendo este a soma das recompensas recebidas. A política do agente é denotada como $\\mu$.\n",
    "\n",
    "No exemplo adotado, o agente faria o papel que o ser humano faz ao jogar o *game*, decidindo quais ações devem ser executadas pelo personagem (para qual lado ele deveria equilibrar a base para a haste não cair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Estado\n",
    "\n",
    "O estado, denotado como $s$, é definido como a situação do ambiente em determinado instante, ou seja, como o ambiente se encontra no momento da observação feita. A representação do estado faz parte da modelagem do ambiente, já que um único problema apresenta diferentes maneiras de modelagem do vetor que representa o estado, de forma que é necessário decidir qual dessas maneiras é a mais adequada para o entendimento do agente.\n",
    "\n",
    "Como citado, não existe apenas uma modelagem do estado, mas podemos defini-lo no problema como a imagem que aparece na tela do monitor, já que é isso que o nosso cérebro utiliza para interpretar o ambiente e fazer as decisões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Recompensa\n",
    "\n",
    "A recompensa é a variável responsável por avaliar o quão positiva ou negativa foi uma determinada ação executada. A partir dessa grandeza é possível ensinar a rede a decidir as ações que serão tomadas, visando sempre maximizar a recompensa recebida.\n",
    "\n",
    "Essa variável, denotada por $r$, é, geralmente, um número real, cujo valor é definido pelas chamadas funções de recompensa, que são funções responsáveis por calcular a recompensa da ação executada a partir de padrões definidos na modelagem do ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entendido o que é RL(*Reinforcement Learning*), agora é necessário compreender o que é DRL(*Deep Reinforcement Learning*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".teste{\n",
    "    color:cyan;</style>\n",
    "<h2 class=\"teste\">2. Aprendizagem profundamente reforçada</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O DRL é uma consequência de toda teoria de RL desenvolvida ao longo de anos. Em 2013, Volodymyr Mnih propõe utilizar uma rede neural profunda como agente de um problema de RL, [leia mais sobre](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), que até então tentava modelar os agentes utilizando, principalmente, modelos estatísticos. A aplicação do artigo, inclusive, era produzir um agente capaz de jogar *games* clássicos do console Atari.\n",
    "\n",
    "Dessa forma, o DRL surge da ideia de se incorporar o *Deep Learning* em problemas de *Reinforcement Learning*, visando resolver problemas de complexidade elevada. A partir dessa união, precisamos entender como a rede neural aprenderá a tomar decisões a partir de um estado do ambiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Equação de Bellman</h3>\n",
    "\n",
    "Para compreender o aprendizado do crítico, voltaremos à definição da variável $Q(s,a)$. Repare que utilizando as propriedades de um somatório, podemos escrever o retorno como:\n",
    "\n",
    "$$ R_t = r_t + \\sum_{k=t+1}^{T} r_k \\\\\n",
    "   R_t = r_t + R_{t+1}\n",
    " $$\n",
    "\n",
    "Nos próximos passos, não entraremos na matemática da dedução por acreditar que o seu entendimento demanda um pouco mais de tempo, mas deixamos uma [referência](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html) que faz essa dedução completa.\n",
    "\n",
    "Para nós, o importante é compreendermos a noção de que o retorno presente pode ser decomposto na recompensa atual adicionada ao retorno do próximo evento e que isso pode ser estendido para o valor do estado-ação, por ser uma variável em função do retorno. \n",
    "\n",
    "$$ Q(s,a) = r_t + Q(s',a') $$\n",
    "\n",
    "em que $s'$ é o estado futuro (no evento $t+1$) e $a'$ é a ação tomada no evento $t+1$. Entretanto, em um cenário em que tanto a transição de estados, quanto a tomada de decisão não são conhecidas, é necessário tratarmos com as **esperanças** dessas variáveis. Assim:\n",
    "\n",
    "$$ Q(s,a) = \\mathbb{E}_{s'\\mathtt{\\sim}P}\\{r_t + \\mathbb{E}_{a'\\mathtt{\\sim}\\mu}[Q(s',a')]\\} $$\n",
    "\n",
    "em que $P$ é a probabilidade de transição dos estados e $\\mu$ é a política para tomada de decisão adotada pelo ator. Essa última equação é conhecida como **Equação de Bellman**, e é a base para o aprendizado de quase todas as arquiteturas de DRL. \n",
    "\n",
    "Na maioria das referências, veremos um termo $\\gamma$ multiplicando a esperança de $Q(s',a')$, chamado de fator de desconto. Essa variável serve para o algoritmo dar mais importância às experiências recentes, em detrimento das que estão em um futuro mais distante. Nessa explicação, ele foi suprimido por simplificação.\n",
    "\n",
    "Vejamos, agora, como podemos utilizar essa equação para ensinar a rede neural do crítico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Processo de aprendizagem</h3>\n",
    "\n",
    "Pensemos no início do treinamento do algoritmo: a rede neural do crítico faz uma estimativa para o valor de $Q$, mas por ainda não estar treinada, essa estimativa não é confiável. Nesse sentido, o que precisaríamos fazer para a rede aprender a melhorar essa estimativa? Qual seria o valor esperado que utilizaríamos para calcular o custo que seria usado no otimizador da rede?\n",
    "\n",
    "Voltemos à Equação de Bellman. Repare que, se a nossa forma de calcular o $Q$ é a partir de uma rede neural, precisaríamos usá-la para calcular ambos os lados da equação:\n",
    "\n",
    "*    No primeiro, passaríamos à rede o estado e ação atuais ($s$ e $a$) como entrada e receberíamos a estimativa de $Q(s,a)$ como saída;\n",
    "*    No segundo, passaríamos à rede o estado e ação futuras ($s'$ e $a'$) como entrada e receberíamos a estimativa de $Q(s',a')$ como saída.\n",
    "\n",
    "Para completar a equação, precisaríamos, ao executar a ação $a$ no estado $s$, coletar $r_t$, que é calculada de maneira determinística pelo próprio ambiente e enviada para o algoritmo.\n",
    "\n",
    "Apesar de a equação de Bellman ser matematicamente bem definida, de forma que os dois lados da equação precisem ser iguais, ao utilizar uma rede neural para o cálculo do $Q$, estamos realizando apenas uma estimativa para essa variável, de forma que, na prática, haverá divergência entre os dois lados da equação.\n",
    "\n",
    "Mas como podemos usar tudo isso para ensinar uma rede totalmente leiga a aprender a estimar o valor de $Q$? \n",
    "\n",
    "Pense que os dois lados da equação de Bellman, calculados a partir da utilização da estimativa da rede neural, possuem grandes incertezas, principalmente no início do processo de aprendizagem. Entretanto, como foi dito, $r_t$ é uma variável coletada diretamente do ambiente, **não havendo incerteza alguma em seu valor**.\n",
    "\n",
    "Assim, graças a esse fato, podemos dizer que, utilizando redes neurais para estimar o valor de $Q$, o cálculo do lado direito da equação de Bellman possui **menos incerteza** do que o lado esquerdo da equação, que depende apenas da saída da rede.\n",
    "\n",
    "Dessa forma, é possível treinar a rede neural do crítico utilizando uma função de perda que calcule o custo considerando $Q(s,a)$ como a estimativa feita pela rede e o lado direito da equação como o valor esperado.\n",
    "\n",
    "Perceba que o valor esperado para treinar a rede depende da saída da própria rede, algo que é inesperado e contraintuitivo, mas pouco a pouco os valores de recompensa recebidos vão direcionando a rede para a convergência, de forma que as estimativas vão melhorando a cada época de treinamento.\n",
    "\n",
    "O fato apontado acima é o principal responsável pelas redes de DRL serem mais instáveis e possuírem convergência mais lenta do que as redes de ML/DL tradicionais, o que faz com que essas arquiteturas precisem ter um treinamento mais longo, além de usarem diversas técnicas para ajudar na convergência do aprendizado.\n",
    "\n",
    "A imagem abaixo ilustra o treinamento de um DDPG em um único evento. Na figura, as setas verdes significam a passagem de entradas e saídas até a estimativa dos dois lados da equação de Bellman e as setas vermelhas mostram o processo de atualização dos parâmetros das redes a partir dos custos calculados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em vista da aplicação que será utilizada de exemplo nós iremos implementar uma técnica de DRL conhecida como DQN. Ela foi escolhida pois ela supera a aprendizagem instável de outras técnicas utilizando as seguintes técnicas : \n",
    "\n",
    "*    Experience Replay\n",
    "*    Target Network\n",
    "*    Clipping Rewards\n",
    "*    Skipping Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Experience Replay</h3>\n",
    "\n",
    "Foi uma ideia proposta em [1993](https://www.semanticscholar.org/paper/Reinforcement-learning-for-robots-using-neural-Lin/54c4cf3a8168c1b70f91cf78a3dc98b671935492?p2df) pois ao utilizar-se uma DNN em problemas como esse, facilmente ocorria overfitting. Para resolver esse problema nós armazenamos as experiencias incluindo transições de estado, recompensas e ações que são necessárias para o bom treinamento da rede,com essas informações serão criados mini-batches para atualizar a rede neural. Através dessa técnica nós obtemos os seguintes resultados : \n",
    "\n",
    "*   Reduz a tendencia nas atualizações da DNN\n",
    "*   Aumenta a velocidade de aprendizado com mini-batches  \n",
    "*   Reutiliza transições passadas para evitar um problema conhecido como ['catastrophic forgetting'](https://towardsdatascience.com/tagged/catastrophic-forgetting?p=4672e8843a7f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Target Network</h3>\n",
    "\n",
    "No cálculo do erro da Diferença Temporal a função target é mudada frequentemente, quando a função target se mostra instável, o treinamento se torna difícil. Em vista disso a técnica de 'Target Network' corrige os parâmetros da função target e atualiza a cada cem episódios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Clipping Rewards</h3>\n",
    "\n",
    "Cada jogo tem uma escala de pontos diferente, como por exemplo no jogo pong que o jogador ganha 1 ponto por vencer uma partida, em contraponto, em spaceinvaders o jogador ganha de 10 a 30 pontos. Essa diferença torna o treinamento instável. Em vista disso a técnica de Clipping Rewards usa +1 como recompensa positiva e -1 como recompensa negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Skipping Frames</h3>\n",
    "\n",
    "Normalmente em ambientes de jogos, são renderizadas 60 frames por segundo, no entanto pessoas não tomam tantas ações por segundo, logo a IA não necessita calcular uma ação para cada frame. Logo a técnica de Skipping Frames consiste em a DQN calcular uma ação para cada 4 frames e usar os ultimos 4 frames como input. \n",
    "Isso reduz o custo computacional e adquire mais experiência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exploration-Exploitation trade off</h3>\n",
    "\n",
    "Nessa técnica um agente tem que decidir a cada iteração se ele irá tomar a melhor decisão de acordo com a estimativa atual, ou se ele irá tomar uma decisão de \"exploração\" que não é considerada ideal segundo a estimativa. Explorar pode trazer novas informações sobre o ambiente e talvez chegar a estados que nunca foram atingidos antes e provavelmente não seriam se o algoritmo seguisse a tendência, com isso podendo chegar a resultados melhores posteriormente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, 64)\n",
    "        self.l2 = nn.Linear(64, 128)\n",
    "        self.l3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # Tamanho do batch\n",
    "GAMMA = 0.9  # Taxa de aprendizado\n",
    "TARGET_UPDATE = 5  # Episódios de atualização das redes target\n",
    "\n",
    "### Parâmetros de exploração ###\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 100\n",
    "\n",
    "### Preparação das redes neurais ###\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Definição do otimizador\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "memory = ReplayMemory(10000)  # Instanciamento da memória\n",
    "\n",
    "\n",
    "def select_action(state, train=True):\n",
    "    '''\n",
    "    Função que seleciona a ação que será realizada\n",
    "    '''\n",
    "    global i_episode\n",
    "\n",
    "    sample = random.random()  # Amostra de um número racional entre 0 e 1\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * i_episode /\n",
    "                 EPS_DECAY)  # Definição da probabilidade de explorar no i_episode\n",
    "    tensor_state = torch.autograd.Variable(state).type(\n",
    "        torch.FloatTensor).to(device)  # ajuste do formato do estado\n",
    "\n",
    "    if train:\n",
    "        if sample > eps_threshold:  # Explotação\n",
    "            with torch.no_grad():\n",
    "                return policy_net(tensor_state).data.max(1)[1].view(1, 1).to(device)\n",
    "        else:  # Exploração\n",
    "            return torch.LongTensor([[random.randrange(2)]]).to(device)\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(tensor_state).data.max(1)[1].view(1, 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    '''\n",
    "    Função utilizada para aprendizado das redes neurais\n",
    "    '''\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:  # Verifica se a memória ja tem BATCH_SIZE's amostras\n",
    "        return\n",
    "\n",
    "    # Define que as redes estão em treinamento\n",
    "    policy_net.train()\n",
    "    target_net.train()\n",
    "\n",
    "    # Amostra a memória em BATCH_SIZE's transições\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Define os tensores que serão utilizados no treinamento,\n",
    "    # agrupando os estados, ações e recompensas de todas as transições\n",
    "    batch_next_state = torch.autograd.Variable(\n",
    "        torch.cat(batch.next_state)).to(device)\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "    # Calcula os Q(s,a) a partir dos estados do batch e seleciona aqueles\n",
    "    # cujas ações foram realizadas e armazenadas no batch\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Calcula os Q(s',a') a partir dos estados futuros do batch\n",
    "    # e selecionam os de maior valor\n",
    "    with torch.no_grad():\n",
    "        next_state_values = target_net(batch_next_state).detach().max(1)[0]\n",
    "\n",
    "    # Calcula o lado direito da equação de Bellman\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Calcula a diferença entre os dois lados da equação de Bellman\n",
    "    loss = criterion(state_action_values,\n",
    "                     expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Otimiza os parâmetros da rede\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 0: retorno=12.0\n",
      "Episodio 1: retorno=14.0\n",
      "Episodio 2: retorno=15.0\n",
      "Episodio 3: retorno=17.0\n",
      "Episodio 4: retorno=23.0\n",
      "Episodio 5: retorno=10.0\n",
      "Episodio 6: retorno=65.0\n",
      "Episodio 7: retorno=11.0\n",
      "Episodio 8: retorno=9.0\n",
      "Episodio 9: retorno=23.0\n",
      "Episodio 10: retorno=17.0\n",
      "Episodio 11: retorno=29.0\n",
      "Episodio 12: retorno=10.0\n",
      "Episodio 13: retorno=12.0\n",
      "Episodio 14: retorno=18.0\n",
      "Episodio 15: retorno=14.0\n",
      "Episodio 16: retorno=38.0\n",
      "Episodio 17: retorno=13.0\n",
      "Episodio 18: retorno=14.0\n",
      "Episodio 19: retorno=20.0\n",
      "Episodio 20: retorno=24.0\n",
      "Episodio 21: retorno=9.0\n",
      "Episodio 22: retorno=48.0\n",
      "Episodio 23: retorno=25.0\n",
      "Episodio 24: retorno=19.0\n",
      "Episodio 25: retorno=22.0\n",
      "Episodio 26: retorno=119.0\n",
      "Episodio 27: retorno=65.0\n",
      "Episodio 28: retorno=18.0\n",
      "Episodio 29: retorno=69.0\n",
      "Episodio 30: retorno=20.0\n",
      "Episodio 31: retorno=20.0\n",
      "Episodio 32: retorno=15.0\n",
      "Episodio 33: retorno=43.0\n",
      "Episodio 34: retorno=47.0\n",
      "Episodio 35: retorno=39.0\n",
      "Episodio 36: retorno=31.0\n",
      "Episodio 37: retorno=24.0\n",
      "Episodio 38: retorno=23.0\n",
      "Episodio 39: retorno=17.0\n",
      "Episodio 40: retorno=12.0\n",
      "Episodio 41: retorno=10.0\n",
      "Episodio 42: retorno=80.0\n",
      "Episodio 43: retorno=11.0\n",
      "Episodio 44: retorno=18.0\n",
      "Episodio 45: retorno=99.0\n",
      "Episodio 46: retorno=47.0\n",
      "Episodio 47: retorno=34.0\n",
      "Episodio 48: retorno=67.0\n",
      "Episodio 49: retorno=44.0\n",
      "Episodio 50: retorno=112.0\n",
      "Episodio 51: retorno=22.0\n",
      "Episodio 52: retorno=20.0\n",
      "Episodio 53: retorno=122.0\n",
      "Episodio 54: retorno=68.0\n",
      "Episodio 55: retorno=98.0\n",
      "Episodio 56: retorno=37.0\n",
      "Episodio 57: retorno=20.0\n",
      "Episodio 58: retorno=123.0\n",
      "Episodio 59: retorno=55.0\n",
      "Episodio 60: retorno=68.0\n",
      "Episodio 61: retorno=174.0\n",
      "Episodio 62: retorno=32.0\n",
      "Episodio 63: retorno=30.0\n",
      "Episodio 64: retorno=112.0\n",
      "Episodio 65: retorno=67.0\n",
      "Episodio 66: retorno=118.0\n",
      "Episodio 67: retorno=75.0\n",
      "Episodio 68: retorno=114.0\n",
      "Episodio 69: retorno=68.0\n",
      "Episodio 70: retorno=20.0\n",
      "Episodio 71: retorno=59.0\n",
      "Episodio 72: retorno=13.0\n",
      "Episodio 73: retorno=118.0\n",
      "Episodio 74: retorno=36.0\n",
      "Episodio 75: retorno=75.0\n",
      "Episodio 76: retorno=61.0\n",
      "Episodio 77: retorno=87.0\n",
      "Episodio 78: retorno=115.0\n",
      "Episodio 79: retorno=15.0\n",
      "Episodio 80: retorno=105.0\n",
      "Episodio 81: retorno=21.0\n",
      "Episodio 82: retorno=12.0\n",
      "Episodio 83: retorno=30.0\n",
      "Episodio 84: retorno=136.0\n",
      "Episodio 85: retorno=14.0\n",
      "Episodio 86: retorno=19.0\n",
      "Episodio 87: retorno=21.0\n",
      "Episodio 88: retorno=14.0\n",
      "Episodio 89: retorno=80.0\n",
      "Episodio 90: retorno=113.0\n",
      "Episodio 91: retorno=45.0\n",
      "Episodio 92: retorno=59.0\n",
      "Episodio 93: retorno=46.0\n",
      "Episodio 94: retorno=105.0\n",
      "Episodio 95: retorno=13.0\n",
      "Episodio 96: retorno=51.0\n",
      "Episodio 97: retorno=75.0\n",
      "Episodio 98: retorno=96.0\n",
      "Episodio 99: retorno=17.0\n",
      "Episodio 100: retorno=32.0\n",
      "Episodio 101: retorno=60.0\n",
      "Episodio 102: retorno=135.0\n",
      "Episodio 103: retorno=99.0\n",
      "Episodio 104: retorno=28.0\n",
      "Episodio 105: retorno=12.0\n",
      "Episodio 106: retorno=72.0\n",
      "Episodio 107: retorno=30.0\n",
      "Episodio 108: retorno=11.0\n",
      "Episodio 109: retorno=14.0\n",
      "Episodio 110: retorno=11.0\n",
      "Episodio 111: retorno=18.0\n",
      "Episodio 112: retorno=130.0\n",
      "Episodio 113: retorno=21.0\n",
      "Episodio 114: retorno=98.0\n",
      "Episodio 115: retorno=88.0\n",
      "Episodio 116: retorno=125.0\n",
      "Episodio 117: retorno=14.0\n",
      "Episodio 118: retorno=45.0\n",
      "Episodio 119: retorno=34.0\n",
      "Episodio 120: retorno=46.0\n",
      "Episodio 121: retorno=102.0\n",
      "Episodio 122: retorno=77.0\n",
      "Episodio 123: retorno=114.0\n",
      "Episodio 124: retorno=70.0\n",
      "Episodio 125: retorno=223.0\n",
      "Episodio 126: retorno=34.0\n",
      "Episodio 127: retorno=18.0\n",
      "Episodio 128: retorno=42.0\n",
      "Episodio 129: retorno=28.0\n",
      "Episodio 130: retorno=15.0\n",
      "Episodio 131: retorno=46.0\n",
      "Episodio 132: retorno=151.0\n",
      "Episodio 133: retorno=107.0\n",
      "Episodio 134: retorno=42.0\n",
      "Episodio 135: retorno=47.0\n",
      "Episodio 136: retorno=186.0\n",
      "Episodio 137: retorno=32.0\n",
      "Episodio 138: retorno=29.0\n",
      "Episodio 139: retorno=18.0\n",
      "Episodio 140: retorno=41.0\n",
      "Episodio 141: retorno=168.0\n",
      "Episodio 142: retorno=126.0\n",
      "Episodio 143: retorno=50.0\n",
      "Episodio 144: retorno=121.0\n",
      "Episodio 145: retorno=34.0\n",
      "Episodio 146: retorno=15.0\n",
      "Episodio 147: retorno=47.0\n",
      "Episodio 148: retorno=69.0\n",
      "Episodio 149: retorno=38.0\n",
      "Episodio 150: retorno=172.0\n",
      "Episodio 151: retorno=113.0\n",
      "Episodio 152: retorno=257.0\n",
      "Episodio 153: retorno=140.0\n",
      "Episodio 154: retorno=256.0\n",
      "Episodio 155: retorno=28.0\n",
      "Episodio 156: retorno=34.0\n",
      "Episodio 157: retorno=64.0\n",
      "Episodio 158: retorno=353.0\n",
      "Episodio 159: retorno=56.0\n",
      "Episodio 160: retorno=343.0\n",
      "Episodio 161: retorno=19.0\n",
      "Episodio 162: retorno=105.0\n",
      "Episodio 163: retorno=125.0\n",
      "Episodio 164: retorno=29.0\n",
      "Episodio 165: retorno=13.0\n",
      "Episodio 166: retorno=351.0\n",
      "Episodio 167: retorno=169.0\n",
      "Episodio 168: retorno=115.0\n",
      "Episodio 169: retorno=98.0\n",
      "Episodio 170: retorno=65.0\n",
      "Episodio 171: retorno=126.0\n",
      "Episodio 172: retorno=241.0\n",
      "Episodio 173: retorno=107.0\n",
      "Episodio 174: retorno=97.0\n",
      "Episodio 175: retorno=92.0\n",
      "Episodio 176: retorno=126.0\n",
      "Episodio 177: retorno=99.0\n",
      "Episodio 178: retorno=83.0\n",
      "Episodio 179: retorno=78.0\n",
      "Episodio 180: retorno=96.0\n",
      "Episodio 181: retorno=118.0\n",
      "Episodio 182: retorno=78.0\n",
      "Episodio 183: retorno=134.0\n",
      "Episodio 184: retorno=15.0\n",
      "Episodio 185: retorno=155.0\n",
      "Episodio 186: retorno=331.0\n",
      "Episodio 187: retorno=141.0\n",
      "Episodio 188: retorno=34.0\n",
      "Episodio 189: retorno=125.0\n",
      "Episodio 190: retorno=13.0\n",
      "Episodio 191: retorno=59.0\n",
      "Episodio 192: retorno=33.0\n",
      "Episodio 193: retorno=109.0\n",
      "Episodio 194: retorno=117.0\n",
      "Episodio 195: retorno=121.0\n",
      "Episodio 196: retorno=183.0\n",
      "Episodio 197: retorno=122.0\n",
      "Episodio 198: retorno=9.0\n",
      "Episodio 199: retorno=15.0\n",
      "Episodio 200: retorno=130.0\n",
      "Episodio 201: retorno=16.0\n",
      "Episodio 202: retorno=112.0\n",
      "Episodio 203: retorno=103.0\n",
      "Episodio 204: retorno=111.0\n",
      "Episodio 205: retorno=114.0\n",
      "Episodio 206: retorno=128.0\n",
      "Episodio 207: retorno=140.0\n",
      "Episodio 208: retorno=193.0\n",
      "Episodio 209: retorno=23.0\n",
      "Episodio 210: retorno=150.0\n",
      "Episodio 211: retorno=149.0\n",
      "Episodio 212: retorno=133.0\n",
      "Episodio 213: retorno=69.0\n",
      "Episodio 214: retorno=668.0\n",
      "Episodio 215: retorno=12.0\n",
      "Episodio 216: retorno=116.0\n",
      "Episodio 217: retorno=134.0\n",
      "Episodio 218: retorno=105.0\n",
      "Episodio 219: retorno=102.0\n",
      "Episodio 220: retorno=32.0\n",
      "Episodio 221: retorno=135.0\n",
      "Episodio 222: retorno=115.0\n",
      "Episodio 223: retorno=107.0\n",
      "Episodio 224: retorno=98.0\n",
      "Episodio 225: retorno=252.0\n",
      "Episodio 226: retorno=103.0\n",
      "Episodio 227: retorno=112.0\n",
      "Episodio 228: retorno=129.0\n",
      "Episodio 229: retorno=114.0\n",
      "Episodio 230: retorno=129.0\n",
      "Episodio 231: retorno=106.0\n",
      "Episodio 232: retorno=20.0\n",
      "Episodio 233: retorno=104.0\n",
      "Episodio 234: retorno=107.0\n",
      "Episodio 235: retorno=128.0\n",
      "Episodio 236: retorno=140.0\n",
      "Episodio 237: retorno=136.0\n",
      "Episodio 238: retorno=138.0\n",
      "Episodio 239: retorno=134.0\n",
      "Episodio 240: retorno=128.0\n",
      "Episodio 241: retorno=125.0\n",
      "Episodio 242: retorno=119.0\n",
      "Episodio 243: retorno=60.0\n",
      "Episodio 244: retorno=136.0\n",
      "Episodio 245: retorno=109.0\n",
      "Episodio 246: retorno=124.0\n",
      "Episodio 247: retorno=120.0\n",
      "Episodio 248: retorno=131.0\n",
      "Episodio 249: retorno=127.0\n",
      "Episodio 250: retorno=135.0\n",
      "Episodio 251: retorno=11.0\n",
      "Episodio 252: retorno=112.0\n",
      "Episodio 253: retorno=128.0\n",
      "Episodio 254: retorno=16.0\n",
      "Episodio 255: retorno=127.0\n",
      "Episodio 256: retorno=140.0\n",
      "Episodio 257: retorno=139.0\n",
      "Episodio 258: retorno=103.0\n",
      "Episodio 259: retorno=146.0\n",
      "Episodio 260: retorno=141.0\n",
      "Episodio 261: retorno=132.0\n",
      "Episodio 262: retorno=137.0\n",
      "Episodio 263: retorno=126.0\n",
      "Episodio 264: retorno=154.0\n",
      "Episodio 265: retorno=131.0\n",
      "Episodio 266: retorno=141.0\n",
      "Episodio 267: retorno=132.0\n",
      "Episodio 268: retorno=140.0\n",
      "Episodio 269: retorno=121.0\n",
      "Episodio 270: retorno=135.0\n",
      "Episodio 271: retorno=132.0\n",
      "Episodio 272: retorno=133.0\n",
      "Episodio 273: retorno=116.0\n",
      "Episodio 274: retorno=122.0\n",
      "Episodio 275: retorno=120.0\n",
      "Episodio 276: retorno=139.0\n",
      "Episodio 277: retorno=124.0\n",
      "Episodio 278: retorno=130.0\n",
      "Episodio 279: retorno=136.0\n",
      "Episodio 280: retorno=123.0\n",
      "Episodio 281: retorno=138.0\n",
      "Episodio 282: retorno=127.0\n",
      "Episodio 283: retorno=161.0\n",
      "Episodio 284: retorno=144.0\n",
      "Episodio 285: retorno=27.0\n",
      "Episodio 286: retorno=130.0\n",
      "Episodio 287: retorno=139.0\n",
      "Episodio 288: retorno=62.0\n",
      "Episodio 289: retorno=133.0\n",
      "Episodio 290: retorno=135.0\n",
      "Episodio 291: retorno=143.0\n",
      "Episodio 292: retorno=34.0\n",
      "Episodio 293: retorno=138.0\n",
      "Episodio 294: retorno=53.0\n",
      "Episodio 295: retorno=204.0\n",
      "Episodio 296: retorno=142.0\n",
      "Episodio 297: retorno=222.0\n",
      "Episodio 298: retorno=119.0\n",
      "Episodio 299: retorno=11.0\n",
      "Episodio 300: retorno=48.0\n",
      "Episodio 301: retorno=33.0\n",
      "Episodio 302: retorno=251.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 303: retorno=50.0\n",
      "Episodio 304: retorno=47.0\n",
      "Episodio 305: retorno=48.0\n",
      "Episodio 306: retorno=235.0\n",
      "Episodio 307: retorno=48.0\n",
      "Episodio 308: retorno=131.0\n",
      "Episodio 309: retorno=76.0\n",
      "Episodio 310: retorno=222.0\n",
      "Episodio 311: retorno=60.0\n",
      "Episodio 312: retorno=263.0\n",
      "Episodio 313: retorno=16.0\n",
      "Episodio 314: retorno=86.0\n",
      "Episodio 315: retorno=52.0\n",
      "Episodio 316: retorno=156.0\n",
      "Episodio 317: retorno=150.0\n",
      "Episodio 318: retorno=54.0\n",
      "Episodio 319: retorno=69.0\n",
      "Episodio 320: retorno=234.0\n",
      "Episodio 321: retorno=339.0\n",
      "Episodio 322: retorno=90.0\n",
      "Episodio 323: retorno=86.0\n",
      "Episodio 324: retorno=242.0\n",
      "Episodio 325: retorno=105.0\n",
      "Episodio 326: retorno=22.0\n",
      "Episodio 327: retorno=16.0\n",
      "Episodio 328: retorno=21.0\n",
      "Episodio 329: retorno=10.0\n",
      "Episodio 330: retorno=12.0\n",
      "Episodio 331: retorno=52.0\n",
      "Episodio 332: retorno=77.0\n",
      "Episodio 333: retorno=61.0\n",
      "Episodio 334: retorno=48.0\n",
      "Episodio 335: retorno=67.0\n",
      "Episodio 336: retorno=65.0\n",
      "Episodio 337: retorno=59.0\n",
      "Episodio 338: retorno=55.0\n",
      "Episodio 339: retorno=59.0\n",
      "Episodio 340: retorno=61.0\n",
      "Episodio 341: retorno=212.0\n",
      "Episodio 342: retorno=236.0\n",
      "Episodio 343: retorno=193.0\n",
      "Episodio 344: retorno=255.0\n",
      "Episodio 345: retorno=44.0\n",
      "Episodio 346: retorno=21.0\n",
      "Episodio 347: retorno=207.0\n",
      "Episodio 348: retorno=257.0\n",
      "Episodio 349: retorno=20.0\n",
      "Episodio 350: retorno=20.0\n",
      "Episodio 351: retorno=12.0\n",
      "Episodio 352: retorno=27.0\n",
      "Episodio 353: retorno=161.0\n",
      "Episodio 354: retorno=153.0\n",
      "Episodio 355: retorno=39.0\n",
      "Episodio 356: retorno=193.0\n",
      "Episodio 357: retorno=119.0\n",
      "Episodio 358: retorno=19.0\n",
      "Episodio 359: retorno=152.0\n",
      "Episodio 360: retorno=143.0\n",
      "Episodio 361: retorno=178.0\n",
      "Episodio 362: retorno=22.0\n",
      "Episodio 363: retorno=22.0\n",
      "Episodio 364: retorno=23.0\n",
      "Episodio 365: retorno=184.0\n",
      "Episodio 366: retorno=154.0\n",
      "Episodio 367: retorno=12.0\n",
      "Episodio 368: retorno=188.0\n",
      "Episodio 369: retorno=188.0\n",
      "Episodio 370: retorno=178.0\n",
      "Episodio 371: retorno=20.0\n",
      "Episodio 372: retorno=191.0\n",
      "Episodio 373: retorno=424.0\n",
      "Episodio 374: retorno=174.0\n",
      "Episodio 375: retorno=511.0\n",
      "Episodio 376: retorno=168.0\n",
      "Episodio 377: retorno=146.0\n",
      "Episodio 378: retorno=281.0\n",
      "Episodio 379: retorno=151.0\n",
      "Episodio 380: retorno=132.0\n",
      "Episodio 381: retorno=538.0\n",
      "Episodio 382: retorno=250.0\n",
      "Episodio 383: retorno=206.0\n",
      "Episodio 384: retorno=35.0\n",
      "Episodio 385: retorno=27.0\n",
      "Episodio 386: retorno=46.0\n",
      "Episodio 387: retorno=332.0\n",
      "Episodio 388: retorno=262.0\n",
      "Episodio 389: retorno=22.0\n",
      "Episodio 390: retorno=195.0\n",
      "Episodio 391: retorno=163.0\n",
      "Episodio 392: retorno=75.0\n",
      "Episodio 393: retorno=60.0\n",
      "Episodio 394: retorno=43.0\n",
      "Episodio 395: retorno=226.0\n",
      "Episodio 396: retorno=46.0\n",
      "Episodio 397: retorno=13.0\n",
      "Episodio 398: retorno=157.0\n",
      "Episodio 399: retorno=38.0\n",
      "Episodio 400: retorno=147.0\n",
      "Episodio 401: retorno=169.0\n",
      "Episodio 402: retorno=12.0\n",
      "Episodio 403: retorno=157.0\n",
      "Episodio 404: retorno=35.0\n",
      "Episodio 405: retorno=22.0\n",
      "Episodio 406: retorno=110.0\n",
      "Episodio 407: retorno=129.0\n",
      "Episodio 408: retorno=18.0\n",
      "Episodio 409: retorno=115.0\n",
      "Episodio 410: retorno=153.0\n",
      "Episodio 411: retorno=109.0\n",
      "Episodio 412: retorno=19.0\n",
      "Episodio 413: retorno=115.0\n",
      "Episodio 414: retorno=126.0\n",
      "Episodio 415: retorno=46.0\n",
      "Episodio 416: retorno=124.0\n",
      "Episodio 417: retorno=232.0\n",
      "Episodio 418: retorno=116.0\n",
      "Episodio 419: retorno=140.0\n",
      "Episodio 420: retorno=123.0\n",
      "Episodio 421: retorno=311.0\n",
      "Episodio 422: retorno=135.0\n",
      "Episodio 423: retorno=528.0\n",
      "Episodio 424: retorno=103.0\n",
      "Episodio 425: retorno=141.0\n",
      "Episodio 426: retorno=148.0\n",
      "Episodio 427: retorno=262.0\n",
      "Episodio 428: retorno=37.0\n",
      "Episodio 429: retorno=480.0\n",
      "Episodio 430: retorno=182.0\n",
      "Episodio 431: retorno=267.0\n",
      "Episodio 432: retorno=159.0\n",
      "Episodio 433: retorno=63.0\n",
      "Episodio 434: retorno=140.0\n",
      "Episodio 435: retorno=292.0\n",
      "Episodio 436: retorno=583.0\n",
      "Episodio 437: retorno=267.0\n",
      "Episodio 438: retorno=301.0\n",
      "Episodio 439: retorno=236.0\n",
      "Episodio 440: retorno=248.0\n",
      "Episodio 441: retorno=310.0\n",
      "Episodio 442: retorno=104.0\n",
      "Episodio 443: retorno=68.0\n",
      "Episodio 444: retorno=17.0\n",
      "Episodio 445: retorno=64.0\n",
      "Episodio 446: retorno=307.0\n",
      "Episodio 447: retorno=222.0\n",
      "Episodio 448: retorno=28.0\n",
      "Episodio 449: retorno=202.0\n",
      "Episodio 450: retorno=269.0\n",
      "Episodio 451: retorno=235.0\n",
      "Episodio 452: retorno=85.0\n",
      "Episodio 453: retorno=12.0\n",
      "Episodio 454: retorno=249.0\n",
      "Episodio 455: retorno=72.0\n",
      "Episodio 456: retorno=17.0\n",
      "Episodio 457: retorno=47.0\n",
      "Episodio 458: retorno=249.0\n",
      "Episodio 459: retorno=326.0\n",
      "Episodio 460: retorno=259.0\n",
      "Episodio 461: retorno=259.0\n",
      "Episodio 462: retorno=83.0\n",
      "Episodio 463: retorno=248.0\n",
      "Episodio 464: retorno=238.0\n",
      "Episodio 465: retorno=192.0\n",
      "Episodio 466: retorno=243.0\n",
      "Episodio 467: retorno=213.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "list_retorno = []\n",
    "for i_episode in range(num_episodes):\n",
    "    # Inicializa o ambiente e coleta o estado inicial\n",
    "    state = env.reset()\n",
    "    # Inicializa as variáveis de retorno e eventos\n",
    "    retorno, steps = 0, 0\n",
    "    # Inicializa a contagem de eventos no episódio\n",
    "    for t in count():\n",
    "        env.render()  # Renderiza o ambiente\n",
    "\n",
    "        action = select_action(torch.FloatTensor([state])).to(\n",
    "            device)  # Seleciona a ação a partir do estado\n",
    "        # Executa a ação e coleta o prox estado,\n",
    "        next_state, reward, done, _ = env.step(action[0].item())\n",
    "        # a recompensa e se o episódio foi finalizado\n",
    "        # Soma a recompensa ao retorno\n",
    "        retorno += reward\n",
    "\n",
    "        # Apesar da coleta da recompensa acima, recalculamos a recompensa de outra maneira,\n",
    "        # já que o cálculo de recompensa original não é muito eficiente\n",
    "        if done:\n",
    "            if steps < 30:\n",
    "                reward -= 10\n",
    "            else:\n",
    "                reward = -1\n",
    "        if steps > 100:\n",
    "            reward += 1\n",
    "        if steps > 200:\n",
    "            reward += 1\n",
    "        if steps > 300:\n",
    "            reward += 1\n",
    "\n",
    "        # Guarda a experiência na memória\n",
    "        memory.push(torch.FloatTensor([state]),\n",
    "                    action,  # action is already a tensor\n",
    "                    torch.FloatTensor([next_state]),\n",
    "                    torch.FloatTensor([reward]).to(device))\n",
    "\n",
    "        # Move para o proximo estado e atualiza o ponteiro de eventos\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        # Otimiza a rede neural do crítico\n",
    "        optimize_model()\n",
    "        # Finaliza o episódio se tiver perdido ou após 5 mil eventos\n",
    "        if done or (steps == 2500):\n",
    "            break\n",
    "\n",
    "    # Atualiza a rede target\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # Printa o retorno do episódio e guarda em uma lista\n",
    "    print(f'Episodio {i_episode}: retorno={round(retorno,2)}')\n",
    "    list_retorno.append(retorno)\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(range(1, num_episodes+1), list_retorno)\n",
    "plt.ylabel('Retorno')\n",
    "plt.xlabel('Episódios')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af64492de94869e60dd12c0426de147da45ea452fa6729c41b1b3a55be70ba70"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
