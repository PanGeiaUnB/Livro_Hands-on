{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lição 5 - Deep Deterministic Policy Gradient (DDPG)"
   ],
   "metadata": {
    "id": "T-836RadfIBp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introdução <a class=\"anchor\" name=\"section_1\"></a>\n",
    "\n"
   ],
   "metadata": {
    "id": "a9qS51y4fIBt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Na última lição, introduzimos a *Deep Q-Network* (DQN), a primeira estrutura do DRL, que mesclava conceitos de RL e conceitos DL. Nós mostramos que é possível encontrar uma política capaz de solucionar o problema do *cartople*, um problema complexo para os algoritmos de outras áreas. Entretanto, depois de 2013, ano de criação do DQN, vários outros algoritmos de DRL foram criados para solucionar algumas limitações que a estrutura pioneira apresentava. Nesse sentido surge o *Deep Deterministic Policy Gradient* (DDPG), um algoritmo clássico que nos fornece flexibilidade para trabalhar tanto com ações discretas (como o DQN), como com ações contínuas."
   ],
   "metadata": {
    "id": "wRpHMoGlfIBu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Índice <a class=\"anchor\" name=\"section_1.1\"></a>\n",
    "\n",
    "* [1. Introdução](#section_1)\n",
    "    * [1.1 Índice](#section_1.1)\n",
    "* [2. O problema do Pêndulo](#section_2)\n",
    "* [3. Redes do tipo Actor-Critic](#section_3)\n",
    "    * [3.1 Deep Deterministic Gradient Policy - DDPG](#section_3.1)\n",
    "    * [3.2 Estratégias de treinamento do DDPG](#section_3.2)\n",
    "        * [3.2.1 Ruído nos parâmetros para exploração](#section_3.2.1)\n",
    "* [4. Implementação do algoritmo](#section_4)\n",
    "* [5. Sugestões](#section_5)"
   ],
   "metadata": {
    "id": "IxLFMBM5fIBv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iniciaremos o problema importando as bibliotecas que usaremos ao longo da nossa implementação."
   ],
   "metadata": {
    "id": "yynkOwWKfIBv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym"
   ],
   "outputs": [],
   "metadata": {
    "id": "p7a5I4V9fIBw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Antes de iniciarmos a implementação do código de fato, vamos verificar se há alguma GPU disponível para rodar os códigos, com o intuito de tornar mais rápido o treinamento do modelo."
   ],
   "metadata": {
    "id": "vZERgxUgfIBw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def testar_gpu():\n",
    "    '''\n",
    "    Função que testa se a GPU está habilitada.\n",
    "    \n",
    "    Outputs:\n",
    "    --------\n",
    "    device: {str}\n",
    "        variável que define se o código será executado na CPU ou na GPU\n",
    "    '''\n",
    "    train_on_gpu = torch.cuda.is_available() #Observa se a GPU está disponivel\n",
    "    if train_on_gpu: #Se sim\n",
    "        device = torch.device('cuda') #Seleciona o device como GPU\n",
    "        print(\"Treinando na GPU\") #E manda a mensagem\n",
    "    else: #Se não\n",
    "        device = torch.device('cpu') #Seleciona o device como cpu\n",
    "        print(\"GPU indisponível, treinando na CPU\") #E avisa que a GPU não esta disponível\n",
    "    \n",
    "    return device\n",
    "\n",
    "#Coleta onde o ambiente será executado\n",
    "device = testar_gpu()"
   ],
   "outputs": [],
   "metadata": {
    "id": "uaY0FBhYfIBx",
    "outputId": "d3fb04ff-50b4-40e2-8aa8-856b576eac2f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como apresentado na última lição, implementaremos a definição de transição de um ambiente DRL, com estado, ação, próximo estado e recompensa."
   ],
   "metadata": {
    "id": "MTXphLiEfIBz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Define uma transição como um tupla com estado, ação, próximo estado e recompensa.\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ],
   "outputs": [],
   "metadata": {
    "id": "1AXKrjdqfIBz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. O problema do Pêndulo <a class=\"anchor\" name=\"section_2\"></a>"
   ],
   "metadata": {
    "id": "iI0Q2bBxfIB0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A última lição mostrou uma implementação de uma *Deep Q-Network*, a rede mais básica de DRL, que nos permitiu elaborar um algoritmo que conseguisse decidir quais ações devem ser tomadas pelo *cartpole*, para evitar que a haste caia. \n",
    "\n",
    "Entretanto, esse algoritmo apresenta uma clara limitação: ela só consegue resolver problemas em que as ações possíveis são **discretas**. Isso é perceptível ao perceber que a rede retorna apenas **qual** ação deve ser tomada, e não **como** ela deve ser executada. Passemos a um outro problema para que isso fique mais claro.\n",
    "\n",
    "Pense em um jogo parecido com o *cartpole*, mas agora nós queremos equilibrar um pêndulo verticalmente na posição mais alta possível. Para isso, nós podemos aplicar, no pêndulo, forças tanto no sentido horário como no anti-horário, em que a intensidade dessa força é um valor contínuo, de forma que a rotação pode ter mais ou menos intensidade, a depender da força aplicada.\n",
    "\n",
    "Assim, a rotação do pêndulo é controlada por um agente que define uma ação tal que $-2 < a < 2$, em que os valores negativos indicam que as forças são no sentido anti-horário e os positivos que são no sentido horário, enquanto o módulo do ação tal que $|a|<2$, indica a intensidade da força aplicada. A imagem abaixo mostra o pêndulo em vermelho e a representação da força aplicada como a seta preta."
   ],
   "metadata": {
    "id": "bV6WGHoTfIB0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center>\n",
    "<img src=\"files/pendulum.PNG\"></img>\n",
    "</center>"
   ],
   "metadata": {
    "id": "wVQYC6n5fIB1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Repare a ação desse ambiente é **contínua**, isto é, não basta o agente saber que precisa aplicar uma força em um ou outro sentido, ele precisa entender **o quanto** de intensidade ele deve colocar na força que será aplicada.\n",
    "\n",
    "Voltemos, então, ao DQL. Para implementar uma DQN, neste caso, precisaríamos discretizar a intensidade da força e combiná-las com o sentido de rotação, considerando que cada uma dessas combinações seja uma ação possível com um $Q(s,a)$ calculável. \n",
    "\n",
    "Perceba que essa não é uma abordagem eficaz mesmo para um problema relativamente simples como esse. Identificando essa limitação, desenvolveu-se outra arquitetura que consegue ultrapassar essa barreira."
   ],
   "metadata": {
    "id": "3jFvKhYzfIB2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make('Pendulum-v0') #Construção do ambiente do pendulo\n",
    "#env = gym.make('MountainCarContinuous-v0') #Construção do ambiente do mountain car"
   ],
   "outputs": [],
   "metadata": {
    "id": "Llj13MTafIB3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Redes do tipo Actor-Critic <a class=\"anchor\" name=\"section_3\"></a>\n",
    "\n",
    "É importante ressaltar que o termo *Actor-critic* **não** designa uma arquitetura de DRL, e sim uma característica que várias arquiteturas da área incorporaram, entre elas o A2C, A3C e o DDPG. Nesse notebook, apresentaremos e utilizaremos o DDPG.\n",
    "\n",
    "Dizer que uma arquitetura é *actor-critic*, significa dizer que ela utiliza, pelo menos, **duas** redes neurais no seu processo de aprendizagem: uma que toma a ação e outra que critica a ação tomada, daí surge o nome *actor-critic*.\n",
    "\n",
    "A rede do ator recebe o estado como entrada e produz uma saída, podendo ser um número ou um vetor. Após a tomada de decisão do ator, o crítico vai estimar o valor de $Q(s,a)$ a partir do estado e dessa ação realizada, tentando avaliar se a ação tomada foi boa ou ruim.\n",
    "\n",
    "Assim, percebemos que essas arquiteturas desagregam o tomador de decisão e o estimador do $Q(s,a)$, de forma que a saída do ator pode ser de natureza contínua, cabendo ao crítico realizar a estimativa do retorno a partir dessa ação tomada. Entendido isso, precisamos entender como as duas redes aprendem."
   ],
   "metadata": {
    "id": "VAqHqKQlfIB3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Deep Deterministic Gradient Policy - DDPG <a class=\"anchor\" name=\"section_3.1\"></a>\n",
    "\n",
    "Repare que o problema para o agente ainda é o mesmo: precisamos desenvolver um algoritmo capaz de maximizar o retorno. Nesse cenário, como treinar o ator?\n",
    "\n",
    "Como o $Q$ é uma estimativa do retorno recebido, é possível treinar o ator através da saída do crítico: se a saída deste for alta, o ator entenderá que agiu de maneira adequada, caso contrário, o ator terá que mudar a sua política de decisão de ação, modificando os seus parâmetros.\n",
    "\n",
    "Certo, mas como treinaremos o crítico para que ele consiga fazer estimativas de $Q(s,a)$ de maneira acurada, de forma que o ator possa \"confiar\" nessa estimativa para atualizar os seus pesos? A resposta é a mesma dada para o aprendizado da DQN, a partir da **Equação de Bellman**:\n",
    "\n",
    "$$ Q(s,a) = \\mathbb{E}_{s'\\mathtt{\\sim}P}\\{r_t + \\mathbb{E}_{a'\\mathtt{\\sim}\\mu}[Q(s',a')]\\} $$\n",
    "\n",
    "Rememorando essa equação, ela nos mostra que o valor do par estado-ação atual pode ser estimado através do valor do par estado-ação futuro, somado a uma recompensa. Apesar de ambos os lados dependerem de estimativas do $Q$, o fato de não haver incerteza na medição da recompensa faz com que o lado direito da equação seja mais acurado, de forma que ele é tratado como o valor alvo para a otimização.\n",
    "\n",
    "Assim podemos resumir o processo de aprendizado utilizando a imagem abaixo. Na figura, as setas verdes significam a passagem de entradas e saídas até a estimativa dos dois lados da equação de Bellman e as setas vermelhas mostram o processo de atualização dos parâmetros das redes a partir dos custos calculados.\n"
   ],
   "metadata": {
    "id": "NaaUmBeMfIB4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center>\n",
    "<img src=\"files/esquematico_DDPG.PNG\"></img>\n",
    "</center>"
   ],
   "metadata": {
    "id": "sImaaTEzfIB5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Na imagem acima, a função de custo do crítico ilustrada pode ser de diferentes naturezas. A mais clássica para identificar a diferença entre duas grandezas é o Erro Quadrático Médio (*Mean Squared Error* - MSE) e é ela que será utilizada. \n",
    "\n",
    "Para atualizar o ator, nós queremos promover o **aumento** do $Q(s',a')$ a cada ação executada, estimulando ações com alto valor do estado-ação. Pensando então no processo de aprendizado, sabemos que a maioria dos algoritmos se baseiam na **diminuição** do gradiente da perda, como adequar o problema?\n",
    "\n",
    "Uma solução é utilizar a perda como $-Q(s',a')$, de forma que, quanto maior o $Q$, menor a função de perda $-Q$ e, consequentemente, menor serão as atualizações dos parâmetros do ator. Em um caso contrário, a rede entenderá que as suas ações estão distantes do objetivo e que a política de tomada das ações deve ser modificada.\n",
    "\n",
    "Para entender mais sobre o processo de treinamento do DDPG, o pseudo-código pode ser encontrado neste [link](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b)."
   ],
   "metadata": {
    "id": "kQ0By3RhfIB5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Estratégias de treinamento do DDPG <a class=\"anchor\" name=\"section_3.2\"></a>\n",
    "\n",
    "Como foi dito, o treinamento das arquiteturas de DRL é um processo instável e, por isso, adotam-se algumas técnicas para tentar diminuir essa instabilidade. Entre essas técnicas estão:\n",
    "\n",
    "*    Experience Replay\n",
    "*    Target Network\n",
    "*    Utilização de ruído nos parâmetros\n",
    "\n",
    "As duas primeiras estratégias já foram explicadas na última lição, mas a última não, então faremos uma breve introdução."
   ],
   "metadata": {
    "id": "PK-KgbJTfIB6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.1 Ruído nos parâmetros para exploração <a class=\"anchor\" name=\"section_3.2.1\"></a>\n",
    "\n",
    "Como foi visto, em problemas com ações discretas, é possível realizar a exploração do ambiente escolhendo algumas ações de maneira aleatória (utilizando a técniga do $\\epsilon$-greedy, por exemplo). Já em problemas com ações contínuas, o mais comum é que se adicione ruído gaussiano na saída da rede neural, variando essas ações de maneira aleatória, como mostrado nessa [publicação](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b), na seção \"*Exploration*\".\n",
    "\n",
    "Entretanto, recentemente surgiu uma nova abordagem para essa exploração. Propôs-se que, em vez de adicionar o mecanismo de exploração após o funcionamento da rede, é possível adicionar esse mecanismo **durante** a atuação da rede neural.\n",
    "\n",
    "A ideia é que nós adicionemos um ruído aleatório nos parâmetros da rede, alterando todos os pesos desta. Essa aleatoriedade garante que a rede tome ações diferentes mesmo em situações parecidas, trazendo a ideia de exploração. Essa [tese de mestrado](https://matthiasplappert.com/publications/2017_Plappert_Master-thesis.pdf) abordou o assunto de forma bastante simples e completa. Será essa a estratégia utilizada no nosso código.\n",
    "\n",
    "Basicamente, a proposta é, a cada episódio, contaminar os parâmetros da rede neural com amostras de um ruído gaussiano de média 0 e desvio padrão $\\sigma$. Essas amostras são geradas aleatoriamente a cada episódio, respeitando a distribuição gaussiana em questão. A imagem abaixo, retirada da tese citada acima, ilustra a diferença entre o ruído na ação e o ruído nos parâmetros."
   ],
   "metadata": {
    "id": "xNsc4J9AfIB7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center>\n",
    "<img src=\"files/parameter_noise.PNG\" width=600 height=200></img>\n",
    "</center>"
   ],
   "metadata": {
    "id": "QLcBaJ6jfIB7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Essa estratégia também é adaptativa, isto é, ela muda as configurações dos parâmetros de ruído, para aumentar ou diminuir o efeito exploratório. De maneira prática, com o passar dos episódios, o desvio padrão $\\sigma$ do ruído é modificado, aumentando o seu valor quando se deseja aumentar a exploração e diminuindo o seu valor quando se deseja diminuir a exploração.\n",
    "\n",
    "Essa decisão é feita analisando se a diferença entre as decisões feitas com e sem ruído é maior do que um certo limiar de decisão $\\delta$, de forma que, se for maior, o $\\sigma$ deve ser diminuído, se for menor, o $\\sigma$ deve ser aumentado. A diferença entre essas ações ruidosas e limpas geralmente é calculada utilizando o Erro Quadrático Médio, como mostrado na imagem abaixo com a função $d(\\cdot,\\cdot)$."
   ],
   "metadata": {
    "id": "jvSqYZNSfIB8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center>\n",
    "<img src=\"files/distance.PNG\" width=500 height=100></img>\n",
    "</center>"
   ],
   "metadata": {
    "id": "XyjJZZdGfIB8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dessa forma, a adaptação no parâmetro $\\sigma$ é feita a partir de um certo $\\alpha$ maior que $1$, geralmente com um valor ligeiramente maior do que 1. Essa adaptação é feita a partir do equacionamento abaixo:\n",
    "\n",
    "$$ \\sigma_{t+1} = \\left\\{\\begin{array}{ll}\n",
    "\\alpha \\sigma_t, \\:\\:\\:\\: d(\\pi(a|s), \\tilde{\\pi}(a|s)) \\leq \\delta \\\\  \n",
    "\\frac{1}{\\alpha} \\sigma_t,\\:\\:\\:\\: cc \\\\\n",
    "\\end{array} \\right. $$\n",
    "\n",
    "em que $d(\\cdot,\\cdot)$ é uma função de cálculo de distância entre vetores, $\\pi(a|s)$ são as ações determinadas pela rede não contaminada com ruído, $\\tilde{\\pi}(a|s)$ são as ações determinadas pela rede com o ruído nor parâmetros, $\\delta$ é o limiar de decisão de adaptação e $\\alpha$ é o parâmetro de adaptação.\n",
    "\n",
    "Na prática, nós criamos uma outra rede neural auxiliar que será sempre uma cópia da rede neural do agente, mas com os parâmetros contaminados, e utilizamos ela para a tomada de decisões no processo de treinamento. Entretanto, na hora de otimizar as redes, o algoritmo otimiza a rede neural **não** contaminada, atualizando seus parâmetros e copiando-os novamente nessa rede auxiliar, que será novamente contaminada com um ruído aleatório para o próximo episódio.\n",
    "\n",
    "Seguindo esse processo e o equacionamento mostrado, o algoritmo atualiza o valor de $\\sigma$ do ruído a cada $T_{adapt}$ episódios, garantindo uma exploração mais inteligente e duradoura. "
   ],
   "metadata": {
    "id": "TgkeS_nrfIB9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A partir do que foi explanado acima, podemos partir pra implementação do nosso código"
   ],
   "metadata": {
    "id": "Y5mdUSfefIB9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Implementação do algoritmo <a class=\"anchor\" name=\"section_4\"></a>"
   ],
   "metadata": {
    "id": "d5Z8HjWZfIB9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iniciaremos o problema implementando a memória do *replay buffer*. A implementação será a mesma utilizada na última lição."
   ],
   "metadata": {
    "id": "678mkNsmfIB-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        '''\n",
    "        Classe que define a memória do Replay Buffer\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        capacity: {int}\n",
    "            número de elementos que podem ser armazenados na memória.\n",
    "        '''\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        '''\n",
    "        Função que salva uma transição na memória\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        *args: {collections.namedtuple}\n",
    "            transições que devem ser armazenadas na memória.\n",
    "        ''' \n",
    "        if len(self.memory) < self.capacity: #Checa se a memória está cheia e, se não estiver,\n",
    "            self.memory.append(None)         # cria um espaço vazio em que a transição será colocada\n",
    "        \n",
    "        self.memory[self.position] = Transition(*args) #armazena na memória usando o ponteiro self.position\n",
    "        self.position = (self.position + 1) % self.capacity #atualiza o ponteiro\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Função que amostra a memória, pegando batch_size's transições aleatoriamente.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        batch_size: {int}\n",
    "            Número de transições que serão coletadas na amostra.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        random.sample(self.memory, batch_size): {list}\n",
    "            lista com batch_size's transições, escolhidas aleatoriamente.\n",
    "        '''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Função que coleta o número de transições guardadas na memória.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        len(self.memory): {int}\n",
    "            Número de transições armazenadas.\n",
    "        '''\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {
    "id": "pOipt3WefIB-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Após a implementação da memória, podemos implementar as redes neurais do agente e do crítico. Elas são duas DNNs simples, constituídas por duas camadas lineares e uma camada de saída. Esperamos que você já tenha familiaridade com esse tipo de implementação."
   ],
   "metadata": {
    "id": "vuBWPm-EfIB_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, outputs):\n",
    "        '''\n",
    "        Classe que define a rede neural do agente do algoritmo.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        input_size: {int}\n",
    "            tamanho da camada de entrada da rede;\n",
    "        outputs: {int}\n",
    "            tamanho da camada de saída da rede\n",
    "        '''\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        #Define a primeira camada linear com normalização\n",
    "        self.linear1 = nn.Linear(input_size, 32)\n",
    "        self.ln1 = nn.LayerNorm(32)\n",
    "\n",
    "        #Define a segunda camada linear com normalização\n",
    "        self.linear2 = nn.Linear(32, 64)\n",
    "        self.ln2 = nn.LayerNorm(64)\n",
    "\n",
    "        #Define a camada de saída\n",
    "        self.linear3 = nn.Linear(64, outputs)\n",
    "        \n",
    "        self.tanh = nn.Tanh() #Definição da função de ativação tangente hiperbólica.\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Função que recebe o estado e decide a ação seguindo a sua política.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        state: {torch.tensor}\n",
    "            tensor que representa o estado do ambiente.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        action: {torch.tensor}\n",
    "            tensor com a ação decidida.\n",
    "        '''\n",
    "        out = nn.functional.relu(self.ln1(self.linear1(state))) #Execução da primeira camada\n",
    "        out = nn.functional.relu(self.ln2(self.linear2(out))) #Execução da segunda camada\n",
    "        action = self.tanh(self.linear3(out)) #Execução da camada de saída\n",
    "        \n",
    "        return action"
   ],
   "outputs": [],
   "metadata": {
    "id": "v9onzwlIfIB_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, outputs):\n",
    "        '''\n",
    "        Classe que define a rede neural do crítico do algoritmo.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        state_size: {int}\n",
    "            tamanho do estado do ambiente;\n",
    "        action_size: {int}\n",
    "            tamanho da ação do ambiente;    \n",
    "        outputs: {int}\n",
    "            tamanho da camada de saída da rede\n",
    "        '''\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        #Define a primeira camada linear com normalização\n",
    "        self.linear1 = nn.Linear(state_size, 32)\n",
    "        self.ln1 = nn.LayerNorm(32)\n",
    "\n",
    "        #Define a segunda camada linear com normalização\n",
    "        self.linear2 = nn.Linear(32 + action_size, 64) #A ação é adicionada apenas nessa segunda camada\n",
    "        self.ln2 = nn.LayerNorm(64)\n",
    "\n",
    "        #Define a camada de saída\n",
    "        self.linear3 = nn.Linear(64, outputs)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        '''\n",
    "        Função que recebe o estado e a ação e retorna o valor do estado-ação, o Q(s,a).\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        state: {torch.tensor}\n",
    "            tensor que representa o estado do ambiente.\n",
    "        action: {torch.tensor}\n",
    "            tensor que representa a ação realizada.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        Q: {torch.tensor}\n",
    "            tensor o valor do estado-ação, o Q(s,a).\n",
    "        '''\n",
    "        s1 = nn.functional.relu(self.ln1(self.linear1(state))) #Execução da primeira camada apenas com o estado\n",
    "        out = torch.cat((s1,action), dim=1) #Concatenação da saída da primeira camada com a ação realizada\n",
    "        out = nn.functional.relu(self.ln2(self.linear2(out))) #Execução da segunda camada\n",
    "        Q = self.linear3(out) #Execução da camada de saída\n",
    "        \n",
    "        return Q\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "5DjQQGHffICA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalizada a implementação da memória e das redes neurais, precisamos definir as configurações do nosso algoritmo. Primeiro, faremos uma investigação no nosso ambiente, analisando qual é o tamanho do estado, das ações esperadas e em qual intervalo a ação deve estar contida. Nas linhas de código abaixo guardamos essas informações."
   ],
   "metadata": {
    "id": "NthyGK_DfICA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_size = int(env.observation_space.shape[0]) #shape do estado\n",
    "output_size = int(env.action_space.shape[0]) #shape da ação\n",
    "max_action = float(env.action_space.high[0]) #range válido para a ação\n",
    "input_size, output_size, max_action"
   ],
   "outputs": [],
   "metadata": {
    "id": "bQBgrO-afICA",
    "outputId": "8326c3d4-3af2-4fc6-ccd9-4ee4dda2c11d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Além da configuração do ambiente, também precisamos configurar o nosso algoritmo. Na célula abaixo, nós definiremos diversos parâmetros importantes para o treinamento. Começamos com os mais tradicionais."
   ],
   "metadata": {
    "id": "XiN29TZzfICB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 64 #Tamanho do batch\n",
    "GAMMA = 0.95 #Taxa de aprendizado\n",
    "tau = 0.05 #Parâmetro de suavização da sincronização"
   ],
   "outputs": [],
   "metadata": {
    "id": "GV_7c-eBfICB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora, precisamos configurar nossos mecanismos de exploração. Nesse caso, nós disponibilizamos uma implementação do ruído nos parâmetros e uma do ruído na ação. Dessa forma, será possível comparar ambas as estratégias. As configurações estão disponíveis abaixo."
   ],
   "metadata": {
    "id": "ZAs14N1rfICB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Configuração do ruído no parâmetro\n",
    "sigma = 0.25 #Desvio padrão do ruído gaussiano que será inserido nos parâmetros\n",
    "alfa = 1.03 #Parâmetro de adaptação do ruído nos parâmetros\n",
    "delta = 1e-1 #Limiar de decisão de correção do ruído nos parâmetros\n",
    "T_adapt = 5 #Número de episódios entre as atualizações do ruído nos parâmetros\n",
    "\n",
    "# Configuração do ruído na ação\n",
    "desv_pad_ac = 0.5 #Desvio padrão do ruído gaussiano que será inserido na ação\n",
    "alfa_ac = 1.08 #Parâmetro de adaptação do ruído na ação\n",
    "T_adapt_ac = 5 #Número de episódios entre as atualizações do ruído na ação"
   ],
   "outputs": [],
   "metadata": {
    "id": "Adhwb4bgfICC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por fim, devemos instanciar as nossas redes neurais, sincronizá-las e configurar os otimizadores e funções de perda para aprendizado das redes. Tudo isso está sendo feito na célula abaixo, além do instanciamento da memória, com capacidade de armazenamento de 10 mil transições."
   ],
   "metadata": {
    "id": "TUhDhxmpfICC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Definição das redes do agente, incluindo a target e a utilizada para implementação do ruído nos parâmetros\n",
    "actor_net = Actor(input_size, output_size).to(device)\n",
    "actor_noise_net = Actor(input_size, output_size).to(device)\n",
    "actor_target_net = Actor(input_size, output_size).to(device)\n",
    "\n",
    "# Definição das redes do crítico, incluindo a target\n",
    "critic_net = Critic(input_size, output_size, 1).to(device)\n",
    "critic_target_net = Critic(input_size, output_size, 1).to(device)\n",
    "\n",
    "# Sincronização das redes\n",
    "actor_target_net.load_state_dict(actor_net.state_dict())\n",
    "actor_target_net.eval()\n",
    "actor_noise_net.load_state_dict(actor_net.state_dict())\n",
    "actor_noise_net.eval()\n",
    "critic_target_net.load_state_dict(critic_net.state_dict())\n",
    "critic_target_net.eval()\n",
    "\n",
    "# Definição dos otimizadores\n",
    "optimizer_critic = torch.optim.Adam(critic_net.parameters(), lr=0.001)\n",
    "optimizer_actor = torch.optim.Adam(actor_net.parameters(), lr=0.0001)\n",
    "\n",
    "# Instanciamento da memória\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Definição das funções de perda\n",
    "loss_critic = nn.MSELoss() #Função de perda para otimização do crítico\n",
    "loss_d = nn.MSELoss() #Função para ajuste dos parâmetros para utilização do ruído nos parâmetros para exploração\n",
    "\n",
    "# Inicialização das variáveis de perda\n",
    "actor_loss = None\n",
    "critic_loss = None"
   ],
   "outputs": [],
   "metadata": {
    "id": "aQLevwMWfICC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A partir de todas essas configurações, podemos definir a função responsável por selecionar a ação que deve ser executada, levando em conta tanto a política, como a estratégia de exploração adotada. Essa função está implementada abaixo."
   ],
   "metadata": {
    "id": "iehjIqFafICD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def select_action(state, action_noise=None, param_noise=None):\n",
    "    '''\n",
    "    Função que recebe o estado e decide a ação, levando em conta a política e o mecanismo de exploração.\n",
    "        \n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    state: {torch.tensor}\n",
    "        tensor que representa o estado do ambiente.\n",
    "    action_noise: {boolean}\n",
    "        flag que define se utilizaremos os ruídos na ação.\n",
    "    param_noise: {boolean}\n",
    "        flag que define se utilizaremos os ruídos nos parâmetros.\n",
    "       \n",
    "    Outputs:\n",
    "    --------\n",
    "    mu: {torch.tensor}\n",
    "        tensor com a ação escolhida pela política.\n",
    "    '''\n",
    "    #Definição das redes em estado de avaliação e não de treino\n",
    "    actor_net.eval()\n",
    "    actor_noise_net.eval()\n",
    "    \n",
    "    if param_noise is not None:  #Se usar o ruído nos parâmetros,\n",
    "        mu = actor_noise_net(state.float()) # utilize a rede ruidosa\n",
    "    else: #Se não usar\n",
    "        mu = actor_net(state.float()) # utilize a rede sem ruído\n",
    "    \n",
    "    mu = mu.data #Correção no formato do tensor\n",
    "    \n",
    "    if action_noise is not None: #Se usar o ruído na ação,\n",
    "        with torch.no_grad(): \n",
    "            # soma o ruído na saída da rede\n",
    "            mu = torch.Tensor(mu.cpu().numpy() + np.random.normal(0,desv_pad_ac,output_size)).to(device) \n",
    "    \n",
    "    mu = mu.squeeze() #corrige o formato do tensor\n",
    "    mu = max_action*mu.clamp(min=-1,max=1) #multiplica a ação no range válido\n",
    "    \n",
    "    return mu"
   ],
   "outputs": [],
   "metadata": {
    "id": "d7f0NBZRfICD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Também definimos uma função responsável por fazer a sincronização suavizada entre os parâmetros de duas redes neurais."
   ],
   "metadata": {
    "id": "YFY164UzfICE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def weightSync(target_model, source_model):\n",
    "    '''\n",
    "    Função que executa a sincronização das redes de maneira suavizada\n",
    "    \n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    target_model: {torch.nn}\n",
    "        rede neural que receberá os parâmetros da rede source_model de maneira suavizada.\n",
    "    source_model: {torch.nn}\n",
    "        rede neural que fornecerá os parâmetros para a outra rede.\n",
    "    '''\n",
    "    #Copia os parâmetros, suavizando a sincronização\n",
    "    for parameter_target, parameter_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        parameter_target.data.copy_((1 - tau) * parameter_target.data + tau * parameter_source.data)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "q1u5GCSIfICE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por fim, devemos implementar a função de otimização das redes neurais, onde a equação de Bellman será implementada e as redes serão atualizadas."
   ],
   "metadata": {
    "id": "xNwY9gSvfICF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def optimize_model():\n",
    "    '''\n",
    "    Função utilizada para aprendizado das redes neurais\n",
    "        \n",
    "    Outputs:\n",
    "    --------\n",
    "    state_batch: {torch.tensor}\n",
    "        batch com os tensores que representam os estados.\n",
    "    action_batch: {torch.tensor}\n",
    "        batch com os tensores que representam as ações.\n",
    "    OBS: essas saídas servem para, posteriormente, ajustar a configuração do ruído nos parâmetros\n",
    "    '''\n",
    "    if len(memory) < BATCH_SIZE: #Verifica se a memória ja tem BATCH_SIZE's amostras\n",
    "        return\n",
    "    \n",
    "    #Define que as redes estão em treinamento\n",
    "    actor_net.train()\n",
    "    critic_net.train()\n",
    "    actor_noise_net.train()\n",
    "    \n",
    "    #Amostra a memória em BATCH_SIZE's transições\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    #Define os tensores que serão utilizados no treinamento, \n",
    "    # agrupando os estados, ações e recompensas de todas as transições\n",
    "    next_state_batch = torch.autograd.Variable(torch.cat(batch.next_state)).to(device)\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = []\n",
    "    for tupla in batch.action:\n",
    "        action_batch.append(np.array(tupla.cpu()))\n",
    "    action_batch = torch.from_numpy(np.array(action_batch)).view(-1,1).to(device)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    #Calcula as ações a partir da rede target recebendo os estados vivenciados\n",
    "    next_actor_action_value = actor_target_net(next_state_batch.float())\n",
    "    #Calcula os Q(s,a) a partir dos estados e das ações calculadas pela rede agente do target\n",
    "    with torch.no_grad():\n",
    "        next_state_values = critic_target_net(next_state_batch.float(), next_actor_action_value.float()).squeeze()\n",
    "\n",
    "    #Calcula o lado direito da equação de Bellman\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    ### Otimização do crítico ###\n",
    "    optimizer_critic.zero_grad()\n",
    "    #Cálculo do Q usando a rede que NÃO é target e as ações decididas originalmente\n",
    "    state_action_values = critic_net(state_batch.float(), action_batch.float())\n",
    "    #Calcula a diferença entre os dois lados da equação de Bellman\n",
    "    critic_loss = loss_critic(state_action_values.float().squeeze(), expected_state_action_values.float())\n",
    "    #Otimiza os parâmetros da rede\n",
    "    critic_loss.backward()\n",
    "    optimizer_critic.step()\n",
    "    ###  ###\n",
    "    \n",
    "    ### Otimização do agente ###\n",
    "    optimizer_actor.zero_grad()\n",
    "    #Cálculo do -Q usando a rede que NÃO é target e as decisões do agente que NÃO é target\n",
    "    gradient = -critic_net(state_batch.float(), actor_net(state_batch.float()).float()) \n",
    "    #Calculo da função de perda como a média dos -Q estimados na linha acima\n",
    "    actor_loss = torch.mean(gradient)\n",
    "    #Otimiza os parâmetros da rede\n",
    "    actor_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    ###  ###\n",
    "    \n",
    "    #Sincronização suavizada entre redes originais e redes target\n",
    "    weightSync(actor_target_net, actor_net)\n",
    "    weightSync(critic_target_net, critic_net)\n",
    "    \n",
    "    return state_batch, action_batch"
   ],
   "outputs": [],
   "metadata": {
    "id": "C810SZ4CfICF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Com todos os elementos configurados e todas as funções necessárias definidas, podemos realizar o processo de treinamento. Ele será feito em 500 episódios e, inicialmente, utilizaremos o ruído nos parâmetros para exploração. O retorno por episódio será armazenado para ser plotado, posteriormente."
   ],
   "metadata": {
    "id": "51Scu4gYfICG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "Rotina de treinamento do algoritmo\r\n",
    "'''\r\n",
    "num_episodes = 500\r\n",
    "list_retorno = []\r\n",
    "param_noise, ac_noise = None, True\r\n",
    "for i_episode in range(num_episodes):\r\n",
    "    #Inicializa o ambiente e coleta o estado inicial\r\n",
    "    state = env.reset()\r\n",
    "    \r\n",
    "    #Inicializa as variáveis de retorno e número de eventos\r\n",
    "    retorno, steps = 0,0\r\n",
    "    \r\n",
    "    #Loop que adiciona o ruído nos parâmetros da rede actor_noise_net()\r\n",
    "    for parameter_ruido, parameter_original in zip(actor_noise_net.parameters(), actor_net.parameters()):\r\n",
    "        parameter_ruido.data.copy_(parameter_original.data + sigma*torch.randn(parameter_original.data.shape).to(device))\r\n",
    "\r\n",
    "    #Inicializa a contagem de eventos no episódio\r\n",
    "    for t in count():\r\n",
    "        env.render() #Renderiza o ambiente\r\n",
    "        \r\n",
    "        action = select_action(torch.FloatTensor([state]).to(device), ac_noise, param_noise).cpu() #Seleciona a ação \r\n",
    "                                                                                                   # a partir do estado\r\n",
    "        next_state, reward, done, _ = env.step([action]) #Executa a ação e coleta o prox estado, \r\n",
    "                                                         # a recompensa e se o episódio foi finalizado\r\n",
    "        reward = reward/10 #Ajusta o valor da recompensa\r\n",
    "        \r\n",
    "        #Soma a recompensa ao retorno\r\n",
    "        retorno += reward\r\n",
    "\r\n",
    "        #Guarda a experiência na memória\r\n",
    "        memory.push(torch.FloatTensor([state]),\r\n",
    "                     action.to(device),\r\n",
    "                     torch.FloatTensor([next_state]),\r\n",
    "                     torch.FloatTensor([reward]).to(device))\r\n",
    "\r\n",
    "        #Move para o proximo estado e atualiza o ponteiro de eventos\r\n",
    "        state = next_state\r\n",
    "        steps += 1\r\n",
    "        \r\n",
    "        #Otimiza a rede neural do crítico\r\n",
    "        if optimize_model() != None:\r\n",
    "            state_batch, action_batch = optimize_model()\r\n",
    "        if done:\r\n",
    "            break\r\n",
    "    \r\n",
    "    # Se estiver usando o ruído na ação\r\n",
    "    if ac_noise == True:\r\n",
    "        if (i_episode+1) % T_adapt_ac == 0: #Checa se está no período de adaptação\r\n",
    "            desv_pad_ac = desv_pad_ac/alfa_ac #Adapta o desvio padrão do ruído que será inserido na ação\r\n",
    "            print('\\n Sigma: ', desv_pad_ac)\r\n",
    "            \r\n",
    "    # Se estiver usando o ruído nos parâmetros\r\n",
    "    if param_noise == True:\r\n",
    "        if (i_episode+1) % T_adapt == 0: #Checa se está no período de adaptação\r\n",
    "            unpertubed_action = select_action(state_batch) #Decide as ações sem nenhum ruído\r\n",
    "            perturbed_actions = action_batch #Guarda as ações geradas pelas redes ruidosas\r\n",
    "            \r\n",
    "            d = loss_d(unpertubed_action, perturbed_actions) #Calcula a diferença quadrática \r\n",
    "                                                             # entre as ações ruidosas e não ruidosas\r\n",
    "            d = torch.mean(d) #Calcula a média no batch\r\n",
    "            \r\n",
    "            if d <= delta: #Se a distância for MENOR do que o limiar de decisão\r\n",
    "                sigma = sigma*alfa #Aumenta o desvio padrão do ruído\r\n",
    "            else: #Se a distância for MAIOR do que o limiar de decisão\r\n",
    "                sigma = sigma/alfa #Diminui o desvio padrão do ruído\r\n",
    "            \r\n",
    "            print(\" \\nD: \", d.item(), '   Sigma: ', sigma)\r\n",
    "    \r\n",
    "    #Printa o retorno do episódio e guarda em uma lista\r\n",
    "    print(f'Episodio {i_episode}: retorno={round(retorno,2)}')\r\n",
    "    list_retorno.append(retorno)\r\n",
    "\r\n",
    "print('Complete')\r\n",
    "env.render()\r\n",
    "env.close()"
   ],
   "outputs": [],
   "metadata": {
    "id": "vLIW3E_NfICG",
    "outputId": "283f841a-46b2-4ae8-d1a0-640ed19640cd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(list_retorno)\r\n",
    "plt.title('Retorno por episódio')\r\n",
    "plt.xlabel('Episódio')\r\n",
    "plt.ylabel('Retorno')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "JdlCNcEhfICH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Sugestões <a class=\"anchor\" name=\"section_5\"></a>"
   ],
   "metadata": {
    "id": "nEJoqwKZfICH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Altere a estrutura da rede neural e veja quais são as mudanças ocorridas.\n",
    "2. Altere a técnica de exploração para a utilização do ruído na ação, setando as variáveis para `action_noise==True` e `param_noise==None`. Altere os parâmetros da técnica, se necessário. Tente comparar o desempenho de ambos os métodos.\n",
    "3. Descomente a construção do ambiente `MountainCarContinuous-v0` presente na célula da Seção 2 em que importamos o ambiente do pêndulo e veja o algoritmo funcionando em um outro problema.\n",
    "4. Repita o processo de alterar o mecanismo de exploração. É esperado que nesse segundo problema, a técnica de exploração escolhida faça mais diferença do que no primeiro, devido às próprias características do ambiente.\n",
    "5. Perceba que tanto o agente quanto o crítico são DNNs, mas isso não é uma obrigação. Essas estruturas podem ser substituídas por CNNs, por exemplo, a depender do problema. A biblioteca `gym` oferece diferentes ambientes, dentre os quais vários possuem representação do estado sendo uma imagem, simulando a visão de um agente. [Nesse link](https://gym.openai.com/envs/FetchReach-v0/) há um exemplo de um problema desse tipo, tente implementar um agente que utiliza uma CNN para atuar nesse ambiente (ou em outro de sua preferência)."
   ],
   "metadata": {
    "id": "_XisgbVOfICH"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DDPG.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}