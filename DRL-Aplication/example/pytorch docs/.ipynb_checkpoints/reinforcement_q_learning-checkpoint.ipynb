{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement Learning (DQN) Tutorial\n",
    "=====================================\n",
    "**Author**: `Adam Paszke <https://github.com/apaszke>`_\n",
    "\n",
    "\n",
    "This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent\n",
    "on the CartPole-v0 task from the `OpenAI Gym <https://gym.openai.com/>`__.\n",
    "\n",
    "**Task**\n",
    "\n",
    "The agent has to decide between two actions - moving the cart left or\n",
    "right - so that the pole attached to it stays upright. You can find an\n",
    "official leaderboard with various algorithms and visualizations at the\n",
    "`Gym website <https://gym.openai.com/envs/CartPole-v0>`__.\n",
    "\n",
    ".. figure:: /_static/img/cartpole.gif\n",
    "   :alt: cartpole\n",
    "\n",
    "   cartpole\n",
    "\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action. In this\n",
    "task, rewards are +1 for every incremental timestep and the environment\n",
    "terminates if the pole falls over too far or the cart moves more then 2.4\n",
    "units away from center. This means better performing scenarios will run\n",
    "for longer duration, accumulating larger return.\n",
    "\n",
    "The CartPole task is designed so that the inputs to the agent are 4 real\n",
    "values representing the environment state (position, velocity, etc.).\n",
    "However, neural networks can solve the task purely by looking at the\n",
    "scene, so we'll use a patch of the screen centered on the cart as an\n",
    "input. Because of this, our results aren't directly comparable to the\n",
    "ones from the official leaderboard - our task is much harder.\n",
    "Unfortunately this does slow down the training, because we have to\n",
    "render all the frames.\n",
    "\n",
    "Strictly speaking, we will present the state as the difference between\n",
    "the current screen patch and the previous one. This will allow the agent\n",
    "to take the velocity of the pole into account from one image.\n",
    "\n",
    "**Packages**\n",
    "\n",
    "\n",
    "First, let's import needed packages. Firstly, we need\n",
    "`gym <https://gym.openai.com/docs>`__ for the environment\n",
    "(Install using `pip install gym`).\n",
    "We'll also use the following from PyTorch:\n",
    "\n",
    "-  neural networks (``torch.nn``)\n",
    "-  optimization (``torch.optim``)\n",
    "-  automatic differentiation (``torch.autograd``)\n",
    "-  utilities for vision tasks (``torchvision`` - `a separate\n",
    "   package <https://github.com/pytorch/vision>`__).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n",
    "\n",
    "We'll be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classses:\n",
    "\n",
    "-  ``Transition`` - a named tuple representing a single transition in\n",
    "   our environment. It essentially maps (state, action) pairs\n",
    "   to their (next_state, reward) result, with the state being the\n",
    "   screen difference image as described later on.\n",
    "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our model. But first, let's quickly recap what a DQN is.\n",
    "\n",
    "DQN algorithm\n",
    "-------------\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are\n",
    "also formulated deterministically for the sake of simplicity. In the\n",
    "reinforcement learning literature, they would also contain expectations\n",
    "over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is also known as the *return*. The discount,\n",
    "$\\gamma$, should be a constant between $0$ and $1$\n",
    "that ensures the sum converges. It makes rewards from the uncertain far\n",
    "future less important for our agent than the ones in the near future\n",
    "that it can be fairly confident about.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
    "us what our return would be, if we were to take an action in a given\n",
    "state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "However, we don't know everything about the world, so we don't have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble\n",
    "$Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
    "\n",
    "The difference between the two sides of the equality is known as the\n",
    "temporal difference error, $\\delta$:\n",
    "\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
    "\n",
    "To minimise this error, we will use the `Huber\n",
    "loss <https://en.wikipedia.org/wiki/Huber_loss>`__. The Huber loss acts\n",
    "like the mean squared error when the error is small, but like the mean\n",
    "absolute error when the error is large - this makes it more robust to\n",
    "outliers when the estimates of $Q$ are very noisy. We calculate\n",
    "this over a batch of transitions, $B$, sampled from the replay\n",
    "memory:\n",
    "\n",
    "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "   \\end{cases}\\end{align}\n",
    "\n",
    "Q-network\n",
    "\n",
    "Our model will be a convolutional neural network that takes in the\n",
    "difference between the current and previous screen patches. It has two\n",
    "outputs, representing $Q(s, \\mathrm{left})$ and\n",
    "$Q(s, \\mathrm{right})$ (where $s$ is the input to the\n",
    "network). In effect, the network is trying to predict the *expected return* of\n",
    "taking each action given the current input.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, 64)\n",
    "        self.l2 = nn.Linear(64, 128)\n",
    "        self.l3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input extraction\n",
    "^^^^^^^^^^^^^^^^\n",
    "\n",
    "The code below are utilities for extracting and processing rendered\n",
    "images from the environment. It uses the ``torchvision`` package, which\n",
    "makes it easy to compose image transforms. Once you run the cell it will\n",
    "display an example patch that it extracted.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "Hyperparameters and utilities\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "This cell instantiates our model and its optimizer, and defines some\n",
    "utilities:\n",
    "\n",
    "-  ``select_action`` - will select an action accordingly to an epsilon\n",
    "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
    "   the action, and sometimes we'll just sample one uniformly. The\n",
    "   probability of choosing a random action will start at ``EPS_START``\n",
    "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
    "   controls the rate of the decay.\n",
    "-  ``plot_durations`` - a helper for plotting the durations of episodes,\n",
    "   along with an average over the last 100 episodes (the measure used in\n",
    "   the official evaluations). The plot will be underneath the cell\n",
    "   containing the main training loop, and will update after every\n",
    "   episode.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #Tamanho do batch\n",
    "GAMMA = 0.9 #Taxa de aprendizado\n",
    "TARGET_UPDATE = 5 #Episódios de atualização das redes target\n",
    "\n",
    "### Parâmetros de exploração ###\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 100\n",
    "\n",
    "### Preparação das redes neurais ###\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001) #Definição do otimizador\n",
    "memory = ReplayMemory(10000) #Instanciamento da memória\n",
    "\n",
    "\n",
    "def select_action(state, train=True):\n",
    "    '''\n",
    "    Função que seleciona a ação que será realizada\n",
    "    '''\n",
    "    global i_episode\n",
    "    \n",
    "    sample = random.random() #Amostra de um número racional entre 0 e 1\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "                    math.exp(-1. * i_episode / EPS_DECAY) #Definição da probabilidade de explorar no i_episode\n",
    "    tensor_state = torch.autograd.Variable(state).type(torch.FloatTensor).to(device) #ajuste do formato do estado\n",
    "    \n",
    "    if train:\n",
    "        if sample > eps_threshold: #Explotação\n",
    "            with torch.no_grad():\n",
    "                return policy_net(tensor_state).data.max(1)[1].view(1, 1).to(device)\n",
    "        else:    #Exploração\n",
    "            return torch.LongTensor([[random.randrange(2)]]).to(device)\n",
    "    \n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(tensor_state).data.max(1)[1].view(1, 1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "^^^^^^^^^^^^^\n",
    "\n",
    "Finally, the code for training our model.\n",
    "\n",
    "Here, you can find an ``optimize_model`` function that performs a\n",
    "single step of the optimization. It first samples a batch, concatenates\n",
    "all the tensors into a single one, computes $Q(s_t, a_t)$ and\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our\n",
    "loss. By definition we set $V(s) = 0$ if $s$ is a terminal\n",
    "state. We also use a target network to compute $V(s_{t+1})$ for\n",
    "added stability. The target network has its weights kept frozen most of\n",
    "the time, but is updated with the policy network's weights every so often.\n",
    "This is usually a set number of steps but we shall use episodes for\n",
    "simplicity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    '''\n",
    "    Função utilizada para aprendizado das redes neurais\n",
    "    '''\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE: #Verifica se a memória ja tem BATCH_SIZE's amostras\n",
    "        return\n",
    "    \n",
    "    #Define que as redes estão em treinamento\n",
    "    policy_net.train()\n",
    "    target_net.train()\n",
    "    \n",
    "    #Amostra a memória em BATCH_SIZE's transições\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    #Define os tensores que serão utilizados no treinamento, \n",
    "    # agrupando os estados, ações e recompensas de todas as transições\n",
    "    batch_next_state = torch.autograd.Variable(torch.cat(batch.next_state)).to(device)\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device)\n",
    "    \n",
    "    #Calcula os Q(s,a) a partir dos estados do batch e seleciona aqueles \n",
    "    # cujas ações foram realizadas e armazenadas no batch\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    #Calcula os Q(s',a') a partir dos estados futuros do batch\n",
    "    # e selecionam os de maior valor\n",
    "    with torch.no_grad():\n",
    "        next_state_values = target_net(batch_next_state).detach().max(1)[0]\n",
    "  \n",
    "    #Calcula o lado direito da equação de Bellman\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    #Calcula a diferença entre os dois lados da equação de Bellman\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    #Otimiza os parâmetros da rede\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can find the main training loop. At the beginning we reset\n",
    "the environment and initialize the ``state`` Tensor. Then, we sample\n",
    "an action, execute it, observe the next screen and the reward (always\n",
    "1), and optimize our model once. When the episode ends (our model\n",
    "fails), we restart the loop.\n",
    "\n",
    "Below, `num_episodes` is set small. You should download\n",
    "the notebook and run lot more epsiodes, such as 300+ for meaningful\n",
    "duration improvements.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 0: retorno=11.0\n",
      "Episodio 1: retorno=26.0\n",
      "Episodio 2: retorno=11.0\n",
      "Episodio 3: retorno=16.0\n",
      "Episodio 4: retorno=16.0\n",
      "Episodio 5: retorno=20.0\n",
      "Episodio 6: retorno=33.0\n",
      "Episodio 7: retorno=16.0\n",
      "Episodio 8: retorno=26.0\n",
      "Episodio 9: retorno=15.0\n",
      "Episodio 10: retorno=26.0\n",
      "Episodio 11: retorno=11.0\n",
      "Episodio 12: retorno=16.0\n",
      "Episodio 13: retorno=10.0\n",
      "Episodio 14: retorno=14.0\n",
      "Episodio 15: retorno=34.0\n",
      "Episodio 16: retorno=12.0\n",
      "Episodio 17: retorno=12.0\n",
      "Episodio 18: retorno=30.0\n",
      "Episodio 19: retorno=20.0\n",
      "Episodio 20: retorno=14.0\n",
      "Episodio 21: retorno=16.0\n",
      "Episodio 22: retorno=12.0\n",
      "Episodio 23: retorno=13.0\n",
      "Episodio 24: retorno=18.0\n",
      "Episodio 25: retorno=25.0\n",
      "Episodio 26: retorno=17.0\n",
      "Episodio 27: retorno=19.0\n",
      "Episodio 28: retorno=10.0\n",
      "Episodio 29: retorno=14.0\n",
      "Episodio 30: retorno=50.0\n",
      "Episodio 31: retorno=17.0\n",
      "Episodio 32: retorno=25.0\n",
      "Episodio 33: retorno=21.0\n",
      "Episodio 34: retorno=28.0\n",
      "Episodio 35: retorno=88.0\n",
      "Episodio 36: retorno=14.0\n",
      "Episodio 37: retorno=16.0\n",
      "Episodio 38: retorno=81.0\n",
      "Episodio 39: retorno=15.0\n",
      "Episodio 40: retorno=43.0\n",
      "Episodio 41: retorno=118.0\n",
      "Episodio 42: retorno=27.0\n",
      "Episodio 43: retorno=51.0\n",
      "Episodio 44: retorno=57.0\n",
      "Episodio 45: retorno=50.0\n",
      "Episodio 46: retorno=20.0\n",
      "Episodio 47: retorno=13.0\n",
      "Episodio 48: retorno=15.0\n",
      "Episodio 49: retorno=100.0\n",
      "Episodio 50: retorno=51.0\n",
      "Episodio 51: retorno=21.0\n",
      "Episodio 52: retorno=56.0\n",
      "Episodio 53: retorno=48.0\n",
      "Episodio 54: retorno=11.0\n",
      "Episodio 55: retorno=85.0\n",
      "Episodio 56: retorno=65.0\n",
      "Episodio 57: retorno=80.0\n",
      "Episodio 58: retorno=190.0\n",
      "Episodio 59: retorno=73.0\n",
      "Episodio 60: retorno=95.0\n",
      "Episodio 61: retorno=114.0\n",
      "Episodio 62: retorno=163.0\n",
      "Episodio 63: retorno=17.0\n",
      "Episodio 64: retorno=114.0\n",
      "Episodio 65: retorno=42.0\n",
      "Episodio 66: retorno=78.0\n",
      "Episodio 67: retorno=153.0\n",
      "Episodio 68: retorno=95.0\n",
      "Episodio 69: retorno=32.0\n",
      "Episodio 70: retorno=63.0\n",
      "Episodio 71: retorno=89.0\n",
      "Episodio 72: retorno=251.0\n",
      "Episodio 73: retorno=122.0\n",
      "Episodio 74: retorno=117.0\n",
      "Episodio 75: retorno=53.0\n",
      "Episodio 76: retorno=81.0\n",
      "Episodio 77: retorno=238.0\n",
      "Episodio 78: retorno=50.0\n",
      "Episodio 79: retorno=86.0\n",
      "Episodio 80: retorno=85.0\n",
      "Episodio 81: retorno=90.0\n",
      "Episodio 82: retorno=82.0\n",
      "Episodio 83: retorno=26.0\n",
      "Episodio 84: retorno=15.0\n",
      "Episodio 85: retorno=16.0\n",
      "Episodio 86: retorno=37.0\n",
      "Episodio 87: retorno=69.0\n",
      "Episodio 88: retorno=81.0\n",
      "Episodio 89: retorno=59.0\n",
      "Episodio 90: retorno=11.0\n",
      "Episodio 91: retorno=23.0\n",
      "Episodio 92: retorno=101.0\n",
      "Episodio 93: retorno=32.0\n",
      "Episodio 94: retorno=64.0\n",
      "Episodio 95: retorno=113.0\n",
      "Episodio 96: retorno=27.0\n",
      "Episodio 97: retorno=47.0\n",
      "Episodio 98: retorno=47.0\n",
      "Episodio 99: retorno=101.0\n",
      "Episodio 100: retorno=89.0\n",
      "Episodio 101: retorno=165.0\n",
      "Episodio 102: retorno=29.0\n",
      "Episodio 103: retorno=25.0\n",
      "Episodio 104: retorno=116.0\n",
      "Episodio 105: retorno=27.0\n",
      "Episodio 106: retorno=104.0\n",
      "Episodio 107: retorno=36.0\n",
      "Episodio 108: retorno=343.0\n",
      "Episodio 109: retorno=27.0\n",
      "Episodio 110: retorno=239.0\n",
      "Episodio 111: retorno=29.0\n",
      "Episodio 112: retorno=36.0\n",
      "Episodio 113: retorno=60.0\n",
      "Episodio 114: retorno=33.0\n",
      "Episodio 115: retorno=45.0\n",
      "Episodio 116: retorno=74.0\n",
      "Episodio 117: retorno=249.0\n",
      "Episodio 118: retorno=81.0\n",
      "Episodio 119: retorno=112.0\n",
      "Episodio 120: retorno=88.0\n",
      "Episodio 121: retorno=189.0\n",
      "Episodio 122: retorno=142.0\n",
      "Episodio 123: retorno=42.0\n",
      "Episodio 124: retorno=222.0\n",
      "Episodio 125: retorno=367.0\n",
      "Episodio 126: retorno=55.0\n",
      "Episodio 127: retorno=230.0\n",
      "Episodio 128: retorno=324.0\n",
      "Episodio 129: retorno=14.0\n",
      "Episodio 130: retorno=168.0\n",
      "Episodio 131: retorno=246.0\n",
      "Episodio 132: retorno=133.0\n",
      "Episodio 133: retorno=33.0\n",
      "Episodio 134: retorno=88.0\n",
      "Episodio 135: retorno=246.0\n",
      "Episodio 136: retorno=141.0\n",
      "Episodio 137: retorno=46.0\n",
      "Episodio 138: retorno=42.0\n",
      "Episodio 139: retorno=143.0\n",
      "Episodio 140: retorno=166.0\n",
      "Episodio 141: retorno=193.0\n",
      "Episodio 142: retorno=98.0\n",
      "Episodio 143: retorno=288.0\n",
      "Episodio 144: retorno=13.0\n",
      "Episodio 145: retorno=121.0\n",
      "Episodio 146: retorno=100.0\n",
      "Episodio 147: retorno=166.0\n",
      "Episodio 148: retorno=81.0\n",
      "Episodio 149: retorno=60.0\n",
      "Episodio 150: retorno=14.0\n",
      "Episodio 151: retorno=51.0\n",
      "Episodio 152: retorno=113.0\n",
      "Episodio 153: retorno=102.0\n",
      "Episodio 154: retorno=98.0\n",
      "Episodio 155: retorno=127.0\n",
      "Episodio 156: retorno=12.0\n",
      "Episodio 157: retorno=98.0\n",
      "Episodio 158: retorno=72.0\n",
      "Episodio 159: retorno=28.0\n",
      "Episodio 160: retorno=390.0\n",
      "Episodio 161: retorno=132.0\n",
      "Episodio 162: retorno=103.0\n",
      "Episodio 163: retorno=88.0\n",
      "Episodio 164: retorno=144.0\n",
      "Episodio 165: retorno=81.0\n",
      "Episodio 166: retorno=27.0\n",
      "Episodio 167: retorno=16.0\n",
      "Episodio 168: retorno=126.0\n",
      "Episodio 169: retorno=77.0\n",
      "Episodio 170: retorno=123.0\n",
      "Episodio 171: retorno=147.0\n",
      "Episodio 172: retorno=141.0\n",
      "Episodio 173: retorno=139.0\n",
      "Episodio 174: retorno=145.0\n",
      "Episodio 175: retorno=21.0\n",
      "Episodio 176: retorno=137.0\n",
      "Episodio 177: retorno=152.0\n",
      "Episodio 178: retorno=143.0\n",
      "Episodio 179: retorno=12.0\n",
      "Episodio 180: retorno=151.0\n",
      "Episodio 181: retorno=116.0\n",
      "Episodio 182: retorno=153.0\n",
      "Episodio 183: retorno=167.0\n",
      "Episodio 184: retorno=185.0\n",
      "Episodio 185: retorno=18.0\n",
      "Episodio 186: retorno=170.0\n",
      "Episodio 187: retorno=183.0\n",
      "Episodio 188: retorno=17.0\n",
      "Episodio 189: retorno=535.0\n",
      "Episodio 190: retorno=181.0\n",
      "Episodio 191: retorno=169.0\n",
      "Episodio 192: retorno=82.0\n",
      "Episodio 193: retorno=204.0\n",
      "Episodio 194: retorno=173.0\n",
      "Episodio 195: retorno=219.0\n",
      "Episodio 196: retorno=43.0\n",
      "Episodio 197: retorno=165.0\n",
      "Episodio 198: retorno=168.0\n",
      "Episodio 199: retorno=15.0\n",
      "Episodio 200: retorno=134.0\n",
      "Episodio 201: retorno=41.0\n",
      "Episodio 202: retorno=140.0\n",
      "Episodio 203: retorno=172.0\n",
      "Episodio 204: retorno=171.0\n",
      "Episodio 205: retorno=189.0\n",
      "Episodio 206: retorno=170.0\n",
      "Episodio 207: retorno=145.0\n",
      "Episodio 208: retorno=159.0\n",
      "Episodio 209: retorno=13.0\n",
      "Episodio 210: retorno=129.0\n",
      "Episodio 211: retorno=19.0\n",
      "Episodio 212: retorno=180.0\n",
      "Episodio 213: retorno=32.0\n",
      "Episodio 214: retorno=159.0\n",
      "Episodio 215: retorno=155.0\n",
      "Episodio 216: retorno=154.0\n",
      "Episodio 217: retorno=148.0\n",
      "Episodio 218: retorno=157.0\n",
      "Episodio 219: retorno=161.0\n",
      "Episodio 220: retorno=166.0\n",
      "Episodio 221: retorno=160.0\n",
      "Episodio 222: retorno=158.0\n",
      "Episodio 223: retorno=13.0\n",
      "Episodio 224: retorno=157.0\n",
      "Episodio 225: retorno=174.0\n",
      "Episodio 226: retorno=12.0\n",
      "Episodio 227: retorno=204.0\n",
      "Episodio 228: retorno=57.0\n",
      "Episodio 229: retorno=180.0\n",
      "Episodio 230: retorno=10.0\n",
      "Episodio 231: retorno=166.0\n",
      "Episodio 232: retorno=155.0\n",
      "Episodio 233: retorno=163.0\n",
      "Episodio 234: retorno=174.0\n",
      "Episodio 235: retorno=167.0\n",
      "Episodio 236: retorno=148.0\n",
      "Episodio 237: retorno=151.0\n",
      "Episodio 238: retorno=189.0\n",
      "Episodio 239: retorno=158.0\n",
      "Episodio 240: retorno=150.0\n",
      "Episodio 241: retorno=216.0\n",
      "Episodio 242: retorno=202.0\n",
      "Episodio 243: retorno=19.0\n",
      "Episodio 244: retorno=238.0\n",
      "Episodio 245: retorno=245.0\n",
      "Episodio 246: retorno=59.0\n",
      "Episodio 247: retorno=168.0\n",
      "Episodio 248: retorno=303.0\n",
      "Episodio 249: retorno=20.0\n",
      "Episodio 250: retorno=178.0\n",
      "Episodio 251: retorno=168.0\n",
      "Episodio 252: retorno=177.0\n",
      "Episodio 253: retorno=185.0\n",
      "Episodio 254: retorno=172.0\n",
      "Episodio 255: retorno=166.0\n",
      "Episodio 256: retorno=158.0\n",
      "Episodio 257: retorno=167.0\n",
      "Episodio 258: retorno=167.0\n",
      "Episodio 259: retorno=165.0\n",
      "Episodio 260: retorno=157.0\n",
      "Episodio 261: retorno=200.0\n",
      "Episodio 262: retorno=172.0\n",
      "Episodio 263: retorno=125.0\n",
      "Episodio 264: retorno=158.0\n",
      "Episodio 265: retorno=188.0\n",
      "Episodio 266: retorno=248.0\n",
      "Episodio 267: retorno=143.0\n",
      "Episodio 268: retorno=209.0\n",
      "Episodio 269: retorno=204.0\n",
      "Episodio 270: retorno=20.0\n",
      "Episodio 271: retorno=226.0\n",
      "Episodio 272: retorno=321.0\n",
      "Episodio 273: retorno=21.0\n",
      "Episodio 274: retorno=313.0\n",
      "Episodio 275: retorno=344.0\n",
      "Episodio 276: retorno=310.0\n",
      "Episodio 277: retorno=581.0\n",
      "Episodio 278: retorno=342.0\n",
      "Episodio 279: retorno=18.0\n",
      "Episodio 280: retorno=311.0\n",
      "Episodio 281: retorno=210.0\n",
      "Episodio 282: retorno=243.0\n",
      "Episodio 283: retorno=258.0\n",
      "Episodio 284: retorno=343.0\n",
      "Episodio 285: retorno=272.0\n",
      "Episodio 286: retorno=49.0\n",
      "Episodio 287: retorno=412.0\n",
      "Episodio 288: retorno=339.0\n",
      "Episodio 289: retorno=653.0\n",
      "Episodio 290: retorno=737.0\n",
      "Episodio 291: retorno=466.0\n",
      "Episodio 292: retorno=241.0\n",
      "Episodio 293: retorno=16.0\n",
      "Episodio 294: retorno=194.0\n",
      "Episodio 295: retorno=231.0\n",
      "Episodio 296: retorno=226.0\n",
      "Episodio 297: retorno=162.0\n",
      "Episodio 298: retorno=38.0\n",
      "Episodio 299: retorno=10.0\n",
      "Episodio 300: retorno=105.0\n",
      "Episodio 301: retorno=150.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 302: retorno=123.0\n",
      "Episodio 303: retorno=144.0\n",
      "Episodio 304: retorno=147.0\n",
      "Episodio 305: retorno=51.0\n",
      "Episodio 306: retorno=161.0\n",
      "Episodio 307: retorno=5000.0\n",
      "Episodio 308: retorno=5000.0\n",
      "Episodio 309: retorno=129.0\n",
      "Episodio 310: retorno=4199.0\n",
      "Episodio 311: retorno=12.0\n",
      "Episodio 312: retorno=399.0\n",
      "Episodio 313: retorno=27.0\n",
      "Episodio 314: retorno=12.0\n",
      "Episodio 315: retorno=14.0\n",
      "Episodio 316: retorno=32.0\n",
      "Episodio 317: retorno=13.0\n",
      "Episodio 318: retorno=14.0\n",
      "Episodio 319: retorno=92.0\n",
      "Episodio 320: retorno=133.0\n",
      "Episodio 321: retorno=201.0\n",
      "Episodio 322: retorno=221.0\n",
      "Episodio 323: retorno=126.0\n",
      "Episodio 324: retorno=20.0\n",
      "Episodio 325: retorno=2387.0\n",
      "Episodio 326: retorno=219.0\n",
      "Episodio 327: retorno=2003.0\n",
      "Episodio 328: retorno=2302.0\n",
      "Episodio 329: retorno=2438.0\n",
      "Episodio 330: retorno=472.0\n",
      "Episodio 331: retorno=320.0\n",
      "Episodio 332: retorno=865.0\n",
      "Episodio 333: retorno=33.0\n",
      "Episodio 334: retorno=5000.0\n",
      "Episodio 335: retorno=904.0\n",
      "Episodio 336: retorno=720.0\n",
      "Episodio 337: retorno=316.0\n",
      "Episodio 338: retorno=118.0\n",
      "Episodio 339: retorno=871.0\n",
      "Episodio 340: retorno=338.0\n",
      "Episodio 341: retorno=883.0\n",
      "Episodio 342: retorno=614.0\n",
      "Episodio 343: retorno=1120.0\n",
      "Episodio 344: retorno=345.0\n",
      "Episodio 345: retorno=1094.0\n",
      "Episodio 346: retorno=388.0\n",
      "Episodio 347: retorno=648.0\n",
      "Episodio 348: retorno=30.0\n",
      "Episodio 349: retorno=783.0\n",
      "Episodio 350: retorno=883.0\n",
      "Episodio 351: retorno=265.0\n",
      "Episodio 352: retorno=28.0\n",
      "Episodio 353: retorno=200.0\n",
      "Episodio 354: retorno=577.0\n",
      "Episodio 355: retorno=485.0\n",
      "Episodio 356: retorno=538.0\n",
      "Episodio 357: retorno=58.0\n",
      "Episodio 358: retorno=52.0\n",
      "Episodio 359: retorno=115.0\n",
      "Episodio 360: retorno=535.0\n",
      "Episodio 361: retorno=461.0\n",
      "Episodio 362: retorno=44.0\n",
      "Episodio 363: retorno=801.0\n",
      "Episodio 364: retorno=477.0\n",
      "Episodio 365: retorno=764.0\n",
      "Episodio 366: retorno=1609.0\n",
      "Episodio 367: retorno=108.0\n",
      "Episodio 368: retorno=291.0\n",
      "Episodio 369: retorno=425.0\n",
      "Episodio 370: retorno=86.0\n",
      "Episodio 371: retorno=62.0\n",
      "Episodio 372: retorno=157.0\n",
      "Episodio 373: retorno=33.0\n",
      "Episodio 374: retorno=295.0\n",
      "Episodio 375: retorno=39.0\n",
      "Episodio 376: retorno=192.0\n",
      "Episodio 377: retorno=31.0\n",
      "Episodio 378: retorno=38.0\n",
      "Episodio 379: retorno=128.0\n",
      "Episodio 380: retorno=45.0\n",
      "Episodio 381: retorno=686.0\n",
      "Episodio 382: retorno=639.0\n",
      "Episodio 383: retorno=13.0\n",
      "Episodio 384: retorno=209.0\n",
      "Episodio 385: retorno=354.0\n",
      "Episodio 386: retorno=213.0\n",
      "Episodio 387: retorno=169.0\n",
      "Episodio 388: retorno=1109.0\n",
      "Episodio 389: retorno=588.0\n",
      "Episodio 390: retorno=186.0\n",
      "Episodio 391: retorno=526.0\n",
      "Episodio 392: retorno=81.0\n",
      "Episodio 393: retorno=13.0\n",
      "Episodio 394: retorno=594.0\n",
      "Episodio 395: retorno=286.0\n",
      "Episodio 396: retorno=654.0\n",
      "Episodio 397: retorno=225.0\n",
      "Episodio 398: retorno=31.0\n",
      "Episodio 399: retorno=83.0\n",
      "Episodio 400: retorno=246.0\n",
      "Episodio 401: retorno=140.0\n",
      "Episodio 402: retorno=258.0\n",
      "Episodio 403: retorno=181.0\n",
      "Episodio 404: retorno=113.0\n",
      "Episodio 405: retorno=53.0\n",
      "Episodio 406: retorno=73.0\n",
      "Episodio 407: retorno=117.0\n",
      "Episodio 408: retorno=314.0\n",
      "Episodio 409: retorno=195.0\n",
      "Episodio 410: retorno=157.0\n",
      "Episodio 411: retorno=123.0\n",
      "Episodio 412: retorno=98.0\n",
      "Episodio 413: retorno=148.0\n",
      "Episodio 414: retorno=97.0\n",
      "Episodio 415: retorno=66.0\n",
      "Episodio 416: retorno=135.0\n",
      "Episodio 417: retorno=53.0\n",
      "Episodio 418: retorno=197.0\n",
      "Episodio 419: retorno=104.0\n",
      "Episodio 420: retorno=144.0\n",
      "Episodio 421: retorno=26.0\n",
      "Episodio 422: retorno=586.0\n",
      "Episodio 423: retorno=344.0\n",
      "Episodio 424: retorno=127.0\n",
      "Episodio 425: retorno=226.0\n",
      "Episodio 426: retorno=199.0\n",
      "Episodio 427: retorno=411.0\n",
      "Episodio 428: retorno=338.0\n",
      "Episodio 429: retorno=123.0\n",
      "Episodio 430: retorno=276.0\n",
      "Episodio 431: retorno=522.0\n",
      "Episodio 432: retorno=128.0\n",
      "Episodio 433: retorno=27.0\n",
      "Episodio 434: retorno=261.0\n",
      "Episodio 435: retorno=1015.0\n",
      "Episodio 436: retorno=84.0\n",
      "Episodio 437: retorno=37.0\n",
      "Episodio 438: retorno=84.0\n",
      "Episodio 439: retorno=60.0\n",
      "Episodio 440: retorno=47.0\n",
      "Episodio 441: retorno=80.0\n",
      "Episodio 442: retorno=683.0\n",
      "Episodio 443: retorno=1634.0\n",
      "Episodio 444: retorno=609.0\n",
      "Episodio 445: retorno=1019.0\n",
      "Episodio 446: retorno=672.0\n",
      "Episodio 447: retorno=391.0\n",
      "Episodio 448: retorno=1954.0\n",
      "Episodio 449: retorno=284.0\n",
      "Episodio 450: retorno=415.0\n",
      "Episodio 451: retorno=320.0\n",
      "Episodio 452: retorno=17.0\n",
      "Episodio 453: retorno=44.0\n",
      "Episodio 454: retorno=205.0\n",
      "Episodio 455: retorno=263.0\n",
      "Episodio 456: retorno=223.0\n",
      "Episodio 457: retorno=82.0\n",
      "Episodio 458: retorno=105.0\n",
      "Episodio 459: retorno=224.0\n",
      "Episodio 460: retorno=1249.0\n",
      "Episodio 461: retorno=112.0\n",
      "Episodio 462: retorno=113.0\n",
      "Episodio 463: retorno=111.0\n",
      "Episodio 464: retorno=312.0\n",
      "Episodio 465: retorno=362.0\n",
      "Episodio 466: retorno=31.0\n",
      "Episodio 467: retorno=159.0\n",
      "Episodio 468: retorno=480.0\n",
      "Episodio 469: retorno=66.0\n",
      "Episodio 470: retorno=919.0\n",
      "Episodio 471: retorno=205.0\n",
      "Episodio 472: retorno=293.0\n",
      "Episodio 473: retorno=412.0\n",
      "Episodio 474: retorno=288.0\n",
      "Episodio 475: retorno=127.0\n",
      "Episodio 476: retorno=83.0\n",
      "Episodio 477: retorno=95.0\n",
      "Episodio 478: retorno=264.0\n",
      "Episodio 479: retorno=282.0\n",
      "Episodio 480: retorno=61.0\n",
      "Episodio 481: retorno=131.0\n",
      "Episodio 482: retorno=188.0\n",
      "Episodio 483: retorno=88.0\n",
      "Episodio 484: retorno=146.0\n",
      "Episodio 485: retorno=456.0\n",
      "Episodio 486: retorno=1569.0\n",
      "Episodio 487: retorno=396.0\n",
      "Episodio 488: retorno=1227.0\n",
      "Episodio 489: retorno=345.0\n",
      "Episodio 490: retorno=131.0\n",
      "Episodio 491: retorno=2427.0\n",
      "Episodio 492: retorno=183.0\n",
      "Episodio 493: retorno=527.0\n",
      "Episodio 494: retorno=1432.0\n",
      "Episodio 495: retorno=1501.0\n",
      "Episodio 496: retorno=411.0\n",
      "Episodio 497: retorno=803.0\n",
      "Episodio 498: retorno=358.0\n",
      "Episodio 499: retorno=382.0\n",
      "Episodio 500: retorno=25.0\n",
      "Episodio 501: retorno=89.0\n",
      "Episodio 502: retorno=220.0\n",
      "Episodio 503: retorno=139.0\n",
      "Episodio 504: retorno=62.0\n",
      "Episodio 505: retorno=60.0\n",
      "Episodio 506: retorno=243.0\n",
      "Episodio 507: retorno=203.0\n",
      "Episodio 508: retorno=70.0\n",
      "Episodio 509: retorno=351.0\n",
      "Episodio 510: retorno=57.0\n",
      "Episodio 511: retorno=351.0\n",
      "Episodio 512: retorno=78.0\n",
      "Episodio 513: retorno=48.0\n",
      "Episodio 514: retorno=200.0\n",
      "Episodio 515: retorno=54.0\n",
      "Episodio 516: retorno=185.0\n",
      "Episodio 517: retorno=109.0\n",
      "Episodio 518: retorno=169.0\n",
      "Episodio 519: retorno=97.0\n",
      "Episodio 520: retorno=27.0\n",
      "Episodio 521: retorno=47.0\n",
      "Episodio 522: retorno=113.0\n",
      "Episodio 523: retorno=440.0\n",
      "Episodio 524: retorno=1120.0\n",
      "Episodio 525: retorno=468.0\n",
      "Episodio 526: retorno=700.0\n",
      "Episodio 527: retorno=1701.0\n",
      "Episodio 528: retorno=1861.0\n",
      "Episodio 529: retorno=1108.0\n",
      "Episodio 530: retorno=374.0\n",
      "Episodio 531: retorno=92.0\n",
      "Episodio 532: retorno=3653.0\n",
      "Episodio 533: retorno=96.0\n",
      "Episodio 534: retorno=234.0\n",
      "Episodio 535: retorno=1416.0\n",
      "Episodio 536: retorno=303.0\n",
      "Episodio 537: retorno=382.0\n",
      "Episodio 538: retorno=1291.0\n",
      "Episodio 539: retorno=127.0\n",
      "Episodio 540: retorno=384.0\n",
      "Episodio 541: retorno=784.0\n",
      "Episodio 542: retorno=73.0\n",
      "Episodio 543: retorno=564.0\n",
      "Episodio 544: retorno=750.0\n",
      "Episodio 545: retorno=959.0\n",
      "Episodio 546: retorno=5000.0\n",
      "Episodio 547: retorno=2620.0\n",
      "Episodio 548: retorno=1605.0\n",
      "Episodio 549: retorno=933.0\n",
      "Episodio 550: retorno=5000.0\n",
      "Episodio 551: retorno=109.0\n",
      "Episodio 552: retorno=3366.0\n",
      "Episodio 553: retorno=727.0\n",
      "Episodio 554: retorno=122.0\n",
      "Episodio 555: retorno=1897.0\n",
      "Episodio 556: retorno=16.0\n",
      "Episodio 557: retorno=182.0\n",
      "Episodio 558: retorno=1087.0\n",
      "Episodio 559: retorno=121.0\n",
      "Episodio 560: retorno=796.0\n",
      "Episodio 561: retorno=17.0\n",
      "Episodio 562: retorno=119.0\n",
      "Episodio 563: retorno=129.0\n",
      "Episodio 564: retorno=792.0\n",
      "Episodio 565: retorno=149.0\n",
      "Episodio 566: retorno=2065.0\n",
      "Episodio 567: retorno=13.0\n",
      "Episodio 568: retorno=747.0\n",
      "Episodio 569: retorno=33.0\n",
      "Episodio 570: retorno=1024.0\n",
      "Episodio 571: retorno=13.0\n",
      "Episodio 572: retorno=14.0\n",
      "Episodio 573: retorno=217.0\n",
      "Episodio 574: retorno=77.0\n",
      "Episodio 575: retorno=65.0\n",
      "Episodio 576: retorno=96.0\n",
      "Episodio 577: retorno=70.0\n",
      "Episodio 578: retorno=71.0\n",
      "Episodio 579: retorno=74.0\n",
      "Episodio 580: retorno=212.0\n",
      "Episodio 581: retorno=121.0\n",
      "Episodio 582: retorno=30.0\n",
      "Episodio 583: retorno=168.0\n",
      "Episodio 584: retorno=10.0\n",
      "Episodio 585: retorno=266.0\n",
      "Episodio 586: retorno=19.0\n",
      "Episodio 587: retorno=635.0\n",
      "Episodio 588: retorno=195.0\n",
      "Episodio 589: retorno=33.0\n",
      "Episodio 590: retorno=462.0\n",
      "Episodio 591: retorno=125.0\n",
      "Episodio 592: retorno=922.0\n",
      "Episodio 593: retorno=117.0\n",
      "Episodio 594: retorno=159.0\n",
      "Episodio 595: retorno=191.0\n",
      "Episodio 596: retorno=144.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 597: retorno=14.0\n",
      "Episodio 598: retorno=522.0\n",
      "Episodio 599: retorno=30.0\n",
      "Episodio 600: retorno=575.0\n",
      "Episodio 601: retorno=230.0\n",
      "Episodio 602: retorno=631.0\n",
      "Episodio 603: retorno=1046.0\n",
      "Episodio 604: retorno=571.0\n",
      "Episodio 605: retorno=478.0\n",
      "Episodio 606: retorno=151.0\n",
      "Episodio 607: retorno=218.0\n",
      "Episodio 608: retorno=179.0\n",
      "Episodio 609: retorno=310.0\n",
      "Episodio 610: retorno=453.0\n",
      "Episodio 611: retorno=156.0\n",
      "Episodio 612: retorno=135.0\n",
      "Episodio 613: retorno=135.0\n",
      "Episodio 614: retorno=135.0\n",
      "Episodio 615: retorno=211.0\n",
      "Episodio 616: retorno=124.0\n",
      "Episodio 617: retorno=219.0\n",
      "Episodio 618: retorno=144.0\n",
      "Episodio 619: retorno=142.0\n",
      "Episodio 620: retorno=217.0\n",
      "Episodio 621: retorno=146.0\n",
      "Episodio 622: retorno=454.0\n",
      "Episodio 623: retorno=714.0\n",
      "Episodio 624: retorno=428.0\n",
      "Episodio 625: retorno=240.0\n",
      "Episodio 626: retorno=126.0\n",
      "Episodio 627: retorno=108.0\n",
      "Episodio 628: retorno=132.0\n",
      "Episodio 629: retorno=141.0\n",
      "Episodio 630: retorno=165.0\n",
      "Episodio 631: retorno=222.0\n",
      "Episodio 632: retorno=119.0\n",
      "Episodio 633: retorno=993.0\n",
      "Episodio 634: retorno=98.0\n",
      "Episodio 635: retorno=415.0\n",
      "Episodio 636: retorno=694.0\n",
      "Episodio 637: retorno=891.0\n",
      "Episodio 638: retorno=5000.0\n",
      "Episodio 639: retorno=5000.0\n",
      "Episodio 640: retorno=57.0\n",
      "Episodio 641: retorno=70.0\n",
      "Episodio 642: retorno=4185.0\n",
      "Episodio 643: retorno=5000.0\n",
      "Episodio 644: retorno=3067.0\n",
      "Episodio 645: retorno=164.0\n",
      "Episodio 646: retorno=1646.0\n",
      "Episodio 647: retorno=251.0\n",
      "Episodio 648: retorno=200.0\n",
      "Episodio 649: retorno=178.0\n",
      "Episodio 650: retorno=186.0\n",
      "Episodio 651: retorno=5000.0\n",
      "Episodio 652: retorno=5000.0\n",
      "Episodio 653: retorno=5000.0\n",
      "Episodio 654: retorno=5000.0\n",
      "Episodio 655: retorno=5000.0\n",
      "Episodio 656: retorno=79.0\n",
      "Episodio 657: retorno=12.0\n",
      "Episodio 658: retorno=28.0\n",
      "Episodio 659: retorno=29.0\n",
      "Episodio 660: retorno=35.0\n",
      "Episodio 661: retorno=33.0\n",
      "Episodio 662: retorno=333.0\n",
      "Episodio 663: retorno=299.0\n",
      "Episodio 664: retorno=68.0\n",
      "Episodio 665: retorno=100.0\n",
      "Episodio 666: retorno=39.0\n",
      "Episodio 667: retorno=52.0\n",
      "Episodio 668: retorno=62.0\n",
      "Episodio 669: retorno=2449.0\n",
      "Episodio 670: retorno=216.0\n",
      "Episodio 671: retorno=91.0\n",
      "Episodio 672: retorno=191.0\n",
      "Episodio 673: retorno=89.0\n",
      "Episodio 674: retorno=83.0\n",
      "Episodio 675: retorno=77.0\n",
      "Episodio 676: retorno=83.0\n",
      "Episodio 677: retorno=75.0\n",
      "Episodio 678: retorno=61.0\n",
      "Episodio 679: retorno=77.0\n",
      "Episodio 680: retorno=104.0\n",
      "Episodio 681: retorno=40.0\n",
      "Episodio 682: retorno=111.0\n",
      "Episodio 683: retorno=47.0\n",
      "Episodio 684: retorno=31.0\n",
      "Episodio 685: retorno=66.0\n",
      "Episodio 686: retorno=49.0\n",
      "Episodio 687: retorno=43.0\n",
      "Episodio 688: retorno=62.0\n",
      "Episodio 689: retorno=92.0\n",
      "Episodio 690: retorno=64.0\n",
      "Episodio 691: retorno=31.0\n",
      "Episodio 692: retorno=101.0\n",
      "Episodio 693: retorno=55.0\n",
      "Episodio 694: retorno=49.0\n",
      "Episodio 695: retorno=45.0\n",
      "Episodio 696: retorno=17.0\n",
      "Episodio 697: retorno=12.0\n",
      "Episodio 698: retorno=114.0\n",
      "Episodio 699: retorno=33.0\n",
      "Episodio 700: retorno=106.0\n",
      "Episodio 701: retorno=115.0\n",
      "Episodio 702: retorno=65.0\n",
      "Episodio 703: retorno=83.0\n",
      "Episodio 704: retorno=64.0\n",
      "Episodio 705: retorno=60.0\n",
      "Episodio 706: retorno=284.0\n",
      "Episodio 707: retorno=100.0\n",
      "Episodio 708: retorno=452.0\n",
      "Episodio 709: retorno=486.0\n",
      "Episodio 710: retorno=1213.0\n",
      "Episodio 711: retorno=5000.0\n",
      "Episodio 712: retorno=116.0\n",
      "Episodio 713: retorno=99.0\n",
      "Episodio 714: retorno=5000.0\n",
      "Episodio 715: retorno=1331.0\n",
      "Episodio 716: retorno=48.0\n",
      "Episodio 717: retorno=85.0\n",
      "Episodio 718: retorno=24.0\n",
      "Episodio 719: retorno=25.0\n",
      "Episodio 720: retorno=24.0\n",
      "Episodio 721: retorno=5000.0\n",
      "Episodio 722: retorno=80.0\n",
      "Episodio 723: retorno=3708.0\n",
      "Episodio 724: retorno=118.0\n",
      "Episodio 725: retorno=5000.0\n",
      "Episodio 726: retorno=67.0\n",
      "Episodio 727: retorno=2236.0\n",
      "Episodio 728: retorno=878.0\n",
      "Episodio 729: retorno=100.0\n",
      "Episodio 730: retorno=207.0\n",
      "Episodio 731: retorno=2039.0\n",
      "Episodio 732: retorno=27.0\n",
      "Episodio 733: retorno=32.0\n",
      "Episodio 734: retorno=348.0\n",
      "Episodio 735: retorno=25.0\n",
      "Episodio 736: retorno=45.0\n",
      "Episodio 737: retorno=164.0\n",
      "Episodio 738: retorno=158.0\n",
      "Episodio 739: retorno=94.0\n",
      "Episodio 740: retorno=445.0\n",
      "Episodio 741: retorno=5000.0\n",
      "Episodio 742: retorno=5000.0\n",
      "Episodio 743: retorno=120.0\n",
      "Episodio 744: retorno=5000.0\n",
      "Episodio 745: retorno=2819.0\n",
      "Episodio 746: retorno=5000.0\n",
      "Episodio 747: retorno=5000.0\n",
      "Episodio 748: retorno=5000.0\n",
      "Episodio 749: retorno=5000.0\n",
      "Episodio 750: retorno=811.0\n",
      "Episodio 751: retorno=575.0\n",
      "Episodio 752: retorno=3439.0\n",
      "Episodio 753: retorno=3561.0\n",
      "Episodio 754: retorno=256.0\n",
      "Episodio 755: retorno=199.0\n",
      "Episodio 756: retorno=404.0\n",
      "Episodio 757: retorno=213.0\n",
      "Episodio 758: retorno=44.0\n",
      "Episodio 759: retorno=165.0\n",
      "Episodio 760: retorno=70.0\n",
      "Episodio 761: retorno=74.0\n",
      "Episodio 762: retorno=14.0\n",
      "Episodio 763: retorno=35.0\n",
      "Episodio 764: retorno=70.0\n",
      "Episodio 765: retorno=9.0\n",
      "Episodio 766: retorno=114.0\n",
      "Episodio 767: retorno=1507.0\n",
      "Episodio 768: retorno=120.0\n",
      "Episodio 769: retorno=242.0\n",
      "Episodio 770: retorno=104.0\n",
      "Episodio 771: retorno=205.0\n",
      "Episodio 772: retorno=104.0\n",
      "Episodio 773: retorno=3733.0\n",
      "Episodio 774: retorno=4609.0\n",
      "Episodio 775: retorno=197.0\n",
      "Episodio 776: retorno=20.0\n",
      "Episodio 777: retorno=55.0\n",
      "Episodio 778: retorno=5000.0\n",
      "Episodio 779: retorno=3411.0\n",
      "Episodio 780: retorno=62.0\n",
      "Episodio 781: retorno=1032.0\n",
      "Episodio 782: retorno=212.0\n",
      "Episodio 783: retorno=46.0\n",
      "Episodio 784: retorno=396.0\n",
      "Episodio 785: retorno=224.0\n",
      "Episodio 786: retorno=1214.0\n",
      "Episodio 787: retorno=78.0\n",
      "Episodio 788: retorno=184.0\n",
      "Episodio 789: retorno=1998.0\n",
      "Episodio 790: retorno=3896.0\n",
      "Episodio 791: retorno=174.0\n",
      "Episodio 792: retorno=206.0\n",
      "Episodio 793: retorno=5000.0\n",
      "Episodio 794: retorno=304.0\n",
      "Episodio 795: retorno=2835.0\n",
      "Episodio 796: retorno=5000.0\n",
      "Episodio 797: retorno=5000.0\n",
      "Episodio 798: retorno=5000.0\n",
      "Episodio 799: retorno=5000.0\n",
      "Episodio 800: retorno=5000.0\n",
      "Episodio 801: retorno=119.0\n",
      "Episodio 802: retorno=11.0\n",
      "Episodio 803: retorno=4259.0\n",
      "Episodio 804: retorno=117.0\n",
      "Episodio 805: retorno=45.0\n",
      "Episodio 806: retorno=3058.0\n",
      "Episodio 807: retorno=5000.0\n",
      "Episodio 808: retorno=1891.0\n",
      "Episodio 809: retorno=14.0\n",
      "Episodio 810: retorno=1607.0\n",
      "Episodio 811: retorno=39.0\n",
      "Episodio 812: retorno=22.0\n",
      "Episodio 813: retorno=50.0\n",
      "Episodio 814: retorno=42.0\n",
      "Episodio 815: retorno=39.0\n",
      "Episodio 816: retorno=192.0\n",
      "Episodio 817: retorno=26.0\n",
      "Episodio 818: retorno=28.0\n",
      "Episodio 819: retorno=19.0\n",
      "Episodio 820: retorno=61.0\n",
      "Episodio 821: retorno=25.0\n",
      "Episodio 822: retorno=24.0\n",
      "Episodio 823: retorno=86.0\n",
      "Episodio 824: retorno=50.0\n",
      "Episodio 825: retorno=67.0\n",
      "Episodio 826: retorno=27.0\n",
      "Episodio 827: retorno=110.0\n",
      "Episodio 828: retorno=70.0\n",
      "Episodio 829: retorno=205.0\n",
      "Episodio 830: retorno=403.0\n",
      "Episodio 831: retorno=404.0\n",
      "Episodio 832: retorno=611.0\n",
      "Episodio 833: retorno=351.0\n",
      "Episodio 834: retorno=441.0\n",
      "Episodio 835: retorno=272.0\n",
      "Episodio 836: retorno=5000.0\n",
      "Episodio 837: retorno=124.0\n",
      "Episodio 838: retorno=107.0\n",
      "Episodio 839: retorno=57.0\n",
      "Episodio 840: retorno=87.0\n",
      "Episodio 841: retorno=5000.0\n",
      "Episodio 842: retorno=139.0\n",
      "Episodio 843: retorno=237.0\n",
      "Episodio 844: retorno=3321.0\n",
      "Episodio 845: retorno=185.0\n",
      "Episodio 846: retorno=181.0\n",
      "Episodio 847: retorno=250.0\n",
      "Episodio 848: retorno=363.0\n",
      "Episodio 849: retorno=233.0\n",
      "Episodio 850: retorno=35.0\n",
      "Episodio 851: retorno=195.0\n",
      "Episodio 852: retorno=226.0\n",
      "Episodio 853: retorno=361.0\n",
      "Episodio 854: retorno=22.0\n",
      "Episodio 855: retorno=23.0\n",
      "Episodio 856: retorno=68.0\n",
      "Episodio 857: retorno=40.0\n",
      "Episodio 858: retorno=201.0\n",
      "Episodio 859: retorno=194.0\n",
      "Episodio 860: retorno=23.0\n",
      "Episodio 861: retorno=124.0\n",
      "Episodio 862: retorno=127.0\n",
      "Episodio 863: retorno=132.0\n",
      "Episodio 864: retorno=2433.0\n",
      "Episodio 865: retorno=51.0\n",
      "Episodio 866: retorno=120.0\n",
      "Episodio 867: retorno=122.0\n",
      "Episodio 868: retorno=145.0\n",
      "Episodio 869: retorno=121.0\n",
      "Episodio 870: retorno=126.0\n",
      "Episodio 871: retorno=136.0\n",
      "Episodio 872: retorno=134.0\n",
      "Episodio 873: retorno=54.0\n",
      "Episodio 874: retorno=113.0\n",
      "Episodio 875: retorno=88.0\n",
      "Episodio 876: retorno=47.0\n",
      "Episodio 877: retorno=120.0\n",
      "Episodio 878: retorno=87.0\n",
      "Episodio 879: retorno=120.0\n",
      "Episodio 880: retorno=339.0\n",
      "Episodio 881: retorno=115.0\n",
      "Episodio 882: retorno=119.0\n",
      "Episodio 883: retorno=211.0\n",
      "Episodio 884: retorno=62.0\n",
      "Episodio 885: retorno=60.0\n",
      "Episodio 886: retorno=112.0\n",
      "Episodio 887: retorno=1017.0\n",
      "Episodio 888: retorno=118.0\n",
      "Episodio 889: retorno=487.0\n",
      "Episodio 890: retorno=120.0\n",
      "Episodio 891: retorno=115.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 892: retorno=119.0\n",
      "Episodio 893: retorno=13.0\n",
      "Episodio 894: retorno=116.0\n",
      "Episodio 895: retorno=44.0\n",
      "Episodio 896: retorno=117.0\n",
      "Episodio 897: retorno=140.0\n",
      "Episodio 898: retorno=139.0\n",
      "Episodio 899: retorno=130.0\n",
      "Episodio 900: retorno=159.0\n",
      "Episodio 901: retorno=124.0\n",
      "Episodio 902: retorno=429.0\n",
      "Episodio 903: retorno=14.0\n",
      "Episodio 904: retorno=127.0\n",
      "Episodio 905: retorno=124.0\n",
      "Episodio 906: retorno=121.0\n",
      "Episodio 907: retorno=13.0\n",
      "Episodio 908: retorno=140.0\n",
      "Episodio 909: retorno=131.0\n",
      "Episodio 910: retorno=125.0\n",
      "Episodio 911: retorno=10.0\n",
      "Episodio 912: retorno=124.0\n",
      "Episodio 913: retorno=124.0\n",
      "Episodio 914: retorno=126.0\n",
      "Episodio 915: retorno=124.0\n",
      "Episodio 916: retorno=124.0\n",
      "Episodio 917: retorno=119.0\n",
      "Episodio 918: retorno=15.0\n",
      "Episodio 919: retorno=123.0\n",
      "Episodio 920: retorno=121.0\n",
      "Episodio 921: retorno=126.0\n",
      "Episodio 922: retorno=124.0\n",
      "Episodio 923: retorno=113.0\n",
      "Episodio 924: retorno=123.0\n",
      "Episodio 925: retorno=108.0\n",
      "Episodio 926: retorno=128.0\n",
      "Episodio 927: retorno=21.0\n",
      "Episodio 928: retorno=124.0\n",
      "Episodio 929: retorno=141.0\n",
      "Episodio 930: retorno=14.0\n",
      "Episodio 931: retorno=28.0\n",
      "Episodio 932: retorno=125.0\n",
      "Episodio 933: retorno=122.0\n",
      "Episodio 934: retorno=127.0\n",
      "Episodio 935: retorno=170.0\n",
      "Episodio 936: retorno=60.0\n",
      "Episodio 937: retorno=63.0\n",
      "Episodio 938: retorno=35.0\n",
      "Episodio 939: retorno=60.0\n",
      "Episodio 940: retorno=41.0\n",
      "Episodio 941: retorno=204.0\n",
      "Episodio 942: retorno=121.0\n",
      "Episodio 943: retorno=213.0\n",
      "Episodio 944: retorno=28.0\n",
      "Episodio 945: retorno=123.0\n",
      "Episodio 946: retorno=121.0\n",
      "Episodio 947: retorno=22.0\n",
      "Episodio 948: retorno=77.0\n",
      "Episodio 949: retorno=61.0\n",
      "Episodio 950: retorno=141.0\n",
      "Episodio 951: retorno=52.0\n",
      "Episodio 952: retorno=163.0\n",
      "Episodio 953: retorno=115.0\n",
      "Episodio 954: retorno=152.0\n",
      "Episodio 955: retorno=80.0\n",
      "Episodio 956: retorno=253.0\n",
      "Episodio 957: retorno=117.0\n",
      "Episodio 958: retorno=57.0\n",
      "Episodio 959: retorno=15.0\n",
      "Episodio 960: retorno=76.0\n",
      "Episodio 961: retorno=109.0\n",
      "Episodio 962: retorno=50.0\n",
      "Episodio 963: retorno=68.0\n",
      "Episodio 964: retorno=54.0\n",
      "Episodio 965: retorno=92.0\n",
      "Episodio 966: retorno=100.0\n",
      "Episodio 967: retorno=119.0\n",
      "Episodio 968: retorno=142.0\n",
      "Episodio 969: retorno=43.0\n",
      "Episodio 970: retorno=114.0\n",
      "Episodio 971: retorno=69.0\n",
      "Episodio 972: retorno=63.0\n",
      "Episodio 973: retorno=119.0\n",
      "Episodio 974: retorno=55.0\n",
      "Episodio 975: retorno=25.0\n",
      "Episodio 976: retorno=25.0\n",
      "Episodio 977: retorno=127.0\n",
      "Episodio 978: retorno=123.0\n",
      "Episodio 979: retorno=156.0\n",
      "Episodio 980: retorno=117.0\n",
      "Episodio 981: retorno=12.0\n",
      "Episodio 982: retorno=124.0\n",
      "Episodio 983: retorno=32.0\n",
      "Episodio 984: retorno=122.0\n",
      "Episodio 985: retorno=123.0\n",
      "Episodio 986: retorno=122.0\n",
      "Episodio 987: retorno=46.0\n",
      "Episodio 988: retorno=120.0\n",
      "Episodio 989: retorno=41.0\n",
      "Episodio 990: retorno=126.0\n",
      "Episodio 991: retorno=131.0\n",
      "Episodio 992: retorno=125.0\n",
      "Episodio 993: retorno=184.0\n",
      "Episodio 994: retorno=128.0\n",
      "Episodio 995: retorno=163.0\n",
      "Episodio 996: retorno=123.0\n",
      "Episodio 997: retorno=167.0\n",
      "Episodio 998: retorno=76.0\n",
      "Episodio 999: retorno=126.0\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "list_retorno = []\n",
    "for i_episode in range(num_episodes):\n",
    "    #Inicializa o ambiente e coleta o estado inicial\n",
    "    state = env.reset()\n",
    "    #Inicializa as variáveis de retorno e eventos\n",
    "    retorno, steps = 0,0\n",
    "    #Inicializa a contagem de eventos no episódio\n",
    "    for t in count():\n",
    "        env.render() #Renderiza o ambiente\n",
    "        \n",
    "        action = select_action(torch.FloatTensor([state])).to(device) #Seleciona a ação a partir do estado\n",
    "        next_state, reward, done, _ = env.step(action[0].item()) #Executa a ação e coleta o prox estado, \n",
    "                                                                 # a recompensa e se o episódio foi finalizado\n",
    "        #Soma a recompensa ao retorno\n",
    "        retorno += reward\n",
    "\n",
    "        #Apesar da coleta da recompensa acima, recalculamos a recompensa de outra maneira,\n",
    "        # já que o cálculo de recompensa original não é muito eficiente\n",
    "        if done:\n",
    "            if steps < 30:\n",
    "                reward -= 10\n",
    "            else:\n",
    "                reward = -1\n",
    "        if steps > 100: reward += 1\n",
    "        if steps > 200: reward += 1\n",
    "        if steps > 300: reward += 1\n",
    "\n",
    "\n",
    "        #Guarda a experiência na memória\n",
    "        memory.push(torch.FloatTensor([state]),\n",
    "                     action,  # action is already a tensor\n",
    "                     torch.FloatTensor([next_state]),\n",
    "                     torch.FloatTensor([reward]).to(device))\n",
    "\n",
    "        #Move para o proximo estado e atualiza o ponteiro de eventos\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        #Otimiza a rede neural do crítico\n",
    "        optimize_model()\n",
    "        #Finaliza o episódio se tiver perdido ou após 5 mil eventos\n",
    "        if done or (steps==5000):\n",
    "            break\n",
    "            \n",
    "    #Atualiza a rede target\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #Printa o retorno do episódio e guarda em uma lista\n",
    "    print(f'Episodio {i_episode}: retorno={round(retorno,2)}')\n",
    "    list_retorno.append(retorno)\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-47823a45504a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_retorno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Retorno'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episódios'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(1,num_episodes+1),list_retorno)\n",
    "plt.ylabel('Retorno')\n",
    "plt.xlabel('Episódios')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the diagram that illustrates the overall resulting data flow.\n",
    "\n",
    ".. figure:: /_static/img/reinforcement_learning_diagram.jpg\n",
    "\n",
    "Actions are chosen either randomly or based on a policy, getting the next\n",
    "step sample from the gym environment. We record the results in the\n",
    "replay memory and also run optimization step on every iteration.\n",
    "Optimization picks a random batch from the replay memory to do training of the\n",
    "new policy. \"Older\" target_net is also used in optimization to compute the\n",
    "expected Q values; it is updated occasionally to keep it current.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ce0952ff6b7703ad76d22f6fe0d43fce5ecd4387907aaa0af2410b7aa492de3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
