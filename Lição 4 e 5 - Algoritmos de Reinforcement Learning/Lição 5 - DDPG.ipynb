{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-836RadfIBp"
   },
   "source": [
    "# Lição 5 - Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9qS51y4fIBt"
   },
   "source": [
    "## 1. Introdução <a class=\"anchor\" name=\"section_1\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRpHMoGlfIBu"
   },
   "source": [
    "Na última lição, introduzimos a *Deep Q-Network* (DQN), a primeira estrutura do DRL, que mesclava conceitos de RL e conceitos DL. Nós mostramos que é possível encontrar uma política capaz de solucionar o problema do *cartople*, um problema complexo para os algoritmos de outras áreas. Entretanto, depois de 2013, ano de criação do DQN, vários outros algoritmos de DRL foram criados para solucionar algumas limitações que a estrutura pioneira apresentava. Nesse sentido surge o *Deep Deterministic Policy Gradient* (DDPG), um algoritmo clássico que nos fornece flexibilidade para trabalhar tanto com ações discretas (como o DQN), como com ações contínuas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxLFMBM5fIBv"
   },
   "source": [
    "### 1.1 Índice <a class=\"anchor\" name=\"section_1.1\"></a>\n",
    "\n",
    "* [1. Introdução](#section_1)\n",
    "    * [1.1 Índice](#section_1.1)\n",
    "* [2. O problema do Pêndulo](#section_2)\n",
    "* [3. Redes do tipo Actor-Critic](#section_3)\n",
    "    * [3.1 Deep Deterministic Gradient Policy - DDPG](#section_3.1)\n",
    "    * [3.2 Estratégias de treinamento do DDPG](#section_3.2)\n",
    "        * [3.2.1 Ruído nos parâmetros para exploração](#section_3.2.1)\n",
    "* [4. Implementação do algoritmo](#section_4)\n",
    "* [5. Sugestões](#section_5)\n",
    "* [6. Referências](#section_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yynkOwWKfIBv"
   },
   "source": [
    "Iniciaremos o problema importando as bibliotecas que usaremos ao longo da nossa implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "p7a5I4V9fIBw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZERgxUgfIBw"
   },
   "source": [
    "Antes de iniciarmos a implementação do código de fato, vamos verificar se há alguma GPU disponível para rodar os códigos, com o intuito de tornar mais rápido o treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uaY0FBhYfIBx",
    "outputId": "d3fb04ff-50b4-40e2-8aa8-856b576eac2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando na GPU\n"
     ]
    }
   ],
   "source": [
    "def testar_gpu():\n",
    "    '''\n",
    "    Função que testa se a GPU está habilitada.\n",
    "    \n",
    "    Outputs:\n",
    "    --------\n",
    "    device: {str}\n",
    "        variável que define se o código será executado na CPU ou na GPU\n",
    "    '''\n",
    "    train_on_gpu = torch.cuda.is_available() #Observa se a GPU está disponivel\n",
    "    if train_on_gpu: #Se sim\n",
    "        device = torch.device('cuda') #Seleciona o device como GPU\n",
    "        print(\"Treinando na GPU\") #E manda a mensagem\n",
    "    else: #Se não\n",
    "        device = torch.device('cpu') #Seleciona o device como cpu\n",
    "        print(\"GPU indisponível, treinando na CPU\") #E avisa que a GPU não esta disponível\n",
    "    \n",
    "    return device\n",
    "\n",
    "#Coleta onde o ambiente será executado\n",
    "device = testar_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTXphLiEfIBz"
   },
   "source": [
    "Como apresentado na última lição, implementaremos a definição de transição de um ambiente DRL, com estado, ação, próximo estado e recompensa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1AXKrjdqfIBz"
   },
   "outputs": [],
   "source": [
    "#Define uma transição como um tupla com estado, ação, próximo estado e recompensa.\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI0Q2bBxfIB0"
   },
   "source": [
    "## 2. O problema do Pêndulo <a class=\"anchor\" name=\"section_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV6WGHoTfIB0"
   },
   "source": [
    "A última lição mostrou uma implementação de uma *Deep Q-Network*, a rede mais básica de DRL, que nos permitiu elaborar um algoritmo que conseguisse decidir quais ações devem ser tomadas pelo *cartpole*, para evitar que a haste caia. \n",
    "\n",
    "Entretanto, esse algoritmo apresenta uma clara limitação: ela só consegue resolver problemas em que as ações possíveis são **discretas**. Isso é perceptível ao perceber que a rede retorna apenas **qual** ação deve ser tomada, e não **como** ela deve ser executada. Passemos a um outro problema para que isso fique mais claro.\n",
    "\n",
    "Pense em um jogo parecido com o *cartpole*, mas agora nós queremos equilibrar um pêndulo verticalmente na posição mais alta possível. Para isso, nós podemos aplicar forças no pêndulo tanto no sentido horário como no anti-horário, tal que a intensidade dessa força é um valor contínuo, de forma que a rotação pode ter mais ou menos intensidade, a depender da força aplicada.\n",
    "\n",
    "Assim, a rotação do pêndulo é controlada por um agente que define uma ação tal que $-2 < a < 2$, em que os valores negativos indicam que as forças são no sentido anti-horário e os positivos que são no sentido horário, enquanto o módulo do ação tal que $|a|<2$, indica a intensidade da força aplicada. A imagem abaixo mostra o pêndulo em vermelho e a representação da força aplicada como a seta preta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVQYC6n5fIB1"
   },
   "source": [
    "<center>\n",
    "<img src=\"imagens/pendulum.PNG\"></img>\n",
    "</center>\n",
    "Figura 1 - Pêndulo e força aplicada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jFvKhYzfIB2"
   },
   "source": [
    "Repare que a ação desse ambiente é **contínua**, isto é, não basta o agente saber que precisa aplicar uma força em um ou outro sentido, ele precisa entender **o quanto** de intensidade ele deve colocar na força que será aplicada.\n",
    "\n",
    "Voltemos, então, ao DQL. Para implementar uma DQN, neste caso, precisaríamos discretizar a intensidade da força e combiná-las com o sentido de rotação, considerando que cada uma dessas combinações seja uma ação possível com um $Q(s,a)$ calculável. \n",
    "\n",
    "Perceba que essa não é uma abordagem eficaz mesmo para um problema relativamente simples como esse. Identificando essa limitação, desenvolveu-se outra arquitetura que consegue ultrapassar essa barreira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Llj13MTafIB3"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0') #Construção do ambiente do pendulo\n",
    "#env = gym.make('MountainCarContinuous-v0') #Construção do ambiente do mountain car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAqHqKQlfIB3"
   },
   "source": [
    "## 3. Redes do tipo Actor-Critic <a class=\"anchor\" name=\"section_3\"></a>\n",
    "\n",
    "É importante ressaltar que o termo *Actor-critic* **não** designa uma arquitetura de DRL, e sim uma característica que várias arquiteturas da área incorporaram, entre elas o A2C, A3C e o DDPG. Nesse notebook, apresentaremos e utilizaremos o DDPG.\n",
    "\n",
    "Dizer que uma arquitetura é *actor-critic*, significa dizer que ela utiliza, pelo menos, **duas** redes neurais no seu processo de aprendizagem: uma que toma a ação e outra que critica a ação tomada, daí surge o nome *actor-critic*.\n",
    "\n",
    "A rede do ator recebe o estado como entrada e produz uma saída, podendo ser um número ou um vetor. Após a tomada de decisão do ator, o crítico vai estimar o valor de $Q(s,a)$ a partir do estado e dessa ação realizada, tentando avaliar se a ação tomada foi boa ou ruim.\n",
    "\n",
    "Assim, percebemos que essas arquiteturas desagregam o tomador de decisão e o estimador do $Q(s,a)$, de forma que a saída do ator pode ser de natureza contínua, cabendo ao crítico realizar a estimativa do retorno a partir dessa ação tomada. Entendido isso, precisamos entender como as duas redes aprendem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaaUmBeMfIB4"
   },
   "source": [
    "### 3.1 Deep Deterministic Gradient Policy - DDPG <a class=\"anchor\" name=\"section_3.1\"></a>\n",
    "\n",
    "Repare que o problema para o agente ainda é o mesmo: precisamos desenvolver um algoritmo capaz de maximizar o retorno. Nesse cenário, como treinar o ator?\n",
    "\n",
    "Como o $Q$ é uma estimativa do retorno recebido, é possível treinar o ator através da saída do crítico: se a saída deste for alta, o ator entenderá que agiu de maneira adequada, caso contrário, o ator terá que mudar a sua política de decisão de ação, modificando os seus parâmetros.\n",
    "\n",
    "Certo, mas como treinaremos o crítico para que ele consiga fazer estimativas de $Q(s,a)$ de maneira acurada, de forma que o ator possa \"confiar\" nessa estimativa para atualizar os seus pesos? A resposta é a mesma dada para o aprendizado da DQN, a partir da **Equação de Bellman**:\n",
    "\n",
    "$$ Q(s,a) = \\mathbb{E}_{s'\\mathtt{\\sim}P}\\{r_t + \\mathbb{E}_{a'\\mathtt{\\sim}\\mu}[Q(s',a')]\\} $$\n",
    "\n",
    "Rememorando essa equação, ela nos mostra que o valor do par estado-ação atual pode ser estimado através do valor do par estado-ação futuro, somado a uma recompensa. Apesar de ambos os lados dependerem de estimativas do $Q$, o fato de não haver incerteza na medição da recompensa faz com que o lado direito da equação seja mais acurado, de forma que ele é tratado como o valor alvo para a otimização.\n",
    "\n",
    "Assim podemos resumir o processo de aprendizado utilizando a imagem abaixo. Na figura, as setas verdes significam a passagem de entradas e saídas até a estimativa dos dois lados da equação de Bellman e as setas vermelhas mostram o processo de atualização dos parâmetros das redes a partir dos custos calculados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sImaaTEzfIB5"
   },
   "source": [
    "<center>\n",
    "<img src=\"imagens/esquematico_DDPG.PNG\"></img>\n",
    "</center>\n",
    "\n",
    "Figura 2 - Processo de Aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ0By3RhfIB5"
   },
   "source": [
    "Na imagem acima, a função de custo do crítico ilustrada pode ser de diferentes naturezas. A mais clássica para identificar a diferença entre duas grandezas é o Erro Quadrático Médio (*Mean Squared Error* - MSE) e é ela que será utilizada. \n",
    "\n",
    "Para atualizar o ator, nós queremos promover o **aumento** do $Q(s',a')$ a cada ação executada, estimulando ações com alto valor do estado-ação. Pensando então no processo de aprendizado, sabemos que a maioria dos algoritmos se baseiam na **diminuição** do gradiente da perda, como adequar o problema?\n",
    "\n",
    "Uma solução é utilizar a perda como $-Q(s',a')$, de forma que, quanto maior o $Q$, menor a função de perda $-Q$ e, consequentemente, menor serão as atualizações dos parâmetros do ator. Em um caso contrário, a rede entenderá que as suas ações estão distantes do objetivo e que a política de tomada das ações deve ser modificada.\n",
    "\n",
    "Para entender mais sobre o processo de treinamento do DDPG, o pseudo-código pode ser encontrado neste [link](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK-KgbJTfIB6"
   },
   "source": [
    "### 3.2 Estratégias de treinamento do DDPG <a class=\"anchor\" name=\"section_3.2\"></a>\n",
    "\n",
    "Como foi dito, o treinamento das arquiteturas de DRL é um processo instável e, por isso, adotam-se algumas técnicas para tentar diminuir essa instabilidade. Entre essas técnicas estão:\n",
    "\n",
    "*    Experience Replay\n",
    "*    Target Network\n",
    "*    Utilização de ruído nos parâmetros\n",
    "\n",
    "As duas primeiras estratégias já foram explicadas na última lição, mas a última não, então faremos uma breve introdução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNsc4J9AfIB7"
   },
   "source": [
    "#### 3.2.1 Ruído nos parâmetros para exploração <a class=\"anchor\" name=\"section_3.2.1\"></a>\n",
    "\n",
    "Como foi visto, em problemas com ações discretas, é possível realizar a exploração do ambiente escolhendo algumas ações de maneira aleatória (utilizando a técniga do $\\epsilon$-greedy, por exemplo). Já em problemas com ações contínuas, o mais comum é que se adicione ruído gaussiano na saída da rede neural, variando essas ações de maneira aleatória, como mostrado nessa [publicação](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b), na seção \"*Exploration*\".\n",
    "\n",
    "Entretanto, recentemente surgiu uma nova abordagem para essa exploração. Propôs-se que, em vez de adicionar o mecanismo de exploração após o funcionamento da rede, é possível adicionar esse mecanismo **durante** a atuação da rede neural.\n",
    "\n",
    "A ideia é que nós adicionemos um ruído aleatório nos parâmetros da rede, alterando todos os pesos desta. Essa aleatoriedade garante que a rede tome ações diferentes mesmo em situações parecidas, trazendo a ideia de exploração. Essa [tese de mestrado](https://matthiasplappert.com/publications/2017_Plappert_Master-thesis.pdf) abordou o assunto de forma bastante simples e completa. Será essa a estratégia utilizada no nosso código.\n",
    "\n",
    "Basicamente, a proposta é, a cada episódio, contaminar os parâmetros da rede neural com amostras de um ruído gaussiano de média 0 e desvio padrão $\\sigma$. Essas amostras são geradas aleatoriamente a cada episódio, respeitando a distribuição gaussiana em questão. A imagem abaixo, retirada da tese citada acima, ilustra a diferença entre o ruído na ação e o ruído nos parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLcBaJ6jfIB7"
   },
   "source": [
    "<center>\n",
    "<img src=\"imagens/parameter_noise.PNG\" width=600 height=200></img>\n",
    "</center>\n",
    "\n",
    "Figura 3 - Diferença entre o ruído na ação e o ruído nos parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvSqYZNSfIB8"
   },
   "source": [
    "Essa estratégia também é adaptativa, isto é, ela muda as configurações dos parâmetros de ruído, para aumentar ou diminuir o efeito exploratório. De maneira prática, com o passar dos episódios, o desvio padrão $\\sigma$ do ruído é modificado, aumentando o seu valor quando se deseja aumentar a exploração e diminuindo o seu valor quando se deseja diminuir a exploração.\n",
    "\n",
    "Essa decisão é feita analisando se a diferença entre as decisões feitas com e sem ruído é maior do que um certo limiar de decisão $\\delta$, de forma que, se for maior, o $\\sigma$ deve ser diminuído, se for menor, o $\\sigma$ deve ser aumentado. A diferença entre essas ações ruidosas e limpas geralmente é calculada utilizando o Erro Quadrático Médio, como mostrado na imagem abaixo com a função $d(\\cdot,\\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyjJZZdGfIB8"
   },
   "source": [
    "<center>\n",
    "<img src=\"imagens/distance.PNG\" width=500 height=100></img>\n",
    "</center>\n",
    "\n",
    "Figura 4 - Diferença entre as ações tomadas pela rede neural original e pela rede neural ruidosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgkeS_nrfIB9"
   },
   "source": [
    "Dessa forma, a adaptação no parâmetro $\\sigma$ é feita a partir de um certo $\\alpha$ maior que $1$, geralmente com um valor ligeiramente maior do que 1. Essa adaptação é feita a partir do equacionamento abaixo:\n",
    "\n",
    "$$ \\sigma_{t+1} = \\left\\{\\begin{array}{ll}\n",
    "\\alpha \\sigma_t, \\:\\:\\:\\: d(\\pi(a|s), \\tilde{\\pi}(a|s)) \\leq \\delta \\\\  \n",
    "\\frac{1}{\\alpha} \\sigma_t,\\:\\:\\:\\: cc \\\\\n",
    "\\end{array} \\right. $$\n",
    "\n",
    "em que $d(\\cdot,\\cdot)$ é uma função de cálculo de distância entre vetores, $\\pi(a|s)$ são as ações determinadas pela rede não contaminada com ruído, $\\tilde{\\pi}(a|s)$ são as ações determinadas pela rede com o ruído nor parâmetros, $\\delta$ é o limiar de decisão de adaptação e $\\alpha$ é o parâmetro de adaptação.\n",
    "\n",
    "Na prática, nós criamos uma outra rede neural auxiliar que será sempre uma cópia da rede neural do agente, mas com os parâmetros contaminados, e utilizamos ela para a tomada de decisões no processo de treinamento. Entretanto, na hora de otimizar as redes, o algoritmo otimiza a rede neural **não** contaminada, atualizando seus parâmetros e copiando-os novamente nessa rede auxiliar, que será novamente contaminada com um ruído aleatório para o próximo episódio.\n",
    "\n",
    "Seguindo esse processo e o equacionamento mostrado, o algoritmo atualiza o valor de $\\sigma$ do ruído a cada $T_{adapt}$ episódios, garantindo uma exploração mais inteligente e duradoura. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5mdUSfefIB9"
   },
   "source": [
    "A partir do que foi explanado acima, podemos partir pra implementação do nosso código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5Z8HjWZfIB9"
   },
   "source": [
    "## 4. Implementação do algoritmo <a class=\"anchor\" name=\"section_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "678mkNsmfIB-"
   },
   "source": [
    "Iniciaremos o problema implementando a memória do *replay buffer*. A implementação será a mesma utilizada na última lição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pOipt3WefIB-"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        '''\n",
    "        Classe que define a memória do Replay Buffer\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        capacity: {int}\n",
    "            número de elementos que podem ser armazenados na memória.\n",
    "        '''\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        '''\n",
    "        Função que salva uma transição na memória\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        *args: {collections.namedtuple}\n",
    "            transições que devem ser armazenadas na memória.\n",
    "        ''' \n",
    "        if len(self.memory) < self.capacity: #Checa se a memória está cheia e, se não estiver,\n",
    "            self.memory.append(None)         # cria um espaço vazio em que a transição será colocada\n",
    "        \n",
    "        self.memory[self.position] = Transition(*args) #armazena na memória usando o ponteiro self.position\n",
    "        self.position = (self.position + 1) % self.capacity #atualiza o ponteiro\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Função que amostra a memória, pegando batch_size's transições aleatoriamente.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        batch_size: {int}\n",
    "            Número de transições que serão coletadas na amostra.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        random.sample(self.memory, batch_size): {list}\n",
    "            lista com batch_size's transições, escolhidas aleatoriamente.\n",
    "        '''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Função que coleta o número de transições guardadas na memória.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        len(self.memory): {int}\n",
    "            Número de transições armazenadas.\n",
    "        '''\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuBWPm-EfIB_"
   },
   "source": [
    "Após a implementação da memória, podemos implementar as redes neurais do agente e do crítico. Elas são duas DNNs simples, constituídas por duas camadas lineares e uma camada de saída. Esperamos que você já tenha familiaridade com esse tipo de implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "v9onzwlIfIB_"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, outputs):\n",
    "        '''\n",
    "        Classe que define a rede neural do agente do algoritmo.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        input_size: {int}\n",
    "            tamanho da camada de entrada da rede;\n",
    "        outputs: {int}\n",
    "            tamanho da camada de saída da rede\n",
    "        '''\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        #Define a primeira camada linear com normalização\n",
    "        self.linear1 = nn.Linear(input_size, 32)\n",
    "        self.ln1 = nn.LayerNorm(32)\n",
    "\n",
    "        #Define a segunda camada linear com normalização\n",
    "        self.linear2 = nn.Linear(32, 64)\n",
    "        self.ln2 = nn.LayerNorm(64)\n",
    "\n",
    "        #Define a camada de saída\n",
    "        self.linear3 = nn.Linear(64, outputs)\n",
    "        \n",
    "        self.tanh = nn.Tanh() #Definição da função de ativação tangente hiperbólica.\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Função que recebe o estado e decide a ação seguindo a sua política.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        state: {torch.tensor}\n",
    "            tensor que representa o estado do ambiente.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        action: {torch.tensor}\n",
    "            tensor com a ação decidida.\n",
    "        '''\n",
    "        out = nn.functional.relu(self.ln1(self.linear1(state))) #Execução da primeira camada\n",
    "        out = nn.functional.relu(self.ln2(self.linear2(out))) #Execução da segunda camada\n",
    "        action = self.tanh(self.linear3(out)) #Execução da camada de saída\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5DjQQGHffICA"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, outputs):\n",
    "        '''\n",
    "        Classe que define a rede neural do crítico do algoritmo.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        state_size: {int}\n",
    "            tamanho do estado do ambiente;\n",
    "        action_size: {int}\n",
    "            tamanho da ação do ambiente;    \n",
    "        outputs: {int}\n",
    "            tamanho da camada de saída da rede\n",
    "        '''\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        #Define a primeira camada linear com normalização\n",
    "        self.linear1 = nn.Linear(state_size, 32)\n",
    "        self.ln1 = nn.LayerNorm(32)\n",
    "\n",
    "        #Define a segunda camada linear com normalização\n",
    "        self.linear2 = nn.Linear(32 + action_size, 64) #A ação é adicionada apenas nessa segunda camada\n",
    "        self.ln2 = nn.LayerNorm(64)\n",
    "\n",
    "        #Define a camada de saída\n",
    "        self.linear3 = nn.Linear(64, outputs)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        '''\n",
    "        Função que recebe o estado e a ação e retorna o valor do estado-ação, o Q(s,a).\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        state: {torch.tensor}\n",
    "            tensor que representa o estado do ambiente.\n",
    "        action: {torch.tensor}\n",
    "            tensor que representa a ação realizada.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        Q: {torch.tensor}\n",
    "            tensor o valor do estado-ação, o Q(s,a).\n",
    "        '''\n",
    "        s1 = nn.functional.relu(self.ln1(self.linear1(state))) #Execução da primeira camada apenas com o estado\n",
    "        out = torch.cat((s1,action), dim=1) #Concatenação da saída da primeira camada com a ação realizada\n",
    "        out = nn.functional.relu(self.ln2(self.linear2(out))) #Execução da segunda camada\n",
    "        Q = self.linear3(out) #Execução da camada de saída\n",
    "        \n",
    "        return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NthyGK_DfICA"
   },
   "source": [
    "Finalizada a implementação da memória e das redes neurais, precisamos definir as configurações do nosso algoritmo. Primeiro, faremos uma investigação no nosso ambiente, analisando qual é o tamanho do estado, das ações esperadas e em qual intervalo a ação deve estar contida. Nas linhas de código abaixo guardamos essas informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bQBgrO-afICA",
    "outputId": "8326c3d4-3af2-4fc6-ccd9-4ee4dda2c11d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 2.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = int(env.observation_space.shape[0]) #shape do estado\n",
    "output_size = int(env.action_space.shape[0]) #shape da ação\n",
    "max_action = float(env.action_space.high[0]) #range válido para a ação\n",
    "input_size, output_size, max_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiN29TZzfICB"
   },
   "source": [
    "Além da configuração do ambiente, também precisamos configurar o nosso algoritmo. Na célula abaixo, nós definiremos diversos parâmetros importantes para o treinamento. Começamos com os mais tradicionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GV_7c-eBfICB"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #Tamanho do batch\n",
    "GAMMA = 0.95 #Taxa de aprendizado\n",
    "tau = 0.05 #Parâmetro de suavização da sincronização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAs14N1rfICB"
   },
   "source": [
    "Agora, precisamos configurar nossos mecanismos de exploração. Nesse caso, nós disponibilizamos uma implementação do ruído nos parâmetros e uma do ruído na ação. Dessa forma, será possível comparar ambas as estratégias. As configurações estão disponíveis abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Adhwb4bgfICC"
   },
   "outputs": [],
   "source": [
    "# Configuração do ruído no parâmetro\n",
    "sigma = 0.25 #Desvio padrão do ruído gaussiano que será inserido nos parâmetros\n",
    "alfa = 1.03 #Parâmetro de adaptação do ruído nos parâmetros\n",
    "delta = 1e-1 #Limiar de decisão de correção do ruído nos parâmetros\n",
    "T_adapt = 5 #Número de episódios entre as atualizações do ruído nos parâmetros\n",
    "\n",
    "# Configuração do ruído na ação\n",
    "desv_pad_ac = 0.5 #Desvio padrão do ruído gaussiano que será inserido na ação\n",
    "alfa_ac = 1.08 #Parâmetro de adaptação do ruído na ação\n",
    "T_adapt_ac = 5 #Número de episódios entre as atualizações do ruído na ação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUhDhxmpfICC"
   },
   "source": [
    "Por fim, devemos instanciar as nossas redes neurais, sincronizá-las e configurar os otimizadores e funções de perda para aprendizado das redes. Tudo isso está sendo feito na célula abaixo, além do instanciamento da memória, com capacidade de armazenamento de 10 mil transições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aQLevwMWfICC"
   },
   "outputs": [],
   "source": [
    "# Definição das redes do agente, incluindo a target e a utilizada para implementação do ruído nos parâmetros\n",
    "actor_net = Actor(input_size, output_size).to(device)\n",
    "actor_noise_net = Actor(input_size, output_size).to(device)\n",
    "actor_target_net = Actor(input_size, output_size).to(device)\n",
    "\n",
    "# Definição das redes do crítico, incluindo a target\n",
    "critic_net = Critic(input_size, output_size, 1).to(device)\n",
    "critic_target_net = Critic(input_size, output_size, 1).to(device)\n",
    "\n",
    "# Sincronização das redes\n",
    "actor_target_net.load_state_dict(actor_net.state_dict())\n",
    "actor_target_net.eval()\n",
    "actor_noise_net.load_state_dict(actor_net.state_dict())\n",
    "actor_noise_net.eval()\n",
    "critic_target_net.load_state_dict(critic_net.state_dict())\n",
    "critic_target_net.eval()\n",
    "\n",
    "# Definição dos otimizadores\n",
    "optimizer_critic = torch.optim.Adam(critic_net.parameters(), lr=0.001)\n",
    "optimizer_actor = torch.optim.Adam(actor_net.parameters(), lr=0.0001)\n",
    "\n",
    "# Instanciamento da memória\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Definição das funções de perda\n",
    "loss_critic = nn.MSELoss() #Função de perda para otimização do crítico\n",
    "loss_d = nn.MSELoss() #Função para ajuste dos parâmetros para utilização do ruído nos parâmetros para exploração\n",
    "\n",
    "# Inicialização das variáveis de perda\n",
    "actor_loss = None\n",
    "critic_loss = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iehjIqFafICD"
   },
   "source": [
    "A partir de todas essas configurações, podemos definir a função responsável por selecionar a ação que deve ser executada, levando em conta tanto a política, como a estratégia de exploração adotada. Essa função está implementada abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d7f0NBZRfICD"
   },
   "outputs": [],
   "source": [
    "def select_action(state, action_noise=None, param_noise=None):\n",
    "    '''\n",
    "    Função que recebe o estado e decide a ação, levando em conta a política e o mecanismo de exploração.\n",
    "        \n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    state: {torch.tensor}\n",
    "        tensor que representa o estado do ambiente.\n",
    "    action_noise: {boolean}\n",
    "        flag que define se utilizaremos os ruídos na ação.\n",
    "    param_noise: {boolean}\n",
    "        flag que define se utilizaremos os ruídos nos parâmetros.\n",
    "       \n",
    "    Outputs:\n",
    "    --------\n",
    "    mu: {torch.tensor}\n",
    "        tensor com a ação escolhida pela política.\n",
    "    '''\n",
    "    #Definição das redes em estado de avaliação e não de treino\n",
    "    actor_net.eval()\n",
    "    actor_noise_net.eval()\n",
    "    \n",
    "    if param_noise is not None:  #Se usar o ruído nos parâmetros,\n",
    "        mu = actor_noise_net(state.float()) # utilize a rede ruidosa\n",
    "    else: #Se não usar\n",
    "        mu = actor_net(state.float()) # utilize a rede sem ruído\n",
    "    \n",
    "    mu = mu.data #Correção no formato do tensor\n",
    "    \n",
    "    if action_noise is not None: #Se usar o ruído na ação,\n",
    "        with torch.no_grad(): \n",
    "            # soma o ruído na saída da rede\n",
    "            mu = torch.Tensor(mu.cpu().numpy() + np.random.normal(0,desv_pad_ac,output_size)).to(device) \n",
    "    \n",
    "    mu = mu.squeeze() #corrige o formato do tensor\n",
    "    mu = max_action*mu.clamp(min=-1,max=1) #multiplica a ação no range válido\n",
    "    \n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFY164UzfICE"
   },
   "source": [
    "Também definimos uma função responsável por fazer a sincronização suavizada entre os parâmetros de duas redes neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "q1u5GCSIfICE"
   },
   "outputs": [],
   "source": [
    "def weightSync(target_model, source_model):\n",
    "    '''\n",
    "    Função que executa a sincronização das redes de maneira suavizada\n",
    "    \n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    target_model: {torch.nn}\n",
    "        rede neural que receberá os parâmetros da rede source_model de maneira suavizada.\n",
    "    source_model: {torch.nn}\n",
    "        rede neural que fornecerá os parâmetros para a outra rede.\n",
    "    '''\n",
    "    #Copia os parâmetros, suavizando a sincronização\n",
    "    for parameter_target, parameter_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        parameter_target.data.copy_((1 - tau) * parameter_target.data + tau * parameter_source.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNwY9gSvfICF"
   },
   "source": [
    "Por fim, devemos implementar a função de otimização das redes neurais, onde a equação de Bellman será implementada e as redes serão atualizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "C810SZ4CfICF"
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    '''\n",
    "    Função utilizada para aprendizado das redes neurais\n",
    "        \n",
    "    Outputs:\n",
    "    --------\n",
    "    state_batch: {torch.tensor}\n",
    "        batch com os tensores que representam os estados.\n",
    "    action_batch: {torch.tensor}\n",
    "        batch com os tensores que representam as ações.\n",
    "    OBS: essas saídas servem para, posteriormente, ajustar a configuração do ruído nos parâmetros\n",
    "    '''\n",
    "    if len(memory) < BATCH_SIZE: #Verifica se a memória ja tem BATCH_SIZE's amostras\n",
    "        return\n",
    "    \n",
    "    #Define que as redes estão em treinamento\n",
    "    actor_net.train()\n",
    "    critic_net.train()\n",
    "    actor_noise_net.train()\n",
    "    \n",
    "    #Amostra a memória em BATCH_SIZE's transições\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    #Define os tensores que serão utilizados no treinamento, \n",
    "    # agrupando os estados, ações e recompensas de todas as transições\n",
    "    next_state_batch = torch.autograd.Variable(torch.cat(batch.next_state)).to(device)\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = []\n",
    "    for tupla in batch.action:\n",
    "        action_batch.append(np.array(tupla.cpu()))\n",
    "    action_batch = torch.from_numpy(np.array(action_batch)).view(-1,1).to(device)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    #Calcula as ações a partir da rede target recebendo os estados vivenciados\n",
    "    next_actor_action_value = actor_target_net(next_state_batch.float())\n",
    "    #Calcula os Q(s,a) a partir dos estados e das ações calculadas pela rede agente do target\n",
    "    with torch.no_grad():\n",
    "        next_state_values = critic_target_net(next_state_batch.float(), next_actor_action_value.float()).squeeze()\n",
    "\n",
    "    #Calcula o lado direito da equação de Bellman\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    ### Otimização do crítico ###\n",
    "    optimizer_critic.zero_grad()\n",
    "    #Cálculo do Q usando a rede que NÃO é target e as ações decididas originalmente\n",
    "    state_action_values = critic_net(state_batch.float(), action_batch.float())\n",
    "    #Calcula a diferença entre os dois lados da equação de Bellman\n",
    "    critic_loss = loss_critic(state_action_values.float().squeeze(), expected_state_action_values.float())\n",
    "    #Otimiza os parâmetros da rede\n",
    "    critic_loss.backward()\n",
    "    optimizer_critic.step()\n",
    "    ###  ###\n",
    "    \n",
    "    ### Otimização do agente ###\n",
    "    optimizer_actor.zero_grad()\n",
    "    #Cálculo do -Q usando a rede que NÃO é target e as decisões do agente que NÃO é target\n",
    "    gradient = -critic_net(state_batch.float(), actor_net(state_batch.float()).float()) \n",
    "    #Calculo da função de perda como a média dos -Q estimados na linha acima\n",
    "    actor_loss = torch.mean(gradient)\n",
    "    #Otimiza os parâmetros da rede\n",
    "    actor_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    ###  ###\n",
    "    \n",
    "    #Sincronização suavizada entre redes originais e redes target\n",
    "    weightSync(actor_target_net, actor_net)\n",
    "    weightSync(critic_target_net, critic_net)\n",
    "    \n",
    "    return state_batch, action_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51Scu4gYfICG"
   },
   "source": [
    "Com todos os elementos configurados e todas as funções necessárias definidas, podemos realizar o processo de treinamento. Ele será feito em 100 episódios e, inicialmente, utilizaremos o ruído nos parâmetros para exploração. O retorno por episódio será armazenado para ser plotado, posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vLIW3E_NfICG",
    "outputId": "283f841a-46b2-4ae8-d1a0-640ed19640cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 0: retorno=-112.92\n",
      "Episodio 1: retorno=-132.74\n",
      "Episodio 2: retorno=-152.29\n",
      "Episodio 3: retorno=-95.38\n",
      "\n",
      " Sigma:  0.4629629629629629\n",
      "Episodio 4: retorno=-25.65\n",
      "Episodio 5: retorno=-25.73\n",
      "Episodio 6: retorno=-12.11\n",
      "Episodio 7: retorno=-23.42\n",
      "Episodio 8: retorno=-25.7\n",
      "\n",
      " Sigma:  0.42866941015089155\n",
      "Episodio 9: retorno=-0.1\n",
      "Episodio 10: retorno=-24.0\n",
      "Episodio 11: retorno=-24.95\n",
      "Episodio 12: retorno=-37.67\n",
      "Episodio 13: retorno=-12.85\n",
      "\n",
      " Sigma:  0.39691612051008474\n",
      "Episodio 14: retorno=-22.91\n",
      "Episodio 15: retorno=-12.34\n",
      "Episodio 16: retorno=-23.39\n",
      "Episodio 17: retorno=-11.72\n",
      "Episodio 18: retorno=-12.43\n",
      "\n",
      " Sigma:  0.3675149263982266\n",
      "Episodio 19: retorno=-12.63\n",
      "Episodio 20: retorno=-12.5\n",
      "Episodio 21: retorno=-24.72\n",
      "Episodio 22: retorno=-12.63\n",
      "Episodio 23: retorno=-24.02\n",
      "\n",
      " Sigma:  0.34029159851687646\n",
      "Episodio 24: retorno=-12.01\n",
      "Episodio 25: retorno=-12.7\n",
      "Episodio 26: retorno=-11.73\n",
      "Episodio 27: retorno=-11.85\n",
      "Episodio 28: retorno=-34.27\n",
      "\n",
      " Sigma:  0.31508481344155226\n",
      "Episodio 29: retorno=-12.63\n",
      "Episodio 30: retorno=-12.69\n",
      "Episodio 31: retorno=-25.11\n",
      "Episodio 32: retorno=-24.25\n",
      "Episodio 33: retorno=-22.68\n",
      "\n",
      " Sigma:  0.2917451976310669\n",
      "Episodio 34: retorno=-12.23\n",
      "Episodio 35: retorno=-25.34\n",
      "Episodio 36: retorno=-24.45\n",
      "Episodio 37: retorno=-23.93\n",
      "Episodio 38: retorno=-24.3\n",
      "\n",
      " Sigma:  0.2701344422509878\n",
      "Episodio 39: retorno=-12.44\n",
      "Episodio 40: retorno=-12.06\n",
      "Episodio 41: retorno=-24.18\n",
      "Episodio 42: retorno=-0.09\n",
      "Episodio 43: retorno=-12.64\n",
      "\n",
      " Sigma:  0.2501244835657294\n",
      "Episodio 44: retorno=-11.99\n",
      "Episodio 45: retorno=-23.86\n",
      "Episodio 46: retorno=-35.19\n",
      "Episodio 47: retorno=-12.26\n",
      "Episodio 48: retorno=-0.15\n",
      "\n",
      " Sigma:  0.23159674404234204\n",
      "Episodio 49: retorno=-12.07\n",
      "Episodio 50: retorno=-27.48\n",
      "Episodio 51: retorno=-13.05\n",
      "Episodio 52: retorno=-12.92\n",
      "Episodio 53: retorno=-12.3\n",
      "\n",
      " Sigma:  0.2144414296688352\n",
      "Episodio 54: retorno=-12.23\n",
      "Episodio 55: retorno=-12.7\n",
      "Episodio 56: retorno=-12.67\n",
      "Episodio 57: retorno=-24.44\n",
      "Episodio 58: retorno=-12.73\n",
      "\n",
      " Sigma:  0.19855687932299554\n",
      "Episodio 59: retorno=-23.47\n",
      "Episodio 60: retorno=-12.33\n",
      "Episodio 61: retorno=-0.3\n",
      "Episodio 62: retorno=-27.19\n",
      "Episodio 63: retorno=-25.35\n",
      "\n",
      " Sigma:  0.18384896233610698\n",
      "Episodio 64: retorno=-25.26\n",
      "Episodio 65: retorno=-12.98\n",
      "Episodio 66: retorno=-11.85\n",
      "Episodio 67: retorno=-13.22\n",
      "Episodio 68: retorno=-12.76\n",
      "\n",
      " Sigma:  0.17023052068158054\n",
      "Episodio 69: retorno=-12.8\n",
      "Episodio 70: retorno=-0.15\n",
      "Episodio 71: retorno=-37.0\n",
      "Episodio 72: retorno=-23.33\n",
      "Episodio 73: retorno=-13.19\n",
      "\n",
      " Sigma:  0.15762085248294494\n",
      "Episodio 74: retorno=-23.59\n",
      "Episodio 75: retorno=-11.52\n",
      "Episodio 76: retorno=-36.54\n",
      "Episodio 77: retorno=-0.16\n",
      "Episodio 78: retorno=-11.92\n",
      "\n",
      " Sigma:  0.14594523378050456\n",
      "Episodio 79: retorno=-12.71\n",
      "Episodio 80: retorno=-12.66\n",
      "Episodio 81: retorno=-24.19\n",
      "Episodio 82: retorno=-12.46\n",
      "Episodio 83: retorno=-24.77\n",
      "\n",
      " Sigma:  0.13513447572268938\n",
      "Episodio 84: retorno=-12.33\n",
      "Episodio 85: retorno=-23.84\n",
      "Episodio 86: retorno=-23.57\n",
      "Episodio 87: retorno=-23.01\n",
      "Episodio 88: retorno=-23.56\n",
      "\n",
      " Sigma:  0.1251245145580457\n",
      "Episodio 89: retorno=-24.23\n",
      "Episodio 90: retorno=-12.06\n",
      "Episodio 91: retorno=-23.8\n",
      "Episodio 92: retorno=-12.05\n",
      "Episodio 93: retorno=-23.52\n",
      "\n",
      " Sigma:  0.11585603199819047\n",
      "Episodio 94: retorno=-11.79\n",
      "Episodio 95: retorno=-12.57\n",
      "Episodio 96: retorno=-12.82\n",
      "Episodio 97: retorno=-24.03\n",
      "Episodio 98: retorno=-12.44\n",
      "\n",
      " Sigma:  0.10727410370202821\n",
      "Episodio 99: retorno=-12.91\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Rotina de treinamento do algoritmo\n",
    "'''\n",
    "num_episodes = 100\n",
    "list_retorno = []\n",
    "param_noise, ac_noise = None, True\n",
    "for i_episode in range(num_episodes):\n",
    "    #Inicializa o ambiente e coleta o estado inicial\n",
    "    state = env.reset()\n",
    "    \n",
    "    #Inicializa as variáveis de retorno e número de eventos\n",
    "    retorno, steps = 0,0\n",
    "    \n",
    "    #Loop que adiciona o ruído nos parâmetros da rede actor_noise_net()\n",
    "    for parameter_ruido, parameter_original in zip(actor_noise_net.parameters(), actor_net.parameters()):\n",
    "        parameter_ruido.data.copy_(parameter_original.data + sigma*torch.randn(parameter_original.data.shape).to(device))\n",
    "\n",
    "    #Inicializa a contagem de eventos no episódio\n",
    "    for t in count():\n",
    "        env.render() #Renderiza o ambiente\n",
    "        \n",
    "        action = select_action(torch.FloatTensor([state]).to(device), ac_noise, param_noise).cpu() #Seleciona a ação \n",
    "                                                                                                   # a partir do estado\n",
    "        next_state, reward, done, _ = env.step([action]) #Executa a ação e coleta o prox estado, \n",
    "                                                         # a recompensa e se o episódio foi finalizado\n",
    "        reward = reward/10 #Ajusta o valor da recompensa\n",
    "        \n",
    "        #Soma a recompensa ao retorno\n",
    "        retorno += reward\n",
    "\n",
    "        #Guarda a experiência na memória\n",
    "        memory.push(torch.FloatTensor([state]),\n",
    "                     action.to(device),\n",
    "                     torch.FloatTensor([next_state]),\n",
    "                     torch.FloatTensor([reward]).to(device))\n",
    "\n",
    "        #Move para o proximo estado e atualiza o ponteiro de eventos\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        #Otimiza a rede neural do crítico\n",
    "        if optimize_model() != None:\n",
    "            state_batch, action_batch = optimize_model()\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Se estiver usando o ruído na ação\n",
    "    if ac_noise == True:\n",
    "        if (i_episode+1) % T_adapt_ac == 0: #Checa se está no período de adaptação\n",
    "            desv_pad_ac = desv_pad_ac/alfa_ac #Adapta o desvio padrão do ruído que será inserido na ação\n",
    "            print('\\n Sigma: ', desv_pad_ac)\n",
    "            \n",
    "    # Se estiver usando o ruído nos parâmetros\n",
    "    if param_noise == True:\n",
    "        if (i_episode+1) % T_adapt == 0: #Checa se está no período de adaptação\n",
    "            unpertubed_action = select_action(state_batch) #Decide as ações sem nenhum ruído\n",
    "            perturbed_actions = action_batch #Guarda as ações geradas pelas redes ruidosas\n",
    "            \n",
    "            d = loss_d(unpertubed_action, perturbed_actions) #Calcula a diferença quadrática \n",
    "                                                             # entre as ações ruidosas e não ruidosas\n",
    "            d = torch.mean(d) #Calcula a média no batch\n",
    "            \n",
    "            if d <= delta: #Se a distância for MENOR do que o limiar de decisão\n",
    "                sigma = sigma*alfa #Aumenta o desvio padrão do ruído\n",
    "            else: #Se a distância for MAIOR do que o limiar de decisão\n",
    "                sigma = sigma/alfa #Diminui o desvio padrão do ruído\n",
    "            \n",
    "            print(\" \\nD: \", d.item(), '   Sigma: ', sigma)\n",
    "    \n",
    "    #Printa o retorno do episódio e guarda em uma lista\n",
    "    print(f'Episodio {i_episode}: retorno={round(retorno,2)}')\n",
    "    list_retorno.append(retorno)\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JdlCNcEhfICH"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list_retorno)\n",
    "plt.title('Retorno por episódio')\n",
    "plt.xlabel('Episódio')\n",
    "plt.ylabel('Retorno')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEJoqwKZfICH"
   },
   "source": [
    "## 5. Sugestões <a class=\"anchor\" name=\"section_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XisgbVOfICH"
   },
   "source": [
    "Neste momento, faremos algumas sugestões para estender o entendimento do leitor e aprofundar o conhecimento acerca do tema abordado neste notebook, assim como se segue:\n",
    "\n",
    "1. Altere a estrutura da rede neural e veja quais são as mudanças ocorridas.\n",
    "2. Altere a técnica de exploração para a utilização do ruído na ação, setando as variáveis para `action_noise==True` e `param_noise==None`. Altere os parâmetros da técnica, se necessário. Tente comparar o desempenho de ambos os métodos.\n",
    "3. Descomente a construção do ambiente `MountainCarContinuous-v0` presente na célula da Seção 2 em que importamos o ambiente do pêndulo e veja o algoritmo funcionando em um outro problema.\n",
    "4. Repita o processo de alterar o mecanismo de exploração. É esperado que nesse segundo problema, a técnica de exploração escolhida faça mais diferença do que no primeiro, devido às próprias características do ambiente.\n",
    "5. Perceba que tanto o agente quanto o crítico são DNNs, mas isso não é uma obrigação. Essas estruturas podem ser substituídas por CNNs, por exemplo, a depender do problema. A biblioteca `gym` oferece diferentes ambientes, dentre os quais vários possuem representação do estado sendo uma imagem, simulando a visão de um agente. [Nesse link](https://gym.openai.com/envs/FetchReach-v0/) há um exemplo de um problema desse tipo, tente implementar um agente que utiliza uma CNN para atuar nesse ambiente (ou em outro de sua preferência)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Referências <a class=\"anchor\" name=\"section_6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Deep Deterministic Policy Gradients Explained](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b)\n",
    "- Plappert, Matthias. \\\"Parameter space noise for exploration in deep reinforcement learning.\\\" arXiv preprint arXiv:1706.01905 (2017). Disponível em: https://matthiasplappert.com/publications/2017_Plappert_Master-thesis.pdf\n",
    "- [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)\n",
    "- [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DDPG.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
