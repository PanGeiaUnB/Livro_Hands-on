{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-836RadfIBp"
   },
   "source": [
    "# Lição 5 - Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9qS51y4fIBt"
   },
   "source": [
    "## 1. Introdução <a class=\"anchor\" name=\"section_1\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRpHMoGlfIBu"
   },
   "source": [
    "Na última lição, introduzimos a *Deep Q-Network* (DQN), a primeira estrutura do DRL, que mesclava conceitos de RL e conceitos DL. Nós mostramos que é possível encontrar uma política capaz de solucionar o problema do *cartople*, um problema complexo para os algoritmos de outras áreas. Entretanto, depois de 2013, ano de criação do DQN, vários outros algoritmos de DRL foram criados para solucionar algumas limitações que a estrutura pioneira apresentava. Nesse sentido surge o *Deep Deterministic Policy Gradient* (DDPG), um algoritmo clássico que nos fornece flexibilidade para trabalhar tanto com ações discretas (como o DQN), como com ações contínuas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxLFMBM5fIBv"
   },
   "source": [
    "### 1.1 Índice <a class=\"anchor\" name=\"section_1.1\"></a>\n",
    "\n",
    "* [1. Introdução](#section_1)\n",
    "    * [1.1 Índice](#section_1.1)\n",
    "* [2. O problema do Pêndulo](#section_2)\n",
    "* [3. Redes do tipo Actor-Critic](#section_3)\n",
    "    * [3.1 Deep Deterministic Gradient Policy - DDPG](#section_3.1)\n",
    "    * [3.2 Estratégias de treinamento do DDPG](#section_3.2)\n",
    "        * [3.2.1 Ruído nos parâmetros para exploração](#section_3.2.1)\n",
    "* [4. Implementação do algoritmo](#section_4)\n",
    "* [5. Sugestões](#section_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yynkOwWKfIBv"
   },
   "source": [
    "Iniciaremos o problema importando as bibliotecas que usaremos ao longo da nossa implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7a5I4V9fIBw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZERgxUgfIBw"
   },
   "source": [
    "Antes de iniciarmos a implementação do código de fato, vamos verificar se há alguma GPU disponível para rodar os códigos, com o intuito de tornar mais rápido o treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaY0FBhYfIBx",
    "outputId": "d3fb04ff-50b4-40e2-8aa8-856b576eac2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando na GPU\n"
     ]
    }
   ],
   "source": [
    "def testar_gpu():\n",
    "    '''\n",
    "    Função que testa se a GPU está habilitada.\n",
    "    \n",
    "    Outputs:\n",
    "    --------\n",
    "    device: {str}\n",
    "        variável que define se o código será executado na CPU ou na GPU\n",
    "    '''\n",
    "    train_on_gpu = torch.cuda.is_available() #Observa se a GPU está disponivel\n",
    "    if train_on_gpu: #Se sim\n",
    "        device = torch.device('cuda') #Seleciona o device como GPU\n",
    "        print(\"Treinando na GPU\") #E manda a mensagem\n",
    "    else: #Se não\n",
    "        device = torch.device('cpu') #Seleciona o device como cpu\n",
    "        print(\"GPU indisponível, treinando na CPU\") #E avisa que a GPU não esta disponível\n",
    "    \n",
    "    return device\n",
    "\n",
    "#Coleta onde o ambiente será executado\n",
    "device = testar_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTXphLiEfIBz"
   },
   "source": [
    "Como apresentado na última lição, implementaremos a definição de transição de um ambiente DRL, com estado, ação, próximo estado e recompensa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AXKrjdqfIBz"
   },
   "outputs": [],
   "source": [
    "#Define uma transição como um tupla com estado, ação, próximo estado e recompensa.\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI0Q2bBxfIB0"
   },
   "source": [
    "## 2. O problema do Pêndulo <a class=\"anchor\" name=\"section_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV6WGHoTfIB0"
   },
   "source": [
    "A última lição mostrou uma implementação de uma *Deep Q-Network*, a rede mais básica de DRL, que nos permitiu elaborar um algoritmo que conseguisse decidir quais ações devem ser tomadas pelo *cartpole*, para evitar que a haste caia. \n",
    "\n",
    "Entretanto, esse algoritmo apresenta uma clara limitação: ela só consegue resolver problemas em que as ações possíveis são **discretas**. Isso é perceptível ao perceber que a rede retorna apenas **qual** ação deve ser tomada, e não **como** ela deve ser executada. Passemos a um outro problema para que isso fique mais claro.\n",
    "\n",
    "Pense em um jogo parecido com o *cartpole*, mas agora nós queremos equilibrar um pêndulo verticalmente na posição mais alta possível. Para isso, nós podemos aplicar, no pêndulo, forças tanto no sentido horário como no anti-horário, em que a intensidade dessa força é um valor contínuo, de forma que a rotação pode ter mais ou menos intensidade, a depender da força aplicada.\n",
    "\n",
    "Assim, a rotação do pêndulo é controlada por um agente que define uma ação tal que $-2 < a < 2$, em que os valores negativos indicam que as forças são no sentido anti-horário e os positivos que são no sentido horário, enquanto o módulo do ação tal que $|a|<2$, indica a intensidade da força aplicada. A imagem abaixo mostra o pêndulo em vermelho e a representação da força aplicada como a seta preta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVQYC6n5fIB1"
   },
   "source": [
    "<center>\n",
    "<img src=\"imagens/pendulum.PNG\"></img>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jFvKhYzfIB2"
   },
   "source": [
    "Repare a ação desse ambiente é **contínua**, isto é, não basta o agente saber que precisa aplicar uma força em um ou outro sentido, ele precisa entender **o quanto** de intensidade ele deve colocar na força que será aplicada.\n",
    "\n",
    "Voltemos, então, ao DQL. Para implementar uma DQN, neste caso, precisaríamos discretizar a intensidade da força e combiná-las com o sentido de rotação, considerando que cada uma dessas combinações seja uma ação possível com um $Q(s,a)$ calculável. \n",
    "\n",
    "Perceba que essa não é uma abordagem eficaz mesmo para um problema relativamente simples como esse. Identificando essa limitação, desenvolveu-se outra arquitetura que consegue ultrapassar essa barreira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Llj13MTafIB3"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0') #Construção do ambiente do pendulo\n",
    "#env = gym.make('MountainCarContinuous-v0') #Construção do ambiente do mountain car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAqHqKQlfIB3"
   },
   "source": [
    "## 3. Redes do tipo Actor-Critic <a class=\"anchor\" name=\"section_3\"></a>\n",
    "\n",
    "É importante ressaltar que o termo *Actor-critic* **não** designa uma arquitetura de DRL, e sim uma característica que várias arquiteturas da área incorporaram, entre elas o A2C, A3C e o DDPG. Nesse notebook, apresentaremos e utilizaremos o DDPG.\n",
    "\n",
    "Dizer que uma arquitetura é *actor-critic*, significa dizer que ela utiliza, pelo menos, **duas** redes neurais no seu processo de aprendizagem: uma que toma a ação e outra que critica a ação tomada, daí surge o nome *actor-critic*.\n",
    "\n",
    "A rede do ator recebe o estado como entrada e produz uma saída, podendo ser um número ou um vetor. Após a tomada de decisão do ator, o crítico vai estimar o valor de $Q(s,a)$ a partir do estado e dessa ação realizada, tentando avaliar se a ação tomada foi boa ou ruim.\n",
    "\n",
    "Assim, percebemos que essas arquiteturas desagregam o tomador de decisão e o estimador do $Q(s,a)$, de forma que a saída do ator pode ser de natureza contínua, cabendo ao crítico realizar a estimativa do retorno a partir dessa ação tomada. Entendido isso, precisamos entender como as duas redes aprendem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaaUmBeMfIB4"
   },
   "source": [
    "### 3.1 Deep Deterministic Gradient Policy - DDPG <a class=\"anchor\" name=\"section_3.1\"></a>\n",
    "\n",
    "Repare que o problema para o agente ainda é o mesmo: precisamos desenvolver um algoritmo capaz de maximizar o retorno. Nesse cenário, como treinar o ator?\n",
    "\n",
    "Como o $Q$ é uma estimativa do retorno recebido, é possível treinar o ator através da saída do crítico: se a saída deste for alta, o ator entenderá que agiu de maneira adequada, caso contrário, o ator terá que mudar a sua política de decisão de ação, modificando os seus parâmetros.\n",
    "\n",
    "Certo, mas como treinaremos o crítico para que ele consiga fazer estimativas de $Q(s,a)$ de maneira acurada, de forma que o ator possa \"confiar\" nessa estimativa para atualizar os seus pesos? A resposta é a mesma dada para o aprendizado da DQN, a partir da **Equação de Bellman**:\n",
    "\n",
    "$$ Q(s,a) = \\mathbb{E}_{s'\\mathtt{\\sim}P}\\{r_t + \\mathbb{E}_{a'\\mathtt{\\sim}\\mu}[Q(s',a')]\\} $$\n",
    "\n",
    "Rememorando essa equação, ela nos mostra que o valor do par estado-ação atual pode ser estimado através do valor do par estado-ação futuro, somado a uma recompensa. Apesar de ambos os lados dependerem de estimativas do $Q$, o fato de não haver incerteza na medição da recompensa faz com que o lado direito da equação seja mais acurado, de forma que ele é tratado como o valor alvo para a otimização.\n",
    "\n",
    "Assim podemos resumir o processo de aprendizado utilizando a imagem abaixo. Na figura, as setas verdes significam a passagem de entradas e saídas até a estimativa dos dois lados da equação de Bellman e as setas vermelhas mostram o processo de atualização dos parâmetros das redes a partir dos custos calculados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sImaaTEzfIB5"
   },
   "source": [
    "<center>\n",
    "<img src=\"imagens/esquematico_DDPG.PNG\"></img>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ0By3RhfIB5"
   },
   "source": [
    "Na imagem acima, a função de custo do crítico ilustrada pode ser de diferentes naturezas. A mais clássica para identificar a diferença entre duas grandezas é o Erro Quadrático Médio (*Mean Squared Error* - MSE) e é ela que será utilizada. \n",
    "\n",
    "Para atualizar o ator, nós queremos promover o **aumento** do $Q(s',a')$ a cada ação executada, estimulando ações com alto valor do estado-ação. Pensando então no processo de aprendizado, sabemos que a maioria dos algoritmos se baseiam na **diminuição** do gradiente da perda, como adequar o problema?\n",
    "\n",
    "Uma solução é utilizar a perda como $-Q(s',a')$, de forma que, quanto maior o $Q$, menor a função de perda $-Q$ e, consequentemente, menor serão as atualizações dos parâmetros do ator. Em um caso contrário, a rede entenderá que as suas ações estão distantes do objetivo e que a política de tomada das ações deve ser modificada.\n",
    "\n",
    "Para entender mais sobre o processo de treinamento do DDPG, o pseudo-código pode ser encontrado neste [link](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK-KgbJTfIB6"
   },
   "source": [
    "### 3.2 Estratégias de treinamento do DDPG <a class=\"anchor\" name=\"section_3.2\"></a>\n",
    "\n",
    "Como foi dito, o treinamento das arquiteturas de DRL é um processo instável e, por isso, adotam-se algumas técnicas para tentar diminuir essa instabilidade. Entre essas técnicas estão:\n",
    "\n",
    "*    Experience Replay\n",
    "*    Target Network\n",
    "*    Utilização de ruído nos parâmetros\n",
    "\n",
    "As duas primeiras estratégias já foram explicadas na última lição, mas a última não, então faremos uma breve introdução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNsc4J9AfIB7"
   },
   "source": [
    "#### 3.2.1 Ruído nos parâmetros para exploração <a class=\"anchor\" name=\"section_3.2.1\"></a>\n",
    "\n",
    "Como foi visto, em problemas com ações discretas, é possível realizar a exploração do ambiente escolhendo algumas ações de maneira aleatória (utilizando a técniga do $\\epsilon$-greedy, por exemplo). Já em problemas com ações contínuas, o mais comum é que se adicione ruído gaussiano na saída da rede neural, variando essas ações de maneira aleatória, como mostrado nessa [publicação](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b), na seção \"*Exploration*\".\n",
    "\n",
    "Entretanto, recentemente surgiu uma nova abordagem para essa exploração. Propôs-se que, em vez de adicionar o mecanismo de exploração após o funcionamento da rede, é possível adicionar esse mecanismo **durante** a atuação da rede neural.\n",
    "\n",
    "A ideia é que nós adicionemos um ruído aleatório nos parâmetros da rede, alterando todos os pesos desta. Essa aleatoriedade garante que a rede tome ações diferentes mesmo em situações parecidas, trazendo a ideia de exploração. Essa [tese de mestrado](https://matthiasplappert.com/publications/2017_Plappert_Master-thesis.pdf) abordou o assunto de forma bastante simples e completa. Será essa a estratégia utilizada no nosso código.\n",
    "\n",
    "Basicamente, a proposta é, a cada episódio, contaminar os parâmetros da rede neural com amostras de um ruído gaussiano de média 0 e desvio padrão $\\sigma$. Essas amostras são geradas aleatoriamente a cada episódio, respeitando a distribuição gaussiana em questão. A imagem abaixo, retirada da tese citada acima, ilustra a diferença entre o ruído na ação e o ruído nos parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLcBaJ6jfIB7"
   },
   "source": [
    "<center>\n",
    "<img src=\"imagens/parameter_noise.PNG\" width=600 height=200></img>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvSqYZNSfIB8"
   },
   "source": [
    "Essa estratégia também é adaptativa, isto é, ela muda as configurações dos parâmetros de ruído, para aumentar ou diminuir o efeito exploratório. De maneira prática, com o passar dos episódios, o desvio padrão $\\sigma$ do ruído é modificado, aumentando o seu valor quando se deseja aumentar a exploração e diminuindo o seu valor quando se deseja diminuir a exploração.\n",
    "\n",
    "Essa decisão é feita analisando se a diferença entre as decisões feitas com e sem ruído é maior do que um certo limiar de decisão $\\delta$, de forma que, se for maior, o $\\sigma$ deve ser diminuído, se for menor, o $\\sigma$ deve ser aumentado. A diferença entre essas ações ruidosas e limpas geralmente é calculada utilizando o Erro Quadrático Médio, como mostrado na imagem abaixo com a função $d(\\cdot,\\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyjJZZdGfIB8"
   },
   "source": [
    "<center>\n",
    "<img src=\"imagens/distance.PNG\" width=500 height=100></img>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgkeS_nrfIB9"
   },
   "source": [
    "Dessa forma, a adaptação no parâmetro $\\sigma$ é feita a partir de um certo $\\alpha$ maior que $1$, geralmente com um valor ligeiramente maior do que 1. Essa adaptação é feita a partir do equacionamento abaixo:\n",
    "\n",
    "$$ \\sigma_{t+1} = \\left\\{\\begin{array}{ll}\n",
    "\\alpha \\sigma_t, \\:\\:\\:\\: d(\\pi(a|s), \\tilde{\\pi}(a|s)) \\leq \\delta \\\\  \n",
    "\\frac{1}{\\alpha} \\sigma_t,\\:\\:\\:\\: cc \\\\\n",
    "\\end{array} \\right. $$\n",
    "\n",
    "em que $d(\\cdot,\\cdot)$ é uma função de cálculo de distância entre vetores, $\\pi(a|s)$ são as ações determinadas pela rede não contaminada com ruído, $\\tilde{\\pi}(a|s)$ são as ações determinadas pela rede com o ruído nor parâmetros, $\\delta$ é o limiar de decisão de adaptação e $\\alpha$ é o parâmetro de adaptação.\n",
    "\n",
    "Na prática, nós criamos uma outra rede neural auxiliar que será sempre uma cópia da rede neural do agente, mas com os parâmetros contaminados, e utilizamos ela para a tomada de decisões no processo de treinamento. Entretanto, na hora de otimizar as redes, o algoritmo otimiza a rede neural **não** contaminada, atualizando seus parâmetros e copiando-os novamente nessa rede auxiliar, que será novamente contaminada com um ruído aleatório para o próximo episódio.\n",
    "\n",
    "Seguindo esse processo e o equacionamento mostrado, o algoritmo atualiza o valor de $\\sigma$ do ruído a cada $T_{adapt}$ episódios, garantindo uma exploração mais inteligente e duradoura. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5mdUSfefIB9"
   },
   "source": [
    "A partir do que foi explanado acima, podemos partir pra implementação do nosso código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5Z8HjWZfIB9"
   },
   "source": [
    "## 4. Implementação do algoritmo <a class=\"anchor\" name=\"section_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "678mkNsmfIB-"
   },
   "source": [
    "Iniciaremos o problema implementando a memória do *replay buffer*. A implementação será a mesma utilizada na última lição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOipt3WefIB-"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        '''\n",
    "        Classe que define a memória do Replay Buffer\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        capacity: {int}\n",
    "            número de elementos que podem ser armazenados na memória.\n",
    "        '''\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        '''\n",
    "        Função que salva uma transição na memória\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        *args: {collections.namedtuple}\n",
    "            transições que devem ser armazenadas na memória.\n",
    "        ''' \n",
    "        if len(self.memory) < self.capacity: #Checa se a memória está cheia e, se não estiver,\n",
    "            self.memory.append(None)         # cria um espaço vazio em que a transição será colocada\n",
    "        \n",
    "        self.memory[self.position] = Transition(*args) #armazena na memória usando o ponteiro self.position\n",
    "        self.position = (self.position + 1) % self.capacity #atualiza o ponteiro\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Função que amostra a memória, pegando batch_size's transições aleatoriamente.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        batch_size: {int}\n",
    "            Número de transições que serão coletadas na amostra.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        random.sample(self.memory, batch_size): {list}\n",
    "            lista com batch_size's transições, escolhidas aleatoriamente.\n",
    "        '''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Função que coleta o número de transições guardadas na memória.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        len(self.memory): {int}\n",
    "            Número de transições armazenadas.\n",
    "        '''\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuBWPm-EfIB_"
   },
   "source": [
    "Após a implementação da memória, podemos implementar as redes neurais do agente e do crítico. Elas são duas DNNs simples, constituídas por duas camadas lineares e uma camada de saída. Esperamos que você já tenha familiaridade com esse tipo de implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9onzwlIfIB_"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, outputs):\n",
    "        '''\n",
    "        Classe que define a rede neural do agente do algoritmo.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        input_size: {int}\n",
    "            tamanho da camada de entrada da rede;\n",
    "        outputs: {int}\n",
    "            tamanho da camada de saída da rede\n",
    "        '''\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        #Define a primeira camada linear com normalização\n",
    "        self.linear1 = nn.Linear(input_size, 32)\n",
    "        self.ln1 = nn.LayerNorm(32)\n",
    "\n",
    "        #Define a segunda camada linear com normalização\n",
    "        self.linear2 = nn.Linear(32, 64)\n",
    "        self.ln2 = nn.LayerNorm(64)\n",
    "\n",
    "        #Define a camada de saída\n",
    "        self.linear3 = nn.Linear(64, outputs)\n",
    "        \n",
    "        self.tanh = nn.Tanh() #Definição da função de ativação tangente hiperbólica.\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Função que recebe o estado e decide a ação seguindo a sua política.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        state: {torch.tensor}\n",
    "            tensor que representa o estado do ambiente.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        action: {torch.tensor}\n",
    "            tensor com a ação decidida.\n",
    "        '''\n",
    "        out = nn.functional.relu(self.ln1(self.linear1(state))) #Execução da primeira camada\n",
    "        out = nn.functional.relu(self.ln2(self.linear2(out))) #Execução da segunda camada\n",
    "        action = self.tanh(self.linear3(out)) #Execução da camada de saída\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DjQQGHffICA"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, outputs):\n",
    "        '''\n",
    "        Classe que define a rede neural do crítico do algoritmo.\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        state_size: {int}\n",
    "            tamanho do estado do ambiente;\n",
    "        action_size: {int}\n",
    "            tamanho da ação do ambiente;    \n",
    "        outputs: {int}\n",
    "            tamanho da camada de saída da rede\n",
    "        '''\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        #Define a primeira camada linear com normalização\n",
    "        self.linear1 = nn.Linear(state_size, 32)\n",
    "        self.ln1 = nn.LayerNorm(32)\n",
    "\n",
    "        #Define a segunda camada linear com normalização\n",
    "        self.linear2 = nn.Linear(32 + action_size, 64) #A ação é adicionada apenas nessa segunda camada\n",
    "        self.ln2 = nn.LayerNorm(64)\n",
    "\n",
    "        #Define a camada de saída\n",
    "        self.linear3 = nn.Linear(64, outputs)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        '''\n",
    "        Função que recebe o estado e a ação e retorna o valor do estado-ação, o Q(s,a).\n",
    "        \n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        state: {torch.tensor}\n",
    "            tensor que representa o estado do ambiente.\n",
    "        action: {torch.tensor}\n",
    "            tensor que representa a ação realizada.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        Q: {torch.tensor}\n",
    "            tensor o valor do estado-ação, o Q(s,a).\n",
    "        '''\n",
    "        s1 = nn.functional.relu(self.ln1(self.linear1(state))) #Execução da primeira camada apenas com o estado\n",
    "        out = torch.cat((s1,action), dim=1) #Concatenação da saída da primeira camada com a ação realizada\n",
    "        out = nn.functional.relu(self.ln2(self.linear2(out))) #Execução da segunda camada\n",
    "        Q = self.linear3(out) #Execução da camada de saída\n",
    "        \n",
    "        return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NthyGK_DfICA"
   },
   "source": [
    "Finalizada a implementação da memória e das redes neurais, precisamos definir as configurações do nosso algoritmo. Primeiro, faremos uma investigação no nosso ambiente, analisando qual é o tamanho do estado, das ações esperadas e em qual intervalo a ação deve estar contida. Nas linhas de código abaixo guardamos essas informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQBgrO-afICA",
    "outputId": "8326c3d4-3af2-4fc6-ccd9-4ee4dda2c11d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = int(env.observation_space.shape[0]) #shape do estado\n",
    "output_size = int(env.action_space.shape[0]) #shape da ação\n",
    "max_action = float(env.action_space.high[0]) #range válido para a ação\n",
    "input_size, output_size, max_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiN29TZzfICB"
   },
   "source": [
    "Além da configuração do ambiente, também precisamos configurar o nosso algoritmo. Na célula abaixo, nós definiremos diversos parâmetros importantes para o treinamento. Começamos com os mais tradicionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GV_7c-eBfICB"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #Tamanho do batch\n",
    "GAMMA = 0.95 #Taxa de aprendizado\n",
    "tau = 0.05 #Parâmetro de suavização da sincronização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAs14N1rfICB"
   },
   "source": [
    "Agora, precisamos configurar nossos mecanismos de exploração. Nesse caso, nós disponibilizamos uma implementação do ruído nos parâmetros e uma do ruído na ação. Dessa forma, será possível comparar ambas as estratégias. As configurações estão disponíveis abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Adhwb4bgfICC"
   },
   "outputs": [],
   "source": [
    "# Configuração do ruído no parâmetro\n",
    "sigma = 0.25 #Desvio padrão do ruído gaussiano que será inserido nos parâmetros\n",
    "alfa = 1.03 #Parâmetro de adaptação do ruído nos parâmetros\n",
    "delta = 1e-1 #Limiar de decisão de correção do ruído nos parâmetros\n",
    "T_adapt = 5 #Número de episódios entre as atualizações do ruído nos parâmetros\n",
    "\n",
    "# Configuração do ruído na ação\n",
    "desv_pad_ac = 0.5 #Desvio padrão do ruído gaussiano que será inserido na ação\n",
    "alfa_ac = 1.08 #Parâmetro de adaptação do ruído na ação\n",
    "T_adapt_ac = 5 #Número de episódios entre as atualizações do ruído na ação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUhDhxmpfICC"
   },
   "source": [
    "Por fim, devemos instanciar as nossas redes neurais, sincronizá-las e configurar os otimizadores e funções de perda para aprendizado das redes. Tudo isso está sendo feito na célula abaixo, além do instanciamento da memória, com capacidade de armazenamento de 10 mil transições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQLevwMWfICC"
   },
   "outputs": [],
   "source": [
    "# Definição das redes do agente, incluindo a target e a utilizada para implementação do ruído nos parâmetros\n",
    "actor_net = Actor(input_size, output_size).to(device)\n",
    "actor_noise_net = Actor(input_size, output_size).to(device)\n",
    "actor_target_net = Actor(input_size, output_size).to(device)\n",
    "\n",
    "# Definição das redes do crítico, incluindo a target\n",
    "critic_net = Critic(input_size, output_size, 1).to(device)\n",
    "critic_target_net = Critic(input_size, output_size, 1).to(device)\n",
    "\n",
    "# Sincronização das redes\n",
    "actor_target_net.load_state_dict(actor_net.state_dict())\n",
    "actor_target_net.eval()\n",
    "actor_noise_net.load_state_dict(actor_net.state_dict())\n",
    "actor_noise_net.eval()\n",
    "critic_target_net.load_state_dict(critic_net.state_dict())\n",
    "critic_target_net.eval()\n",
    "\n",
    "# Definição dos otimizadores\n",
    "optimizer_critic = torch.optim.Adam(critic_net.parameters(), lr=0.001)\n",
    "optimizer_actor = torch.optim.Adam(actor_net.parameters(), lr=0.0001)\n",
    "\n",
    "# Instanciamento da memória\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Definição das funções de perda\n",
    "loss_critic = nn.MSELoss() #Função de perda para otimização do crítico\n",
    "loss_d = nn.MSELoss() #Função para ajuste dos parâmetros para utilização do ruído nos parâmetros para exploração\n",
    "\n",
    "# Inicialização das variáveis de perda\n",
    "actor_loss = None\n",
    "critic_loss = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iehjIqFafICD"
   },
   "source": [
    "A partir de todas essas configurações, podemos definir a função responsável por selecionar a ação que deve ser executada, levando em conta tanto a política, como a estratégia de exploração adotada. Essa função está implementada abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7f0NBZRfICD"
   },
   "outputs": [],
   "source": [
    "def select_action(state, action_noise=None, param_noise=None):\n",
    "    '''\n",
    "    Função que recebe o estado e decide a ação, levando em conta a política e o mecanismo de exploração.\n",
    "        \n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    state: {torch.tensor}\n",
    "        tensor que representa o estado do ambiente.\n",
    "    action_noise: {boolean}\n",
    "        flag que define se utilizaremos os ruídos na ação.\n",
    "    param_noise: {boolean}\n",
    "        flag que define se utilizaremos os ruídos nos parâmetros.\n",
    "       \n",
    "    Outputs:\n",
    "    --------\n",
    "    mu: {torch.tensor}\n",
    "        tensor com a ação escolhida pela política.\n",
    "    '''\n",
    "    #Definição das redes em estado de avaliação e não de treino\n",
    "    actor_net.eval()\n",
    "    actor_noise_net.eval()\n",
    "    \n",
    "    if param_noise is not None:  #Se usar o ruído nos parâmetros,\n",
    "        mu = actor_noise_net(state.float()) # utilize a rede ruidosa\n",
    "    else: #Se não usar\n",
    "        mu = actor_net(state.float()) # utilize a rede sem ruído\n",
    "    \n",
    "    mu = mu.data #Correção no formato do tensor\n",
    "    \n",
    "    if action_noise is not None: #Se usar o ruído na ação,\n",
    "        with torch.no_grad(): \n",
    "            # soma o ruído na saída da rede\n",
    "            mu = torch.Tensor(mu.cpu().numpy() + np.random.normal(0,desv_pad_ac,output_size)).to(device) \n",
    "    \n",
    "    mu = mu.squeeze() #corrige o formato do tensor\n",
    "    mu = max_action*mu.clamp(min=-1,max=1) #multiplica a ação no range válido\n",
    "    \n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFY164UzfICE"
   },
   "source": [
    "Também definimos uma função responsável por fazer a sincronização suavizada entre os parâmetros de duas redes neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1u5GCSIfICE"
   },
   "outputs": [],
   "source": [
    "def weightSync(target_model, source_model):\n",
    "    '''\n",
    "    Função que executa a sincronização das redes de maneira suavizada\n",
    "    \n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    target_model: {torch.nn}\n",
    "        rede neural que receberá os parâmetros da rede source_model de maneira suavizada.\n",
    "    source_model: {torch.nn}\n",
    "        rede neural que fornecerá os parâmetros para a outra rede.\n",
    "    '''\n",
    "    #Copia os parâmetros, suavizando a sincronização\n",
    "    for parameter_target, parameter_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        parameter_target.data.copy_((1 - tau) * parameter_target.data + tau * parameter_source.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNwY9gSvfICF"
   },
   "source": [
    "Por fim, devemos implementar a função de otimização das redes neurais, onde a equação de Bellman será implementada e as redes serão atualizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C810SZ4CfICF"
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    '''\n",
    "    Função utilizada para aprendizado das redes neurais\n",
    "        \n",
    "    Outputs:\n",
    "    --------\n",
    "    state_batch: {torch.tensor}\n",
    "        batch com os tensores que representam os estados.\n",
    "    action_batch: {torch.tensor}\n",
    "        batch com os tensores que representam as ações.\n",
    "    OBS: essas saídas servem para, posteriormente, ajustar a configuração do ruído nos parâmetros\n",
    "    '''\n",
    "    if len(memory) < BATCH_SIZE: #Verifica se a memória ja tem BATCH_SIZE's amostras\n",
    "        return\n",
    "    \n",
    "    #Define que as redes estão em treinamento\n",
    "    actor_net.train()\n",
    "    critic_net.train()\n",
    "    actor_noise_net.train()\n",
    "    \n",
    "    #Amostra a memória em BATCH_SIZE's transições\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    #Define os tensores que serão utilizados no treinamento, \n",
    "    # agrupando os estados, ações e recompensas de todas as transições\n",
    "    next_state_batch = torch.autograd.Variable(torch.cat(batch.next_state)).to(device)\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = []\n",
    "    for tupla in batch.action:\n",
    "        action_batch.append(np.array(tupla.cpu()))\n",
    "    action_batch = torch.from_numpy(np.array(action_batch)).view(-1,1).to(device)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    #Calcula as ações a partir da rede target recebendo os estados vivenciados\n",
    "    next_actor_action_value = actor_target_net(next_state_batch.float())\n",
    "    #Calcula os Q(s,a) a partir dos estados e das ações calculadas pela rede agente do target\n",
    "    with torch.no_grad():\n",
    "        next_state_values = critic_target_net(next_state_batch.float(), next_actor_action_value.float()).squeeze()\n",
    "\n",
    "    #Calcula o lado direito da equação de Bellman\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    ### Otimização do crítico ###\n",
    "    optimizer_critic.zero_grad()\n",
    "    #Cálculo do Q usando a rede que NÃO é target e as ações decididas originalmente\n",
    "    state_action_values = critic_net(state_batch.float(), action_batch.float())\n",
    "    #Calcula a diferença entre os dois lados da equação de Bellman\n",
    "    critic_loss = loss_critic(state_action_values.float().squeeze(), expected_state_action_values.float())\n",
    "    #Otimiza os parâmetros da rede\n",
    "    critic_loss.backward()\n",
    "    optimizer_critic.step()\n",
    "    ###  ###\n",
    "    \n",
    "    ### Otimização do agente ###\n",
    "    optimizer_actor.zero_grad()\n",
    "    #Cálculo do -Q usando a rede que NÃO é target e as decisões do agente que NÃO é target\n",
    "    gradient = -critic_net(state_batch.float(), actor_net(state_batch.float()).float()) \n",
    "    #Calculo da função de perda como a média dos -Q estimados na linha acima\n",
    "    actor_loss = torch.mean(gradient)\n",
    "    #Otimiza os parâmetros da rede\n",
    "    actor_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    ###  ###\n",
    "    \n",
    "    #Sincronização suavizada entre redes originais e redes target\n",
    "    weightSync(actor_target_net, actor_net)\n",
    "    weightSync(critic_target_net, critic_net)\n",
    "    \n",
    "    return state_batch, action_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51Scu4gYfICG"
   },
   "source": [
    "Com todos os elementos configurados e todas as funções necessárias definidas, podemos realizar o processo de treinamento. Ele será feito em 500 episódios e, inicialmente, utilizaremos o ruído nos parâmetros para exploração. O retorno por episódio será armazenado para ser plotado, posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLIW3E_NfICG",
    "outputId": "283f841a-46b2-4ae8-d1a0-640ed19640cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 0: retorno=-2.56\n",
      "Episodio 1: retorno=-2.59\n",
      "Episodio 2: retorno=-2.36\n",
      "Episodio 3: retorno=-2.35\n",
      "\n",
      " Sigma:  0.4629629629629629\n",
      "Episodio 4: retorno=-2.32\n",
      "Episodio 5: retorno=-1.96\n",
      "Episodio 6: retorno=-2.09\n",
      "Episodio 7: retorno=-2.1\n",
      "Episodio 8: retorno=-2.13\n",
      "\n",
      " Sigma:  0.42866941015089155\n",
      "Episodio 9: retorno=-2.07\n",
      "Episodio 10: retorno=-1.79\n",
      "Episodio 11: retorno=-1.88\n",
      "Episodio 12: retorno=-1.68\n",
      "Episodio 13: retorno=-1.73\n",
      "\n",
      " Sigma:  0.39691612051008474\n",
      "Episodio 14: retorno=-1.83\n",
      "Episodio 15: retorno=-1.64\n",
      "Episodio 16: retorno=-1.52\n",
      "Episodio 17: retorno=-1.49\n",
      "Episodio 18: retorno=-1.54\n",
      "\n",
      " Sigma:  0.3675149263982266\n",
      "Episodio 19: retorno=-1.49\n",
      "Episodio 20: retorno=-1.39\n",
      "Episodio 21: retorno=-1.38\n",
      "Episodio 22: retorno=-1.33\n",
      "Episodio 23: retorno=-1.39\n",
      "\n",
      " Sigma:  0.34029159851687646\n",
      "Episodio 24: retorno=-1.41\n",
      "Episodio 25: retorno=-1.18\n",
      "Episodio 26: retorno=-1.12\n",
      "Episodio 27: retorno=-1.22\n",
      "Episodio 28: retorno=-1.2\n",
      "\n",
      " Sigma:  0.31508481344155226\n",
      "Episodio 29: retorno=-1.11\n",
      "Episodio 30: retorno=-1.02\n",
      "Episodio 31: retorno=-1.0\n",
      "Episodio 32: retorno=-1.03\n",
      "Episodio 33: retorno=-0.98\n",
      "\n",
      " Sigma:  0.2917451976310669\n",
      "Episodio 34: retorno=-1.02\n",
      "Episodio 35: retorno=-0.85\n",
      "Episodio 36: retorno=-0.89\n",
      "Episodio 37: retorno=-0.84\n",
      "Episodio 38: retorno=-0.99\n",
      "\n",
      " Sigma:  0.2701344422509878\n",
      "Episodio 39: retorno=-0.81\n",
      "Episodio 40: retorno=-0.77\n",
      "Episodio 41: retorno=-0.75\n",
      "Episodio 42: retorno=-0.8\n",
      "Episodio 43: retorno=-0.77\n",
      "\n",
      " Sigma:  0.2501244835657294\n",
      "Episodio 44: retorno=-0.77\n",
      "Episodio 45: retorno=-0.63\n",
      "Episodio 46: retorno=-0.65\n",
      "Episodio 47: retorno=-0.65\n",
      "Episodio 48: retorno=-0.63\n",
      "\n",
      " Sigma:  0.23159674404234204\n",
      "Episodio 49: retorno=-0.67\n",
      "Episodio 50: retorno=-0.53\n",
      "Episodio 51: retorno=-0.53\n",
      "Episodio 52: retorno=-0.53\n",
      "Episodio 53: retorno=-0.55\n",
      "\n",
      " Sigma:  0.2144414296688352\n",
      "Episodio 54: retorno=-0.56\n",
      "Episodio 55: retorno=-0.46\n",
      "Episodio 56: retorno=-0.46\n",
      "Episodio 57: retorno=-0.48\n",
      "Episodio 58: retorno=-0.46\n",
      "\n",
      " Sigma:  0.19855687932299554\n",
      "Episodio 59: retorno=-0.47\n",
      "Episodio 60: retorno=-0.4\n",
      "Episodio 61: retorno=-0.39\n",
      "Episodio 62: retorno=-0.46\n",
      "Episodio 63: retorno=-0.42\n",
      "\n",
      " Sigma:  0.18384896233610698\n",
      "Episodio 64: retorno=-0.37\n",
      "Episodio 65: retorno=-0.36\n",
      "Episodio 66: retorno=-0.35\n",
      "Episodio 67: retorno=-0.37\n",
      "Episodio 68: retorno=-0.38\n",
      "\n",
      " Sigma:  0.17023052068158054\n",
      "Episodio 69: retorno=-0.35\n",
      "Episodio 70: retorno=-0.34\n",
      "Episodio 71: retorno=-0.3\n",
      "Episodio 72: retorno=-0.3\n",
      "Episodio 73: retorno=-0.31\n",
      "\n",
      " Sigma:  0.15762085248294494\n",
      "Episodio 74: retorno=-0.31\n",
      "Episodio 75: retorno=-0.25\n",
      "Episodio 76: retorno=-0.24\n",
      "Episodio 77: retorno=-0.27\n",
      "Episodio 78: retorno=-0.25\n",
      "\n",
      " Sigma:  0.14594523378050456\n",
      "Episodio 79: retorno=-0.24\n",
      "Episodio 80: retorno=-0.21\n",
      "Episodio 81: retorno=-0.21\n",
      "Episodio 82: retorno=-0.23\n",
      "Episodio 83: retorno=-0.22\n",
      "\n",
      " Sigma:  0.13513447572268938\n",
      "Episodio 84: retorno=-0.22\n",
      "Episodio 85: retorno=-0.2\n",
      "Episodio 86: retorno=-0.2\n",
      "Episodio 87: retorno=-0.2\n",
      "Episodio 88: retorno=-0.21\n",
      "\n",
      " Sigma:  0.1251245145580457\n",
      "Episodio 89: retorno=-0.2\n",
      "Episodio 90: retorno=-0.16\n",
      "Episodio 91: retorno=-0.16\n",
      "Episodio 92: retorno=-0.17\n",
      "Episodio 93: retorno=-0.16\n",
      "\n",
      " Sigma:  0.11585603199819047\n",
      "Episodio 94: retorno=-0.17\n",
      "Episodio 95: retorno=-0.14\n",
      "Episodio 96: retorno=-0.14\n",
      "Episodio 97: retorno=-0.15\n",
      "Episodio 98: retorno=-0.14\n",
      "\n",
      " Sigma:  0.10727410370202821\n",
      "Episodio 99: retorno=-0.14\n",
      "Episodio 100: retorno=-0.13\n",
      "Episodio 101: retorno=-0.12\n",
      "Episodio 102: retorno=-0.12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2318412e200a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m#Inicializa a contagem de eventos no episódio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Renderiza o ambiente\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mac_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_noise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Seleciona a ação\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\continuous_mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcartrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_always_dwm\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dwm_composition_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                     \u001b[0m_dwmapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDwmFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Rotina de treinamento do algoritmo\n",
    "'''\n",
    "num_episodes = 500\n",
    "list_retorno = []\n",
    "param_noise, ac_noise = None, True\n",
    "for i_episode in range(num_episodes):\n",
    "    #Inicializa o ambiente e coleta o estado inicial\n",
    "    state = env.reset()\n",
    "    \n",
    "    #Inicializa as variáveis de retorno e número de eventos\n",
    "    retorno, steps = 0,0\n",
    "    \n",
    "    #Loop que adiciona o ruído nos parâmetros da rede actor_noise_net()\n",
    "    for parameter_ruido, parameter_original in zip(actor_noise_net.parameters(), actor_net.parameters()):\n",
    "        parameter_ruido.data.copy_(parameter_original.data + sigma*torch.randn(parameter_original.data.shape).to(device))\n",
    "\n",
    "    #Inicializa a contagem de eventos no episódio\n",
    "    for t in count():\n",
    "        env.render() #Renderiza o ambiente\n",
    "        \n",
    "        action = select_action(torch.FloatTensor([state]).to(device), ac_noise, param_noise).cpu() #Seleciona a ação \n",
    "                                                                                                   # a partir do estado\n",
    "        next_state, reward, done, _ = env.step([action]) #Executa a ação e coleta o prox estado, \n",
    "                                                         # a recompensa e se o episódio foi finalizado\n",
    "        reward = reward/10 #Ajusta o valor da recompensa\n",
    "        \n",
    "        #Soma a recompensa ao retorno\n",
    "        retorno += reward\n",
    "\n",
    "        #Guarda a experiência na memória\n",
    "        memory.push(torch.FloatTensor([state]),\n",
    "                     action.to(device),\n",
    "                     torch.FloatTensor([next_state]),\n",
    "                     torch.FloatTensor([reward]).to(device))\n",
    "\n",
    "        #Move para o proximo estado e atualiza o ponteiro de eventos\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        #Otimiza a rede neural do crítico\n",
    "        if optimize_model() != None:\n",
    "            state_batch, action_batch = optimize_model()\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Se estiver usando o ruído na ação\n",
    "    if ac_noise == True:\n",
    "        if (i_episode+1) % T_adapt_ac == 0: #Checa se está no período de adaptação\n",
    "            desv_pad_ac = desv_pad_ac/alfa_ac #Adapta o desvio padrão do ruído que será inserido na ação\n",
    "            print('\\n Sigma: ', desv_pad_ac)\n",
    "            \n",
    "    # Se estiver usando o ruído nos parâmetros\n",
    "    if param_noise == True:\n",
    "        if (i_episode+1) % T_adapt == 0: #Checa se está no período de adaptação\n",
    "            unpertubed_action = select_action(state_batch) #Decide as ações sem nenhum ruído\n",
    "            perturbed_actions = action_batch #Guarda as ações geradas pelas redes ruidosas\n",
    "            \n",
    "            d = loss_d(unpertubed_action, perturbed_actions) #Calcula a diferença quadrática \n",
    "                                                             # entre as ações ruidosas e não ruidosas\n",
    "            d = torch.mean(d) #Calcula a média no batch\n",
    "            \n",
    "            if d <= delta: #Se a distância for MENOR do que o limiar de decisão\n",
    "                sigma = sigma*alfa #Aumenta o desvio padrão do ruído\n",
    "            else: #Se a distância for MAIOR do que o limiar de decisão\n",
    "                sigma = sigma/alfa #Diminui o desvio padrão do ruído\n",
    "            \n",
    "            print(\" \\nD: \", d.item(), '   Sigma: ', sigma)\n",
    "    \n",
    "    #Printa o retorno do episódio e guarda em uma lista\n",
    "    print(f'Episodio {i_episode}: retorno={round(retorno,2)}')\n",
    "    list_retorno.append(retorno)\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdlCNcEhfICH"
   },
   "outputs": [],
   "source": [
    "plt.plot(list_retorno)\n",
    "plt.title('Retorno por episódio')\n",
    "plt.xlabel('Episódio')\n",
    "plt.ylabel('Retorno')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEJoqwKZfICH"
   },
   "source": [
    "## 5. Sugestões <a class=\"anchor\" name=\"section_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XisgbVOfICH"
   },
   "source": [
    "1. Altere a estrutura da rede neural e veja quais são as mudanças ocorridas.\n",
    "2. Altere a técnica de exploração para a utilização do ruído na ação, setando as variáveis para `action_noise==True` e `param_noise==None`. Altere os parâmetros da técnica, se necessário. Tente comparar o desempenho de ambos os métodos.\n",
    "3. Descomente a construção do ambiente `MountainCarContinuous-v0` presente na célula da Seção 2 em que importamos o ambiente do pêndulo e veja o algoritmo funcionando em um outro problema.\n",
    "4. Repita o processo de alterar o mecanismo de exploração. É esperado que nesse segundo problema, a técnica de exploração escolhida faça mais diferença do que no primeiro, devido às próprias características do ambiente.\n",
    "5. Perceba que tanto o agente quanto o crítico são DNNs, mas isso não é uma obrigação. Essas estruturas podem ser substituídas por CNNs, por exemplo, a depender do problema. A biblioteca `gym` oferece diferentes ambientes, dentre os quais vários possuem representação do estado sendo uma imagem, simulando a visão de um agente. [Nesse link](https://gym.openai.com/envs/FetchReach-v0/) há um exemplo de um problema desse tipo, tente implementar um agente que utiliza uma CNN para atuar nesse ambiente (ou em outro de sua preferência)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DDPG.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
